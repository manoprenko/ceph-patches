# -*- mode: YAML -*-
---

options:
- name: host
  type: str
  level: basic
  desc: local hostname
  long_desc: if blank, ceph assumes the short hostname (hostname -s)
  tags:
  - network
  services:
  - common
  flags:
  - no_mon_update
  with_legacy: true
- name: fsid
  type: uuid
  level: basic
  desc: cluster fsid (uuid)
  fmt_desc: The cluster ID. One per cluster.
    May be generated by a deployment tool if not specified.
  note: Do not set this value if you use a deployment tool that does
    it for you.
  tags:
  - service
  services:
  - common
  flags:
  - no_mon_update
  - startup
- name: public_addr
  type: addr
  level: basic
  desc: public-facing address to bind to
  fmt_desc: The IP address for the public (front-side) network.
   Set for each daemon.
  services:
  - mon
  - mds
  - osd
  - mgr
  flags:
  - startup
  with_legacy: true
- name: public_addrv
  type: addrvec
  level: basic
  desc: public-facing address to bind to
  services:
  - mon
  - mds
  - osd
  - mgr
  flags:
  - startup
  with_legacy: true
- name: public_bind_addr
  type: addr
  level: advanced
  services:
  - mon
  flags:
  - startup
  fmt_desc: In some dynamic deployments the Ceph MON daemon might bind
   to an IP address locally that is different from the ``public_addr``
   advertised to other peers in the network. The environment must ensure
   that routing rules are set correctly. If ``public_bind_addr`` is set
   the Ceph Monitor daemon will bind to it locally and use ``public_addr``
   in the monmaps to advertise its address to peers. This behavior is limited
   to the Monitor daemon.
  with_legacy: true
- name: cluster_addr
  type: addr
  level: basic
  desc: cluster-facing address to bind to
  fmt_desc: The IP address for the cluster (back-side) network.
   Set for each daemon.
  tags:
  - network
  services:
  - osd
  flags:
  - startup
  with_legacy: true
- name: public_network
  type: str
  level: advanced
  desc: Network(s) from which to choose a public address to bind to
  fmt_desc: The IP address and netmask of the public (front-side) network
   (e.g., ``192.168.0.0/24``). Set in ``[global]``. You may specify
   comma-separated subnets. The format of it looks like
   ``{ip-address}/{netmask} [, {ip-address}/{netmask}]``
  tags:
  - network
  services:
  - mon
  - mds
  - osd
  - mgr
  flags:
  - startup
  with_legacy: true
- name: public_network_interface
  type: str
  level: advanced
  desc: Interface name(s) from which to choose an address from a public_network to
    bind to; public_network must also be specified.
  tags:
  - network
  services:
  - mon
  - mds
  - osd
  - mgr
  see_also:
  - public_network
  flags:
  - startup
- name: cluster_network
  type: str
  level: advanced
  desc: Network(s) from which to choose a cluster address to bind to
  fmt_desc: The IP address and netmask of the cluster (back-side) network
   (e.g., ``10.0.0.0/24``).  Set in ``[global]``. You may specify
   comma-separated subnets. The format of it looks like
   ``{ip-address}/{netmask} [, {ip-address}/{netmask}]``
  tags:
  - network
  services:
  - osd
  flags:
  - startup
  with_legacy: true
- name: cluster_network_interface
  type: str
  level: advanced
  desc: Interface name(s) from which to choose an address from a cluster_network to
    bind to; cluster_network must also be specified.
  tags:
  - network
  services:
  - mon
  - mds
  - osd
  - mgr
  see_also:
  - cluster_network
  flags:
  - startup
- name: monmap
  type: str
  level: advanced
  desc: path to MonMap file
  long_desc: This option is normally used during mkfs, but can also be used to identify
    which monitors to connect to.
  services:
  - mon
  flags:
  - no_mon_update
  - create
- name: mon_host
  type: str
  level: basic
  desc: list of hosts or addresses to search for a monitor
  long_desc: This is a comma, whitespace, or semicolon separated list of IP addresses
    or hostnames. Hostnames are resolved via DNS and all A or AAAA records are included
    in the search list.
  services:
  - common
  flags:
  - no_mon_update
  - startup
- name: mon_host_override
  type: str
  level: advanced
  desc: monitor(s) to use overriding the MonMap
  fmt_desc: the list of monitors for the cluster to
    **initially** contact when beginning a new instance of communication with the
    Ceph cluster.  This overrides the known monitor list derived from MonMap
    updates sent to older Ceph instances (like librados cluster handles).  It is
    expected this option is primarily useful for debugging.
  services:
  - common
  flags:
  - no_mon_update
  - startup
- name: mon_dns_srv_name
  type: str
  level: advanced
  desc: name of DNS SRV record to check for monitor addresses
  fmt_desc: the service name used querying the DNS for the monitor hosts/addresses
  default: ceph-mon
  tags:
  - network
  services:
  - common
  see_also:
  - mon_host
  flags:
  - startup
- name: container_image
  type: str
  level: basic
  desc: container image (used by cephadm orchestrator)
  default: docker.io/ceph/daemon-base:latest-master-devel
  flags:
  - startup
- name: no_config_file
  type: bool
  level: advanced
  desc: signal that we don't require a config file to be present
  long_desc: When specified, we won't be looking for a configuration file, and will
    instead expect that whatever options or values are required for us to work will
    be passed as arguments.
  default: false
  tags:
  - config
  services:
  - common
  flags:
  - no_mon_update
  - startup
- name: lockdep
  type: bool
  level: dev
  desc: enable lockdep lock dependency analyzer
  default: false
  services:
  - common
  flags:
  - no_mon_update
  - startup
  with_legacy: true
- name: lockdep_force_backtrace
  type: bool
  level: dev
  desc: always gather current backtrace at every lock
  default: false
  services:
  - common
  see_also:
  - lockdep
  flags:
  - startup
  with_legacy: true
- name: run_dir
  type: str
  level: advanced
  desc: path for the 'run' directory for storing pid and socket files
  default: /var/run/ceph
  services:
  - common
  see_also:
  - admin_socket
  flags:
  - startup
  with_legacy: true
- name: admin_socket
  type: str
  level: advanced
  desc: path for the runtime control socket file, used by the 'ceph daemon' command
  fmt_desc: The socket for executing administrative commands on a daemon,
    irrespective of whether Ceph Monitors have established a quorum.
  daemon_default: $run_dir/$cluster-$name.asok
  services:
  - common
  flags:
  - startup
  # default changed by common_preinit()
  with_legacy: true
- name: admin_socket_mode
  type: str
  level: advanced
  desc: file mode to set for the admin socket file, e.g, '0755'
  services:
  - common
  see_also:
  - admin_socket
  flags:
  - startup
  with_legacy: true
- name: daemonize
  type: bool
  level: advanced
  desc: whether to daemonize (background) after startup
  default: false
  daemon_default: true
  tags:
  - service
  services:
  - mon
  - mgr
  - osd
  - mds
  see_also:
  - pid_file
  - chdir
  flags:
  - no_mon_update
  - startup
  # default changed by common_preinit()
  with_legacy: true
- name: setuser
  type: str
  level: advanced
  desc: uid or user name to switch to on startup
  long_desc: This is normally specified by the systemd unit file.
  tags:
  - service
  services:
  - mon
  - mgr
  - osd
  - mds
  see_also:
  - setgroup
  flags:
  - startup
  with_legacy: true
- name: setgroup
  type: str
  level: advanced
  desc: gid or group name to switch to on startup
  long_desc: This is normally specified by the systemd unit file.
  tags:
  - service
  services:
  - mon
  - mgr
  - osd
  - mds
  see_also:
  - setuser
  flags:
  - startup
  with_legacy: true
- name: setuser_match_path
  type: str
  level: advanced
  desc: if set, setuser/setgroup is condition on this path matching ownership
  long_desc: If setuser or setgroup are specified, and this option is non-empty, then
    the uid/gid of the daemon will only be changed if the file or directory specified
    by this option has a matching uid and/or gid.  This exists primarily to allow
    switching to user ceph for OSDs to be conditional on whether the osd data contents
    have also been chowned after an upgrade.  This is normally specified by the systemd
    unit file.
  tags:
  - service
  services:
  - mon
  - mgr
  - osd
  - mds
  see_also:
  - setuser
  - setgroup
  flags:
  - startup
  with_legacy: true
- name: pid_file
  type: str
  level: advanced
  desc: path to write a pid file (if any)
  fmt_desc: The file in which the mon, osd or mds will write its
    PID.  For instance, ``/var/run/$cluster/$type.$id.pid``
    will create /var/run/ceph/mon.a.pid for the ``mon`` with
    id ``a`` running in the ``ceph`` cluster. The ``pid
    file`` is removed when the daemon stops gracefully. If
    the process is not daemonized (i.e. runs with the ``-f``
    or ``-d`` option), the ``pid file`` is not created.
  tags:
  - service
  services:
  - mon
  - mgr
  - osd
  - mds
  flags:
  - startup
  with_legacy: true
- name: chdir
  type: str
  level: advanced
  desc: path to chdir(2) to after daemonizing
  fmt_desc: The directory Ceph daemons change to once they are
    up and running. Default ``/`` directory recommended.
  tags:
  - service
  services:
  - mon
  - mgr
  - osd
  - mds
  see_also:
  - daemonize
  flags:
  - no_mon_update
  - startup
  with_legacy: true
- name: fatal_signal_handlers
  type: bool
  level: advanced
  desc: whether to register signal handlers for SIGABRT etc that dump a stack trace
  long_desc: This is normally true for daemons and values for libraries.
  fmt_desc: If set, we will install signal handlers for SEGV, ABRT, BUS, ILL,
    FPE, XCPU, XFSZ, SYS signals to generate a useful log message
  default: true
  tags:
  - service
  services:
  - mon
  - mgr
  - osd
  - mds
  flags:
  - startup
  with_legacy: true
- name: crash_dir
  type: str
  level: advanced
  desc: Directory where crash reports are archived
  default: /var/lib/ceph/crash
  flags:
  - startup
  with_legacy: true
- name: restapi_log_level
  type: str
  level: advanced
  desc: default set by python code
  with_legacy: true
- name: restapi_base_url
  type: str
  level: advanced
  desc: default set by python code
  with_legacy: true
- name: erasure_code_dir
  type: str
  level: advanced
  desc: directory where erasure-code plugins can be found
  default: @CEPH_INSTALL_FULL_PKGLIBDIR@/erasure-code
  services:
  - mon
  - osd
  flags:
  - startup
  with_legacy: true
- name: log_file
  type: str
  level: basic
  desc: path to log file
  daemon_default: /var/log/ceph/$cluster-$name.log
  see_also:
  - log_to_file
  - log_to_stderr
  - err_to_stderr
  - log_to_syslog
  - err_to_syslog
  # default changed by common_preinit()
  with_legacy: true
- name: log_max_new
  type: int
  level: advanced
  desc: max unwritten log entries to allow before waiting to flush to the log
  default: 1000
  see_also:
  - log_max_recent
  # default changed by common_preinit()
  with_legacy: true
- name: log_max_recent
  type: int
  level: advanced
  desc: recent log entries to keep in memory to dump in the event of a crash
  long_desc: The purpose of this option is to log at a higher debug level only to
    the in-memory buffer, and write out the detailed log messages only if there is
    a crash.  Only log entries below the lower log level will be written unconditionally
    to the log.  For example, debug_osd=1/5 will write everything <= 1 to the log
    unconditionally but keep entries at levels 2-5 in memory.  If there is a seg fault
    or assertion failure, all entries will be dumped to the log.
  default: 500
  daemon_default: 10000
  # default changed by common_preinit()
  with_legacy: true
- name: log_to_file
  type: bool
  level: basic
  desc: send log lines to a file
  default: true
  see_also:
  - log_file
  with_legacy: true
- name: log_to_stderr
  type: bool
  level: basic
  desc: send log lines to stderr
  default: true
  daemon_default: false
  with_legacy: true
- name: err_to_stderr
  type: bool
  level: basic
  desc: send critical error log lines to stderr
  default: false
  daemon_default: true
  with_legacy: true
- name: log_stderr_prefix
  type: str
  level: advanced
  desc: String to prefix log messages with when sent to stderr
  long_desc: This is useful in container environments when combined with mon_cluster_log_to_stderr.  The
    mon log prefixes each line with the channel name (e.g., 'default', 'audit'), while
    log_stderr_prefix can be set to 'debug '.
  see_also:
  - mon_cluster_log_to_stderr
- name: log_to_syslog
  type: bool
  level: basic
  desc: send log lines to syslog facility
  default: false
  with_legacy: true
- name: err_to_syslog
  type: bool
  level: basic
  desc: send critical error log lines to syslog facility
  default: false
  with_legacy: true
- name: log_flush_on_exit
  type: bool
  level: advanced
  desc: set a process exit handler to ensure the log is flushed on exit
  default: false
  with_legacy: true
- name: log_stop_at_utilization
  type: float
  level: basic
  desc: stop writing to the log file when device utilization reaches this ratio
  default: 0.97
  see_also:
  - log_file
  min: 0
  max: 1
  with_legacy: true
- name: log_to_graylog
  type: bool
  level: basic
  desc: send log lines to remote graylog server
  default: false
  see_also:
  - err_to_graylog
  - log_graylog_host
  - log_graylog_port
  with_legacy: true
- name: err_to_graylog
  type: bool
  level: basic
  desc: send critical error log lines to remote graylog server
  default: false
  see_also:
  - log_to_graylog
  - log_graylog_host
  - log_graylog_port
  with_legacy: true
- name: log_graylog_host
  type: str
  level: basic
  desc: address or hostname of graylog server to log to
  default: 127.0.0.1
  see_also:
  - log_to_graylog
  - err_to_graylog
  - log_graylog_port
  with_legacy: true
- name: log_graylog_port
  type: int
  level: basic
  desc: port number for the remote graylog server
  default: 12201
  see_also:
  - log_graylog_host
  with_legacy: true
- name: log_to_journald
  type: bool
  level: basic
  desc: send log lines to journald
  default: false
  see_also:
  - err_to_journald
- name: err_to_journald
  type: bool
  level: basic
  desc: send critical error log lines to journald
  default: false
  see_also:
  - log_to_journald
- name: log_coarse_timestamps
  type: bool
  level: advanced
  desc: timestamp log entries from coarse system clock to improve performance
  default: true
  tags:
  - performance
  - service
  services:
  - common
# options will take k/v pairs, or single-item that will be assumed as general
# default for all, regardless of channel.
# e.g., "info" would be taken as the same as "default=info"
# also, "default=daemon audit=local0" would mean
#    "default all to 'daemon', override 'audit' with 'local0'
- name: clog_to_monitors
  type: str
  level: advanced
  desc: Make daemons send cluster log messages to monitors
  default: default=true
  flags:
  - runtime
  with_legacy: true
- name: clog_to_syslog
  type: str
  level: advanced
  desc: Make daemons send cluster log messages to syslog
  default: 'false'
  flags:
  - runtime
  with_legacy: true
- name: clog_to_syslog_level
  type: str
  level: advanced
  desc: Syslog level for cluster log messages
  default: info
  see_also:
  - clog_to_syslog
  flags:
  - runtime
  with_legacy: true
- name: clog_to_syslog_facility
  type: str
  level: advanced
  desc: Syslog facility for cluster log messages
  default: default=daemon audit=local0
  see_also:
  - clog_to_syslog
  flags:
  - runtime
  with_legacy: true
- name: clog_to_graylog
  type: str
  level: advanced
  desc: Make daemons send cluster log to graylog
  default: 'false'
  flags:
  - runtime
- name: clog_to_graylog_host
  type: str
  level: advanced
  desc: Graylog host to cluster log messages
  default: 127.0.0.1
  see_also:
  - clog_to_graylog
  flags:
  - runtime
  with_legacy: true
- name: clog_to_graylog_port
  type: str
  level: advanced
  desc: Graylog port number for cluster log messages
  default: '12201'
  see_also:
  - clog_to_graylog
  flags:
  - runtime
  with_legacy: true
- name: mon_cluster_log_to_stderr
  type: bool
  level: advanced
  desc: Make monitor send cluster log messages to stderr (prefixed by channel)
  default: false
  services:
  - mon
  see_also:
  - log_stderr_prefix
  flags:
  - runtime
  with_legacy: true
- name: mon_cluster_log_to_syslog
  type: str
  level: advanced
  desc: Make monitor send cluster log messages to syslog
  default: default=false
  services:
  - mon
  flags:
  - runtime
  with_legacy: true
- name: mon_cluster_log_to_syslog_level
  type: str
  level: advanced
  desc: Syslog level for cluster log messages
  default: info
  services:
  - mon
  see_also:
  - mon_cluster_log_to_syslog
  flags:
  - runtime
  with_legacy: true
- name: mon_cluster_log_to_syslog_facility
  type: str
  level: advanced
  desc: Syslog facility for cluster log messages
  default: daemon
  services:
  - mon
  see_also:
  - mon_cluster_log_to_syslog
  flags:
  - runtime
  with_legacy: true
- name: mon_cluster_log_to_file
  type: bool
  level: advanced
  desc: Make monitor send cluster log messages to file
  default: true
  services:
  - mon
  see_also:
  - mon_cluster_log_file
  flags:
  - runtime
  with_legacy: true
- name: mon_cluster_log_file
  type: str
  level: advanced
  desc: File(s) to write cluster log to
  long_desc: This can either be a simple file name to receive all messages, or a list
    of key/value pairs where the key is the log channel and the value is the filename,
    which may include $cluster and $channel metavariables
  default: default=/var/log/ceph/$cluster.$channel.log cluster=/var/log/ceph/$cluster.log
  services:
  - mon
  see_also:
  - mon_cluster_log_to_file
  flags:
  - runtime
  with_legacy: true
- name: mon_cluster_log_file_level
  type: str
  level: advanced
  desc: Lowest level to include is cluster log file
  default: debug
  services:
  - mon
  see_also:
  - mon_cluster_log_file
  flags:
  - runtime
  with_legacy: true
- name: mon_cluster_log_to_graylog
  type: str
  level: advanced
  desc: Make monitor send cluster log to graylog
  default: 'false'
  services:
  - mon
  flags:
  - runtime
  with_legacy: true
- name: mon_cluster_log_to_graylog_host
  type: str
  level: advanced
  desc: Graylog host for cluster log messages
  default: 127.0.0.1
  services:
  - mon
  see_also:
  - mon_cluster_log_to_graylog
  flags:
  - runtime
  with_legacy: true
- name: mon_cluster_log_to_graylog_port
  type: str
  level: advanced
  desc: Graylog port for cluster log messages
  default: '12201'
  services:
  - mon
  see_also:
  - mon_cluster_log_to_graylog
  flags:
  - runtime
  with_legacy: true
- name: mon_cluster_log_to_journald
  type: str
  level: advanced
  desc: Make monitor send cluster log to journald
  default: 'false'
  services:
  - mon
  flags:
  - runtime
- name: enable_experimental_unrecoverable_data_corrupting_features
  type: str
  level: advanced
  desc: Enable named (or all with '*') experimental features that may be untested,
    dangerous, and/or cause permanent data loss
  flags:
  - runtime
  with_legacy: true
- name: plugin_dir
  type: str
  level: advanced
  desc: Base directory for dynamically loaded plugins
  default: @CEPH_INSTALL_FULL_PKGLIBDIR@
  services:
  - mon
  - osd
  flags:
  - startup
- name: compressor_zlib_isal
  type: bool
  level: advanced
  desc: Use Intel ISA-L accelerated zlib implementation if available
  default: false
  with_legacy: true
# regular zlib compression level, not applicable to isa-l optimized version
- name: compressor_zlib_level
  type: int
  level: advanced
  desc: Zlib compression level to use
  default: 5
  with_legacy: true
# regular zlib compression winsize, not applicable to isa-l optimized version
- name: compressor_zlib_winsize
  type: int
  level: advanced
  desc: Zlib compression winsize to use
  default: -15
  min: -15
  max: 32
  with_legacy: true
# regular zstd compression level
- name: compressor_zstd_level
  type: int
  level: advanced
  desc: Zstd compression level to use
  default: 1
  with_legacy: true
- name: qat_compressor_enabled
  type: bool
  level: advanced
  desc: Enable Intel QAT acceleration support for compression if available
  default: false
  with_legacy: true
- name: plugin_crypto_accelerator
  type: str
  level: advanced
  desc: Crypto accelerator library to use
  default: crypto_isal
  with_legacy: true
- name: openssl_engine_opts
  type: str
  level: advanced
  desc: Use engine for specific openssl algorithm
  long_desc: 'Pass opts in this way: engine_id=engine1,dynamic_path=/some/path/engine1.so,default_algorithms=DIGESTS:engine_id=engine2,dynamic_path=/some/path/engine2.so,default_algorithms=CIPHERS,other_ctrl=other_value'
  flags:
  - startup
  with_legacy: true
- name: mempool_debug
  type: bool
  level: dev
  default: false
  flags:
  - no_mon_update
  with_legacy: true
- name: thp
  type: bool
  level: dev
  desc: enable transparent huge page (THP) support
  long_desc: Ceph is known to suffer from memory fragmentation due to THP use. This
    is indicated by RSS usage above configured memory targets. Enabling THP is currently
    discouraged until selective use of THP by Ceph is implemented.
  default: false
  flags:
  - startup
- name: key
  type: str
  level: advanced
  desc: Authentication key
  long_desc: A CephX authentication key, base64 encoded.  It normally looks something
    like 'AQAtut9ZdMbNJBAAHz6yBAWyJyz2yYRyeMWDag=='.
  fmt_desc: The key (i.e., the text string of the key itself). Not recommended.
  see_also:
  - keyfile
  - keyring
  flags:
  - no_mon_update
  - startup
  with_legacy: true
- name: keyfile
  type: str
  level: advanced
  desc: Path to a file containing a key
  long_desc: The file should contain a CephX authentication key and optionally a trailing
    newline, but nothing else.
  fmt_desc: The path to a key file (i.e,. a file containing only the key).
  see_also:
  - key
  flags:
  - no_mon_update
  - startup
  with_legacy: true
- name: keyring
  type: str
  level: advanced
  desc: Path to a keyring file.
  long_desc: A keyring file is an INI-style formatted file where the section names
    are client or daemon names (e.g., 'osd.0') and each section contains a 'key' property
    with CephX authentication key as the value.
  # please note, document are generated without accessing to the CMake
  # variables, so please update the document manually with a representive
  # default value using the ":default:" option of ".. confval::" directive.
  default: @keyring_paths@
  see_also:
  - key
  - keyfile
  flags:
  - no_mon_update
  - startup
  with_legacy: true
- name: heartbeat_interval
  type: int
  level: advanced
  desc: Frequency of internal heartbeat checks (seconds)
  default: 5
  flags:
  - startup
  with_legacy: true
- name: heartbeat_file
  type: str
  level: advanced
  desc: File to touch on successful internal heartbeat
  long_desc: If set, this file will be touched every time an internal heartbeat check
    succeeds.
  see_also:
  - heartbeat_interval
  flags:
  - startup
  with_legacy: true
- name: heartbeat_inject_failure
  type: int
  level: dev
  default: 0
  with_legacy: true
- name: perf
  type: bool
  level: advanced
  desc: Enable internal performance metrics
  long_desc: If enabled, collect and expose internal health metrics
  default: true
  with_legacy: true
- name: ms_type
  type: str
  level: advanced
  desc: Messenger implementation to use for network communication
  fmt_desc: Transport type used by Async Messenger. Can be ``async+posix``,
    ``async+dpdk`` or ``async+rdma``. Posix uses standard TCP/IP networking and is
    default. Other transports may be experimental and support may be limited.
  default: async+posix
  flags:
  - startup
  with_legacy: true
- name: ms_public_type
  type: str
  level: advanced
  desc: Messenger implementation to use for the public network
  long_desc: If not specified, use ms_type
  see_also:
  - ms_type
  flags:
  - startup
  with_legacy: true
- name: ms_cluster_type
  type: str
  level: advanced
  desc: Messenger implementation to use for the internal cluster network
  long_desc: If not specified, use ms_type
  see_also:
  - ms_type
  flags:
  - startup
  with_legacy: true
- name: ms_mon_cluster_mode
  type: str
  level: basic
  desc: Connection modes (crc, secure) for intra-mon connections in order of preference
  fmt_desc: the connection mode (or permitted modes) to use between monitors.
  default: secure crc
  see_also:
  - ms_mon_service_mode
  - ms_mon_client_mode
  - ms_service_mode
  - ms_cluster_mode
  - ms_client_mode
  flags:
  - startup
- name: ms_mon_service_mode
  type: str
  level: basic
  desc: Allowed connection modes (crc, secure) for connections to mons
  fmt_desc: a list of permitted modes for clients or
    other Ceph daemons to use when connecting to monitors.
  default: secure crc
  see_also:
  - ms_service_mode
  - ms_mon_cluster_mode
  - ms_mon_client_mode
  - ms_cluster_mode
  - ms_client_mode
  flags:
  - startup
- name: ms_mon_client_mode
  type: str
  level: basic
  desc: Connection modes (crc, secure) for connections from clients to monitors in
    order of preference
  fmt_desc: a list of connection modes, in order of
    preference, for clients or non-monitor daemons to use when
    connecting to monitors.
  default: secure crc
  see_also:
  - ms_mon_service_mode
  - ms_mon_cluster_mode
  - ms_service_mode
  - ms_cluster_mode
  - ms_client_mode
  flags:
  - startup
- name: ms_cluster_mode
  type: str
  level: basic
  desc: Connection modes (crc, secure) for intra-cluster connections in order of preference
  fmt_desc: connection mode (or permitted modes) used
    for intra-cluster communication between Ceph daemons.  If multiple
    modes are listed, the modes listed first are preferred.
  default: crc secure
  see_also:
  - ms_service_mode
  - ms_client_mode
  flags:
  - startup
- name: ms_service_mode
  type: str
  level: basic
  desc: Allowed connection modes (crc, secure) for connections to daemons
  fmt_desc: a list of permitted modes for clients to use
    when connecting to the cluster.
  default: crc secure
  see_also:
  - ms_cluster_mode
  - ms_client_mode
  flags:
  - startup
- name: ms_client_mode
  type: str
  level: basic
  desc: Connection modes (crc, secure) for connections from clients in order of preference
  fmt_desc: a list of connection modes, in order of
    preference, for clients to use (or allow) when talking to a Ceph
    cluster.
  default: crc secure
  see_also:
  - ms_cluster_mode
  - ms_service_mode
  flags:
  - startup
- name: ms_learn_addr_from_peer
  type: bool
  level: advanced
  desc: Learn address from what IP our first peer thinks we connect from
  long_desc: Use the IP address our first peer (usually a monitor) sees that we are
    connecting from.  This is useful if a client is behind some sort of NAT and we
    want to see it identified by its local (not NATed) address.
  default: true
  with_legacy: true
- name: ms_tcp_nodelay
  type: bool
  level: advanced
  desc: Disable Nagle's algorithm and send queued network traffic immediately
  fmt_desc: Ceph enables ``ms_tcp_nodelay`` so that each request is sent
   immediately (no buffering). Disabling `Nagle's algorithm`_
   increases network traffic, which can introduce latency. If you
   experience large numbers of small packets, you may try
   disabling ``ms_tcp_nodelay``.
  default: true
  with_legacy: true
- name: ms_tcp_rcvbuf
  type: size
  level: advanced
  desc: Size of TCP socket receive buffer
  fmt_desc: The size of the socket buffer on the receiving end of a network
   connection. Disable by default.
  default: 0
  with_legacy: true
- name: ms_tcp_prefetch_max_size
  type: size
  level: advanced
  desc: Maximum amount of data to prefetch out of the socket receive buffer
  default: 4_K
  with_legacy: true
- name: ms_initial_backoff
  type: float
  level: advanced
  desc: Initial backoff after a network error is detected (seconds)
  fmt_desc: The initial time to wait before reconnecting on a fault.
  default: 0.2
  with_legacy: true
- name: ms_max_backoff
  type: float
  level: advanced
  desc: Maximum backoff after a network error before retrying (seconds)
  fmt_desc: The maximum time to wait before reconnecting on a fault.
  default: 15
  see_also:
  - ms_initial_backoff
  with_legacy: true
- name: ms_crc_data
  type: bool
  level: dev
  desc: Set and/or verify crc32c checksum on data payload sent over network
  default: true
  with_legacy: true
- name: ms_crc_header
  type: bool
  level: dev
  desc: Set and/or verify crc32c checksum on header payload sent over network
  default: true
  with_legacy: true
- name: ms_die_on_bad_msg
  type: bool
  level: dev
  desc: Induce a daemon crash/exit when a bad network message is received
  fmt_desc: Debug option; do not configure.
  default: false
  with_legacy: true
- name: ms_die_on_unhandled_msg
  type: bool
  level: dev
  desc: Induce a daemon crash/exit when an unrecognized message is received
  default: false
  with_legacy: true
- name: ms_die_on_old_message
  type: bool
  level: dev
  desc: Induce a daemon crash/exit when a old, undecodable message is received
  default: false
  with_legacy: true
- name: ms_die_on_skipped_message
  type: bool
  level: dev
  desc: Induce a daemon crash/exit if sender skips a message sequence number
  default: false
  with_legacy: true
- name: ms_die_on_bug
  type: bool
  level: dev
  desc: Induce a crash/exit on various bugs (for testing purposes)
  default: false
  with_legacy: true
- name: ms_dispatch_throttle_bytes
  type: size
  level: advanced
  desc: Limit messages that are read off the network but still being processed
  fmt_desc: Throttles total size of messages waiting to be dispatched.
  default: 100_M
  with_legacy: true
- name: ms_bind_ipv4
  type: bool
  level: advanced
  desc: Bind servers to IPv4 address(es)
  fmt_desc: Enables Ceph daemons to bind to IPv4 addresses.
  default: true
  see_also:
  - ms_bind_ipv6
- name: ms_bind_ipv6
  type: bool
  level: advanced
  desc: Bind servers to IPv6 address(es)
  fmt_desc: Enables Ceph daemons to bind to IPv6 addresses.
  default: false
  see_also:
  - ms_bind_ipv4
  with_legacy: true
- name: ms_bind_prefer_ipv4
  type: bool
  level: advanced
  desc: Prefer IPV4 over IPV6 address(es)
  default: false
- name: ms_bind_msgr1
  type: bool
  level: advanced
  desc: Bind servers to msgr1 (legacy) protocol address(es)
  default: true
  see_also:
  - ms_bind_msgr2
- name: ms_bind_msgr2
  type: bool
  level: advanced
  desc: Bind servers to msgr2 (nautilus+) protocol address(es)
  default: true
  see_also:
  - ms_bind_msgr1
- name: ms_bind_port_min
  type: int
  level: advanced
  desc: Lowest port number to bind daemon(s) to
  fmt_desc: The minimum port number to which an OSD or MDS daemon will bind.
  default: 6800
  with_legacy: true
- name: ms_bind_port_max
  type: int
  level: advanced
  desc: Highest port number to bind daemon(s) to
  fmt_desc: The maximum port number to which an OSD or MDS daemon will bind.
  default: 7300
  with_legacy: true
# FreeBSD does not use SO_REAUSEADDR so allow for a bit more time per default
- name: ms_bind_retry_count
  type: int
  level: advanced
  desc: Number of attempts to make while bind(2)ing to a port
  default: @ms_bind_retry_count@
  with_legacy: true
# FreeBSD does not use SO_REAUSEADDR so allow for a bit more time per default
- name: ms_bind_retry_delay
  type: int
  level: advanced
  desc: Delay between bind(2) attempts (seconds)
  default: @ms_bind_retry_delay@
  with_legacy: true
- name: ms_bind_before_connect
  type: bool
  level: advanced
  desc: Call bind(2) on client sockets
  default: false
  with_legacy: true
- name: ms_tcp_listen_backlog
  type: int
  level: advanced
  desc: Size of queue of incoming connections for accept(2)
  default: 512
  with_legacy: true
- name: ms_connection_ready_timeout
  type: uint
  level: advanced
  desc: Time before we declare a not yet ready connection as dead (seconds)
  default: 10
  with_legacy: true
- name: ms_connection_idle_timeout
  type: uint
  level: advanced
  desc: Time before an idle connection is closed (seconds)
  default: 900
  with_legacy: true
- name: ms_pq_max_tokens_per_priority
  type: uint
  level: dev
  default: 16_M
  with_legacy: true
- name: ms_pq_min_cost
  type: size
  level: dev
  default: 64_K
  with_legacy: true
- name: ms_inject_socket_failures
  type: uint
  level: dev
  desc: Inject a socket failure every Nth socket operation
  fmt_desc: Debug option; do not configure.
  default: 0
  with_legacy: true
- name: ms_inject_delay_type
  type: str
  level: dev
  desc: Entity type to inject delays for
  flags:
  - runtime
  with_legacy: true
- name: ms_inject_delay_max
  type: float
  level: dev
  desc: Max delay to inject
  default: 1
  with_legacy: true
- name: ms_inject_delay_probability
  type: float
  level: dev
  default: 0
  with_legacy: true
- name: ms_inject_internal_delays
  type: float
  level: dev
  desc: Inject various internal delays to induce races (seconds)
  default: 0
  with_legacy: true
- name: ms_blackhole_osd
  type: bool
  level: dev
  default: false
  with_legacy: true
- name: ms_blackhole_mon
  type: bool
  level: dev
  default: false
  with_legacy: true
- name: ms_blackhole_mds
  type: bool
  level: dev
  default: false
  with_legacy: true
- name: ms_blackhole_mgr
  type: bool
  level: dev
  default: false
  with_legacy: true
- name: ms_blackhole_client
  type: bool
  level: dev
  default: false
  with_legacy: true
- name: ms_dump_on_send
  type: bool
  level: advanced
  desc: Hexdump message to debug log on message send
  default: false
  with_legacy: true
- name: ms_dump_corrupt_message_level
  type: int
  level: advanced
  desc: Log level at which to hexdump corrupt messages we receive
  default: 1
  with_legacy: true
# number of worker processing threads for async messenger created on init
- name: ms_async_op_threads
  type: uint
  level: advanced
  desc: Threadpool size for AsyncMessenger (ms_type=async)
  fmt_desc: Initial number of worker threads used by each Async Messenger instance.
    Should be at least equal to highest number of replicas, but you can
    decrease it if you are low on CPU core count and/or you host a lot of
    OSDs on single server.
  default: 3
  min: 1
  max: 24
  with_legacy: true
- name: ms_async_rdma_device_name
  type: str
  level: advanced
  with_legacy: true
- name: ms_async_rdma_enable_hugepage
  type: bool
  level: advanced
  default: false
  with_legacy: true
- name: ms_async_rdma_buffer_size
  type: size
  level: advanced
  default: 128_K
  with_legacy: true
- name: ms_async_rdma_send_buffers
  type: uint
  level: advanced
  default: 1_K
  with_legacy: true
# size of the receive buffer pool, 0 is unlimited
- name: ms_async_rdma_receive_buffers
  type: uint
  level: advanced
  default: 32_K
  with_legacy: true
# max number of wr in srq
- name: ms_async_rdma_receive_queue_len
  type: uint
  level: advanced
  default: 4_K
  with_legacy: true
# support srq
- name: ms_async_rdma_support_srq
  type: bool
  level: advanced
  default: true
  with_legacy: true
- name: ms_async_rdma_port_num
  type: uint
  level: advanced
  default: 1
  with_legacy: true
- name: ms_async_rdma_polling_us
  type: uint
  level: advanced
  default: 1000
  with_legacy: true
- name: ms_async_rdma_gid_idx
  type: int
  level: advanced
  desc: use gid_idx to select GID for choosing RoCEv1 or RoCEv2
  default: 0
  with_legacy: true
# GID format: "fe80:0000:0000:0000:7efe:90ff:fe72:6efe", no zero folding
- name: ms_async_rdma_local_gid
  type: str
  level: advanced
  with_legacy: true
# 0=RoCEv1, 1=RoCEv2, 2=RoCEv1.5
- name: ms_async_rdma_roce_ver
  type: int
  level: advanced
  default: 1
  with_legacy: true
# in RoCE, this means PCP
- name: ms_async_rdma_sl
  type: int
  level: advanced
  default: 3
  with_legacy: true
# in RoCE, this means DSCP
- name: ms_async_rdma_dscp
  type: int
  level: advanced
  default: 96
  with_legacy: true
# when there are enough accept failures, indicating there are unrecoverable failures,
# just do ceph_abort() . Here we make it configurable.
- name: ms_max_accept_failures
  type: int
  level: advanced
  desc: The maximum number of consecutive failed accept() calls before considering
    the daemon is misconfigured and abort it.
  default: 4
  with_legacy: true
# rdma connection management
- name: ms_async_rdma_cm
  type: bool
  level: advanced
  default: false
  with_legacy: true
- name: ms_async_rdma_type
  type: str
  level: advanced
  default: ib
  with_legacy: true
- name: ms_dpdk_port_id
  type: int
  level: advanced
  default: 0
  with_legacy: true
# it is modified in unittest so that use SAFE_OPTION to declare
- name: ms_dpdk_coremask
  type: str
  level: advanced
  default: '0xF'
  see_also:
  - ms_async_op_threads
  with_legacy: true
- name: ms_dpdk_memory_channel
  type: str
  level: advanced
  default: '4'
  with_legacy: true
- name: ms_dpdk_hugepages
  type: str
  level: advanced
  with_legacy: true
- name: ms_dpdk_pmd
  type: str
  level: advanced
  with_legacy: true
- name: ms_dpdk_host_ipv4_addr
  type: str
  level: advanced
  with_legacy: true
- name: ms_dpdk_gateway_ipv4_addr
  type: str
  level: advanced
  with_legacy: true
- name: ms_dpdk_netmask_ipv4_addr
  type: str
  level: advanced
  with_legacy: true
- name: ms_dpdk_lro
  type: bool
  level: advanced
  default: true
  with_legacy: true
- name: ms_dpdk_hw_flow_control
  type: bool
  level: advanced
  default: true
  with_legacy: true
# Weighing of a hardware network queue relative to a software queue (0=no work, 1=     equal share)")
- name: ms_dpdk_hw_queue_weight
  type: float
  level: advanced
  default: 1
  with_legacy: true
- name: ms_dpdk_debug_allow_loopback
  type: bool
  level: dev
  default: false
  with_legacy: true
- name: ms_dpdk_rx_buffer_count_per_core
  type: int
  level: advanced
  default: 8192
  with_legacy: true
- name: inject_early_sigterm
  type: bool
  level: dev
  desc: send ourselves a SIGTERM early during startup
  default: false
  with_legacy: true
- name: mon_enable_op_tracker
  type: bool
  level: advanced
  desc: enable/disable MON op tracking
  default: true
  services:
  - mon
- name: mon_op_complaint_time
  type: secs
  level: advanced
  desc: time after which to consider a monitor operation blocked after no updates
  default: 30
  services:
  - mon
- name: mon_op_log_threshold
  type: int
  level: advanced
  desc: max number of slow ops to display
  default: 5
  services:
  - mon
- name: mon_op_history_size
  type: uint
  level: advanced
  desc: max number of completed ops to track
  default: 20
  services:
  - mon
- name: mon_op_history_duration
  type: secs
  level: advanced
  desc: expiration time in seconds of historical MON OPS
  default: 10_min
  services:
  - mon
- name: mon_op_history_slow_op_size
  type: uint
  level: advanced
  desc: max number of slow historical MON OPS to keep
  default: 20
  services:
  - mon
- name: mon_op_history_slow_op_threshold
  type: secs
  level: advanced
  desc: duration of an op to be considered as a historical slow op
  default: 10
  services:
  - mon
- name: mon_data
  type: str
  level: advanced
  desc: path to mon database
  fmt_desc: The monitor's data location.
  default: /var/lib/ceph/mon/$cluster-$id
  services:
  - mon
  flags:
  - no_mon_update
  with_legacy: true
# list of initial cluster mon ids; if specified, need majority to form initial quorum and create new cluster
- name: mon_initial_members
  type: str
  level: advanced
  fmt_desc: The IDs of initial monitors in a cluster during startup. If 
    specified, Ceph requires an odd number of monitors to form an 
    initial quorum (e.g., 3).
  note: A *majority* of monitors in your cluster must be able to reach 
    each other in order to establish a quorum. You can decrease the initial 
    number of monitors to establish a quorum with this setting.
  services:
  - mon
  flags:
  - no_mon_update
  - cluster_create
  with_legacy: true
# compact leveldb on ceph-mon start
- name: mon_compact_on_start
  type: bool
  level: advanced
  default: false
  services:
  - mon
  fmt_desc: Compact the database used as Ceph Monitor store on
    ``ceph-mon`` start. A manual compaction helps to shrink the
    monitor database and improve the performance of it if the regular
    compaction fails to work.
  with_legacy: true
# trigger leveldb compaction on bootstrap
- name: mon_compact_on_bootstrap
  type: bool
  level: advanced
  default: false
  services:
  - mon
  fmt_desc: Compact the database used as Ceph Monitor store
    on bootstrap. Monitors probe each other to establish
    a quorum after bootstrap. If a monitor times out before joining the
    quorum, it will start over and bootstrap again.
  with_legacy: true
# compact (a prefix) when we trim old states
- name: mon_compact_on_trim
  type: bool
  level: advanced
  default: true
  services:
  - mon
  fmt_desc: Compact a certain prefix (including paxos) when we trim its old states.
  with_legacy: true
- name: mon_osdmap_full_prune_enabled
  type: bool
  level: advanced
  desc: enables pruning full osdmap versions when we go over a given number of maps
  default: true
  services:
  - mon
  see_also:
  - mon_osdmap_full_prune_min
  - mon_osdmap_full_prune_interval
  - mon_osdmap_full_prune_txsize
- name: mon_osdmap_full_prune_min
  type: uint
  level: advanced
  desc: minimum number of versions in the store to trigger full map pruning
  default: 10000
  services:
  - mon
  see_also:
  - mon_osdmap_full_prune_enabled
  - mon_osdmap_full_prune_interval
  - mon_osdmap_full_prune_txsize
- name: mon_osdmap_full_prune_interval
  type: uint
  level: advanced
  desc: interval between maps that will not be pruned; maps in the middle will be
    pruned.
  default: 10
  services:
  - mon
  see_also:
  - mon_osdmap_full_prune_enabled
  - mon_osdmap_full_prune_interval
  - mon_osdmap_full_prune_txsize
- name: mon_osdmap_full_prune_txsize
  type: uint
  level: advanced
  desc: number of maps we will prune per iteration
  default: 100
  services:
  - mon
  see_also:
  - mon_osdmap_full_prune_enabled
  - mon_osdmap_full_prune_interval
  - mon_osdmap_full_prune_txsize
- name: mon_osd_cache_size
  type: int
  level: advanced
  desc: maximum number of OSDMaps to cache in memory
  fmt_desc: The size of osdmaps cache, not to rely on underlying store's cache
  default: 500
  services:
  - mon
  with_legacy: true
- name: mon_osd_cache_size_min
  type: size
  level: advanced
  desc: The minimum amount of bytes to be kept mapped in memory for osd monitor caches.
  fmt_desc: The minimum amount of bytes to be kept mapped in memory for osd
     monitor caches.
  default: 128_M
  services:
  - mon
  with_legacy: true
- name: mon_memory_target
  type: size
  level: basic
  desc: The amount of bytes pertaining to osd monitor caches and kv cache to be kept
    mapped in memory with cache auto-tuning enabled
  fmt_desc: The amount of bytes pertaining to OSD monitor caches and KV cache
    to be kept mapped in memory with cache auto-tuning enabled.
  default: 2_G
  services:
  - mon
  flags:
  - runtime
  with_legacy: true
- name: mon_memory_autotune
  type: bool
  level: basic
  desc: Autotune the cache memory being used for osd monitors and kv database
  fmt_desc: Autotune the cache memory used for OSD monitors and KV
    database.
  default: true
  services:
  - mon
  flags:
  - runtime
  with_legacy: true
- name: mon_cpu_threads
  type: int
  level: advanced
  desc: worker threads for CPU intensive background work
  fmt_desc: Number of threads for performing CPU intensive work on monitor.
  default: 4
  services:
  - mon
  with_legacy: true
- name: mon_osd_mapping_pgs_per_chunk
  type: int
  level: dev
  desc: granularity of PG placement calculation background work
  fmt_desc: We calculate the mapping from placement group to OSDs in chunks.
    This option specifies the number of placement groups per chunk.
  default: 4096
  services:
  - mon
  with_legacy: true
- name: mon_clean_pg_upmaps_per_chunk
  type: uint
  level: dev
  desc: granularity of PG upmap validation background work
  default: 256
  services:
  - mon
  with_legacy: true
- name: mon_osd_max_creating_pgs
  type: int
  level: advanced
  desc: maximum number of PGs the mon will create at once
  default: 1024
  services:
  - mon
  with_legacy: true
- name: mon_osd_max_initial_pgs
  type: int
  level: advanced
  desc: maximum number of PGs a pool will created with
  long_desc: If the user specifies more PGs than this, the cluster will subsequently
    split PGs after the pool is created in order to reach the target.
  default: 1024
  services:
  - mon
- name: mon_tick_interval
  type: int
  level: advanced
  desc: interval for internal mon background checks
  fmt_desc: A monitor's tick interval in seconds.
  default: 5
  services:
  - mon
  with_legacy: true
- name: mon_session_timeout
  type: int
  level: advanced
  desc: close inactive mon client connections after this many seconds
  fmt_desc: Monitor will terminate inactive sessions stay idle over this
    time limit.
  default: 5_min
  services:
  - mon
  with_legacy: true
- name: mon_subscribe_interval
  type: float
  level: dev
  desc: subscribe interval for pre-jewel clients
  fmt_desc: The refresh interval (in seconds) for subscriptions. The
    subscription mechanism enables obtaining cluster maps
    and log information.
  default: 1_day
  services:
  - mon
  with_legacy: true
- name: mon_delta_reset_interval
  type: float
  level: advanced
  desc: window duration for rate calculations in 'ceph status'
  fmt_desc: Seconds of inactivity before we reset the PG delta to 0. We keep
    track of the delta of the used space of each pool, so, for
    example, it would be easier for us to understand the progress of
    recovery or the performance of cache tier. But if there's no
    activity reported for a certain pool, we just reset the history of
    deltas of that pool.
  default: 10
  services:
  - mon
  with_legacy: true
- name: mon_osd_laggy_halflife
  type: int
  level: advanced
  desc: halflife of OSD 'lagginess' factor
  fmt_desc: The number of seconds laggy estimates will decay.
  default: 1_hr
  services:
  - mon
  with_legacy: true
- name: mon_osd_laggy_weight
  type: float
  level: advanced
  desc: how heavily to weight OSD marking itself back up in overall laggy_probability
  long_desc: 1.0 means that an OSD marking itself back up (because it was marked down
    but not actually dead) means a 100% laggy_probability; 0.0 effectively disables
    tracking of laggy_probability.
  fmt_desc: The weight for new samples in laggy estimation decay.
  default: 0.3
  services:
  - mon
  min: 0
  max: 1
  with_legacy: true
- name: mon_osd_laggy_max_interval
  type: int
  level: advanced
  desc: cap value for period for OSD to be marked for laggy_interval calculation
  fmt_desc: Maximum value of ``laggy_interval`` in laggy estimations (in seconds).
              Monitor uses an adaptive approach to evaluate the ``laggy_interval`` of
              a certain OSD. This value will be used to calculate the grace time for
              that OSD.
  default: 5_min
  services:
  - mon
  with_legacy: true
- name: mon_osd_adjust_heartbeat_grace
  type: bool
  level: advanced
  desc: increase OSD heartbeat grace if peers appear to be laggy
  long_desc: If an OSD is marked down but then marks itself back up, it implies it
    wasn't actually down but was unable to respond to heartbeats.  If this option
    is true, we can use the laggy_probability and laggy_interval values calculated
    to model this situation to increase the heartbeat grace period for this OSD so
    that it isn't marked down again.  laggy_probability is an estimated probability
    that the given OSD is down because it is laggy (not actually down), and laggy_interval
    is an estiate on how long it stays down when it is laggy.
  fmt_desc: If set to ``true``, Ceph will scale based on laggy estimations.
  default: true
  services:
  - mon
  see_also:
  - mon_osd_laggy_halflife
  - mon_osd_laggy_weight
  - mon_osd_laggy_max_interval
  with_legacy: true
- name: mon_osd_adjust_down_out_interval
  type: bool
  level: advanced
  desc: increase the mon_osd_down_out_interval if an OSD appears to be laggy
  fmt_desc: If set to ``true``, Ceph will scaled based on laggy estimations.
  default: true
  services:
  - mon
  see_also:
  - mon_osd_adjust_heartbeat_grace
  with_legacy: true
- name: mon_osd_auto_mark_in
  type: bool
  level: advanced
  desc: mark any OSD that comes up 'in'
  fmt_desc: Ceph will mark any booting Ceph OSD Daemons as ``in``
              the Ceph Storage Cluster.
  default: false
  services:
  - mon
  with_legacy: true
- name: mon_osd_auto_mark_auto_out_in
  type: bool
  level: advanced
  desc: mark any OSD that comes up that was automatically marked 'out' back 'in'
  fmt_desc: Ceph will mark booting Ceph OSD Daemons auto marked ``out``
              of the Ceph Storage Cluster as ``in`` the cluster.
  default: true
  services:
  - mon
  see_also:
  - mon_osd_down_out_interval
  with_legacy: true
- name: mon_osd_auto_mark_new_in
  type: bool
  level: advanced
  desc: mark any new OSD that comes up 'in'
  fmt_desc: Ceph will mark booting new Ceph OSD Daemons as ``in`` the
              Ceph Storage Cluster.
  default: true
  services:
  - mon
  with_legacy: true
- name: mon_osd_destroyed_out_interval
  type: int
  level: advanced
  desc: mark any OSD 'out' that has been 'destroy'ed for this long (seconds)
  default: 10_min
  services:
  - mon
  with_legacy: true
- name: mon_osd_down_out_interval
  type: int
  level: advanced
  desc: mark any OSD 'out' that has been 'down' for this long (seconds)
  fmt_desc: The number of seconds Ceph waits before marking a Ceph OSD Daemon
              ``down`` and ``out`` if it doesn't respond.
  default: 10_min
  services:
  - mon
  with_legacy: true
- name: mon_osd_down_out_subtree_limit
  type: str
  level: advanced
  desc: do not automatically mark OSDs 'out' if an entire subtree of this size is
    down
  fmt_desc: The smallest :term:`CRUSH` unit type that Ceph will **not**
              automatically mark out. For instance, if set to ``host`` and if
              all OSDs of a host are down, Ceph will not automatically mark out
              these OSDs.
  default: rack
  services:
  - mon
  see_also:
  - mon_osd_down_out_interval
  flags:
  - runtime
- name: mon_osd_min_up_ratio
  type: float
  level: advanced
  desc: do not automatically mark OSDs 'out' if fewer than this many OSDs are 'up'
  fmt_desc: The minimum ratio of ``up`` Ceph OSD Daemons before Ceph will
              mark Ceph OSD Daemons ``down``.
  default: 0.3
  services:
  - mon
  see_also:
  - mon_osd_down_out_interval
  with_legacy: true
- name: mon_osd_min_in_ratio
  type: float
  level: advanced
  desc: do not automatically mark OSDs 'out' if fewer than this many OSDs are 'in'
  fmt_desc: The minimum ratio of ``in`` Ceph OSD Daemons before Ceph will
              mark Ceph OSD Daemons ``out``.
  default: 0.75
  services:
  - mon
  see_also:
  - mon_osd_down_out_interval
  with_legacy: true
- name: mon_osd_warn_op_age
  type: float
  level: advanced
  desc: issue REQUEST_SLOW health warning if OSD ops are slower than this age (seconds)
  default: 32
  services:
  - mgr
  with_legacy: true
- name: mon_osd_warn_num_repaired
  type: uint
  level: advanced
  desc: issue OSD_TOO_MANY_REPAIRS health warning if an OSD has more than this many
    read repairs
  default: 10
  services:
  - mon
- name: mon_osd_err_op_age_ratio
  type: float
  level: advanced
  desc: issue REQUEST_STUCK health error if OSD ops are slower than is age (seconds)
  default: 128
  services:
  - mgr
  with_legacy: true
- name: mon_osd_prime_pg_temp
  type: bool
  level: dev
  desc: minimize peering work by priming pg_temp values after a map change
  fmt_desc: Enables or disables priming the PGMap with the previous OSDs when an ``out``
    OSD comes back into the cluster. With the ``true`` setting, clients
    will continue to use the previous OSDs until the newly ``in`` OSDs for
    a PG have peered.
  default: true
  services:
  - mon
  with_legacy: true
- name: mon_osd_prime_pg_temp_max_time
  type: float
  level: dev
  desc: maximum time to spend precalculating PG mappings on map change (seconds)
  fmt_desc: How much time in seconds the monitor should spend trying to prime the
    PGMap when an out OSD comes back into the cluster.
  default: 0.5
  services:
  - mon
  with_legacy: true
- name: mon_osd_prime_pg_temp_max_estimate
  type: float
  level: advanced
  desc: calculate all PG mappings if estimated fraction of PGs that change is above
    this amount
  fmt_desc: Maximum estimate of time spent on each PG before we prime all PGs
    in parallel.
  default: 0.25
  services:
  - mon
  with_legacy: true
- name: mon_stat_smooth_intervals
  type: uint
  level: advanced
  desc: number of PGMaps stats over which we calc the average read/write throughput
    of the whole cluster
  fmt_desc: Ceph will smooth statistics over the last ``N`` PG maps.
  default: 6
  services:
  - mgr
  min: 1
- name: mon_election_timeout
  type: float
  level: advanced
  desc: maximum time for a mon election (seconds)
  fmt_desc: On election proposer, maximum waiting time for all ACKs in seconds.
  default: 5
  services:
  - mon
  with_legacy: true
- name: mon_election_default_strategy
  type: uint
  level: advanced
  desc: The election strategy to set when constructing the first monmap.
  default: 1
  min: 1
  max: 3
- name: mon_lease
  type: float
  level: advanced
  desc: lease interval between quorum monitors (seconds)
  long_desc: This setting controls how sensitive your mon quorum is to intermittent
    network issues or other failures.
  fmt_desc: The length (in seconds) of the lease on the monitor's versions.
  default: 5
  services:
  - mon
  with_legacy: true
- name: mon_lease_renew_interval_factor
  type: float
  level: advanced
  desc: multiple of mon_lease for the lease renewal interval
  long_desc: Leases must be renewed before they time out.  A smaller value means frequent
    renewals, while a value close to 1 makes a lease expiration more likely.
  fmt_desc: |
    ``mon_lease`` \* ``mon_lease_renew_interval_factor`` will be the
    interval for the Leader to renew the other monitor's leases. The
    factor should be less than ``1.0``.
  default: 0.6
  services:
  - mon
  see_also:
  - mon_lease
  min: 0
  max: 0.9999999
  with_legacy: true
- name: mon_lease_ack_timeout_factor
  type: float
  level: advanced
  desc: multiple of mon_lease for the lease ack interval before calling new election
  fmt_desc: The Leader will wait ``mon_lease`` \* ``mon_lease_ack_timeout_factor``
    for the Providers to acknowledge the lease extension.
  default: 2
  services:
  - mon
  see_also:
  - mon_lease
  min: 1.0001
  max: 100
  with_legacy: true
- name: mon_accept_timeout_factor
  type: float
  level: advanced
  desc: multiple of mon_lease for follower mons to accept proposed state changes before
    calling a new election
  fmt_desc: The Leader will wait ``mon_lease`` \* ``mon_accept_timeout_factor``
    for the Requester(s) to accept a Paxos update. It is also used
    during the Paxos recovery phase for similar purposes.
  default: 2
  services:
  - mon
  see_also:
  - mon_lease
  with_legacy: true
- name: mon_elector_ping_timeout
  type: float
  level: advanced
  desc: The time after which a ping 'times out' and a connection is considered down
  default: 2
  services:
  - mon
  see_also:
  - mon_elector_ping_divisor
- name: mon_elector_ping_divisor
  type: uint
  level: advanced
  desc: We will send a ping up to this many times per timeout per
  default: 2
  services:
  - mon
  see_also:
  - mon_elector_ping_timeout
- name: mon_con_tracker_persist_interval
  type: uint
  level: advanced
  desc: how many updates the ConnectionTracker takes before it persists to disk
  default: 10
  services:
  - mon
  min: 1
  max: 100000
- name: mon_con_tracker_score_halflife
  type: uint
  level: advanced
  desc: The 'halflife' used when updating/calculating peer connection scores
  default: 43200
  services:
  - mon
  min: 60
- name: mon_elector_ignore_propose_margin
  type: float
  level: advanced
  desc: The difference in connection score allowed before a peon stops ignoring out-of-quorum
    PROPOSEs
  default: 0.0005
  services:
  - mon
- name: mon_warn_on_degraded_stretch_mode
  type: bool
  level: advanced
  desc: Issue a health warning if we are in degraded stretch mode
  default: true
  services:
  - mon
- name: mon_stretch_cluster_recovery_ratio
  type: float
  level: advanced
  desc: the ratio of up OSDs at which a degraded stretch cluster enters recovery
  default: 0.6
  services:
  - mon
  min: 0.51
  max: 1
- name: mon_stretch_recovery_min_wait
  type: float
  level: advanced
  desc: how long the monitors wait before considering fully-healthy PGs as evidence
    the stretch mode is repaired
  default: 15
  services:
  - mon
  min: 1
- name: mon_stretch_pool_size
  type: uint
  level: dev
  default: 4
  services:
  - mon
  min: 3
  max: 6
- name: mon_stretch_pool_min_size
  type: uint
  level: dev
  default: 2
  services:
  - mon
  min: 2
  max: 4
- name: mon_clock_drift_allowed
  type: float
  level: advanced
  desc: allowed clock drift (in seconds) between mons before issuing a health warning
  default: 0.05
  services:
  - mon
  with_legacy: true
# exponential backoff for clock drift warnings
- name: mon_clock_drift_warn_backoff
  type: float
  level: advanced
  desc: exponential backoff factor for logging clock drift warnings in the cluster
    log
  default: 5
  services:
  - mon
  with_legacy: true
# on leader, timecheck (clock drift check) interval (seconds)
- name: mon_timecheck_interval
  type: float
  level: advanced
  desc: frequency of clock synchronization checks between monitors (seconds)
  fmt_desc: The time check interval (clock drift check) in seconds
    for the Leader.
  default: 5_min
  services:
  - mon
  with_legacy: true
# on leader, timecheck (clock drift check) interval when in presence of a skew (seconds)
- name: mon_timecheck_skew_interval
  type: float
  level: advanced
  desc: frequency of clock synchronization (re)checks between monitors while clocks
    are believed to be skewed (seconds)
  fmt_desc: The time check interval (clock drift check) in seconds when in
    presence of a skew in seconds for the Leader.
  default: 30
  services:
  - mon
  see_also:
  - mon_timecheck_interval
  with_legacy: true
- name: mon_pg_stuck_threshold
  type: int
  level: advanced
  desc: number of seconds after which pgs can be considered stuck inactive, unclean,
    etc
  long_desc: see doc/control.rst under dump_stuck for more info
  fmt_desc: Number of seconds after which PGs can be considered as
    being stuck.
  default: 1_min
  services:
  - mgr
- name: mon_pg_warn_min_per_osd
  type: uint
  level: advanced
  desc: minimal number PGs per (in) osd before we warn the admin
  fmt_desc: Raise ``HEALTH_WARN`` if the average number
    of PGs per ``in`` OSD is under this number. A non-positive number
    disables this.
  default: 0
  services:
  - mgr
- name: mon_max_pg_per_osd
  type: uint
  level: advanced
  desc: Max number of PGs per OSD the cluster will allow
  long_desc: If the number of PGs per OSD exceeds this, a health warning will be visible
    in `ceph status`.  This is also used in automated PG management, as the threshold
    at which some pools' pg_num may be shrunk in order to enable increasing the pg_num
    of others.
  default: 250
  services:
  - mgr
  min: 1
- name: mon_target_pg_per_osd
  type: uint
  level: advanced
  desc: Automated PG management creates this many PGs per OSD
  long_desc: When creating pools, the automated PG management logic will attempt to
    reach this target.  In some circumstances, it may exceed this target, up to the
    ``mon_max_pg_per_osd`` limit. Conversely, a lower number of PGs per OSD may be
    created if the cluster is not yet fully utilised
  default: 100
  min: 1
- name: mon_pg_warn_max_object_skew
  type: float
  level: advanced
  desc: max skew few average in objects per pg
  fmt_desc: Raise ``HEALTH_WARN`` if the average RADOS object count per PG
    of any pool is greater than ``mon_pg_warn_max_object_skew`` times
    the average RADOS object count per PG of all pools. Zero or a non-positive
    number disables this. Note that this option applies to ``ceph-mgr`` daemons.
  default: 10
  services:
  - mgr
- name: mon_pg_warn_min_objects
  type: int
  level: advanced
  desc: 'do not warn below this object #'
  fmt_desc: Do not warn if the total number of RADOS objects in cluster is below
    this number
  default: 10000
  services:
  - mgr
- name: mon_pg_warn_min_pool_objects
  type: int
  level: advanced
  desc: 'do not warn on pools below this object #'
  fmt_desc: Do not warn on pools whose RADOS object count is below this number
  default: 1000
  services:
  - mgr
- name: mon_pg_check_down_all_threshold
  type: float
  level: advanced
  desc: threshold of down osds after which we check all pgs
  fmt_desc: Percentage threshold of ``down`` OSDs above which we check all PGs
    for stale ones.
  default: 0.5
  services:
  - mgr
  with_legacy: true
- name: mon_cache_target_full_warn_ratio
  type: float
  level: advanced
  desc: issue CACHE_POOL_NEAR_FULL health warning when cache pool utilization exceeds
    this ratio of usable space
  fmt_desc: Position between pool's ``cache_target_full`` and ``target_max_object``
    where we start warning
  default: 0.66
  services:
  - mgr
  flags:
  - no_mon_update
  - cluster_create
  with_legacy: true
- name: mon_osd_full_ratio
  type: float
  level: advanced
  desc: full ratio of OSDs to be set during initial creation of the cluster
  default: 0.95
  flags:
  - no_mon_update
  - cluster_create
  with_legacy: true
- name: mon_osd_backfillfull_ratio
  type: float
  level: advanced
  default: 0.9
  flags:
  - no_mon_update
  - cluster_create
  with_legacy: true
- name: mon_osd_nearfull_ratio
  type: float
  level: advanced
  desc: nearfull ratio for OSDs to be set during initial creation of cluster
  default: 0.85
  flags:
  - no_mon_update
  - cluster_create
  with_legacy: true
- name: mon_osd_initial_require_min_compat_client
  type: str
  level: advanced
  default: luminous
  flags:
  - no_mon_update
  - cluster_create
  with_legacy: true
- name: mon_allow_pool_delete
  type: bool
  level: advanced
  desc: allow pool deletions
  fmt_desc: Should monitors allow pools to be removed, regardless of what the pool flags say?
  default: false
  services:
  - mon
  with_legacy: true
- name: mon_fake_pool_delete
  type: bool
  level: advanced
  desc: fake pool deletions by renaming the rados pool
  default: false
  services:
  - mon
  with_legacy: true
- name: mon_globalid_prealloc
  type: uint
  level: advanced
  desc: number of globalid values to preallocate
  long_desc: This setting caps how many new clients can authenticate with the cluster
    before the monitors have to perform a write to preallocate more.  Large values
    burn through the 64-bit ID space more quickly.
  fmt_desc: The number of global IDs to pre-allocate for clients and daemons in the cluster.
  default: 10000
  services:
  - mon
  with_legacy: true
- name: mon_osd_report_timeout
  type: int
  level: advanced
  desc: time before OSDs who do not report to the mons are marked down (seconds)
  fmt_desc: The grace period in seconds before declaring
              unresponsive Ceph OSD Daemons ``down``.
  default: 15_min
  services:
  - mon
  with_legacy: true
- name: mon_warn_on_insecure_global_id_reclaim
  type: bool
  level: advanced
  desc: issue AUTH_INSECURE_GLOBAL_ID_RECLAIM health warning if any connected
    clients are insecurely reclaiming global_id
  default: true
  services:
  - mon
  see_also:
  - mon_warn_on_insecure_global_id_reclaim_allowed
  - auth_allow_insecure_global_id_reclaim
  - auth_expose_insecure_global_id_reclaim
- name: mon_warn_on_insecure_global_id_reclaim_allowed
  type: bool
  level: advanced
  desc: issue AUTH_INSECURE_GLOBAL_ID_RECLAIM_ALLOWED health warning if insecure
    global_id reclaim is allowed
  default: true
  services:
  - mon
  see_also:
  - mon_warn_on_insecure_global_id_reclaim
  - auth_allow_insecure_global_id_reclaim
  - auth_expose_insecure_global_id_reclaim
- name: mon_warn_on_msgr2_not_enabled
  type: bool
  level: advanced
  desc: issue MON_MSGR2_NOT_ENABLED health warning if monitors are all running Nautilus
    but not all binding to a msgr2 port
  default: true
  services:
  - mon
  see_also:
  - ms_bind_msgr2
- name: mon_warn_on_legacy_crush_tunables
  type: bool
  level: advanced
  desc: issue OLD_CRUSH_TUNABLES health warning if CRUSH tunables are older than mon_crush_min_required_version
  fmt_desc: Raise ``HEALTH_WARN`` when CRUSH tunables are too old (older than ``mon_min_crush_required_version``)
  default: true
  services:
  - mgr
  see_also:
  - mon_crush_min_required_version
  with_legacy: true
- name: mon_crush_min_required_version
  type: str
  level: advanced
  desc: minimum ceph release to use for mon_warn_on_legacy_crush_tunables
  fmt_desc: The minimum tunable profile required by the cluster. See
    :ref:`CRUSH map tunables <crush-map-tunables>` for details.
  default: hammer
  services:
  - mgr
  see_also:
  - mon_warn_on_legacy_crush_tunables
  with_legacy: true
- name: mon_warn_on_crush_straw_calc_version_zero
  type: bool
  level: advanced
  desc: issue OLD_CRUSH_STRAW_CALC_VERSION health warning if the CRUSH map's straw_calc_version
    is zero
  fmt_desc: Raise ``HEALTH_WARN`` when the CRUSH ``straw_calc_version`` is zero. See
    :ref:`CRUSH map tunables <crush-map-tunables>` for details.
  default: true
  services:
  - mgr
  with_legacy: true
- name: mon_warn_on_osd_down_out_interval_zero
  type: bool
  level: advanced
  desc: issue OSD_NO_DOWN_OUT_INTERVAL health warning if mon_osd_down_out_interval
    is zero
  long_desc: Having mon_osd_down_out_interval set to 0 means that down OSDs are not
    marked out automatically and the cluster does not heal itself without administrator
    intervention.
  fmt_desc: Raise ``HEALTH_WARN`` when ``mon_osd_down_out_interval`` is zero. Having this
    option set to zero on the leader acts much like the ``noout`` flag. It's hard to figure
    out what's going wrong with clusters without the ``noout`` flag set but acting like that
    just the same, so we report a warning in this case.
  default: true
  services:
  - mgr
  see_also:
  - mon_osd_down_out_interval
  with_legacy: true
- name: mon_warn_on_cache_pools_without_hit_sets
  type: bool
  level: advanced
  desc: issue CACHE_POOL_NO_HIT_SET health warning for cache pools that do not have
    hit sets configured
  fmt_desc: Raise ``HEALTH_WARN`` when a cache pool does not have the ``hit_set_type``
    value configured. See :ref:`hit_set_type <hit_set_type>` for more details.
  default: true
  services:
  - mgr
  with_legacy: true
- name: mon_warn_on_pool_no_app
  type: bool
  level: dev
  desc: issue POOL_APP_NOT_ENABLED health warning if pool has not application enabled
  default: true
  services:
  - mgr
- name: mon_warn_on_pool_pg_num_not_power_of_two
  type: bool
  level: dev
  desc: issue POOL_PG_NUM_NOT_POWER_OF_TWO warning if pool has a non-power-of-two
    pg_num value
  default: true
  services:
  - mon
- name: mon_warn_on_pool_no_redundancy
  type: bool
  level: advanced
  desc: Issue a health warning if any pool is configured with no replicas
  fmt_desc: Raise ``HEALTH_WARN`` if any pool is configured with no replicas.
  default: true
  services:
  - mon
  see_also:
  - osd_pool_default_size
  - osd_pool_default_min_size
- name: mon_allow_pool_size_one
  type: bool
  level: advanced
  desc: allow configuring pool with no replicas
  default: false
  services:
  - mon
- name: mon_warn_on_misplaced
  type: bool
  level: advanced
  desc: Issue a health warning if there are misplaced objects
  default: false
  services:
  - mgr
  with_legacy: true
- name: mon_warn_on_too_few_osds
  type: bool
  level: advanced
  desc: Issue a health warning if there are fewer OSDs than osd_pool_default_size
  default: true
  services:
  - mgr
- name: mon_warn_on_slow_ping_time
  type: float
  level: advanced
  desc: Override mon_warn_on_slow_ping_ratio with specified threshold in milliseconds
  fmt_desc: Override ``mon_warn_on_slow_ping_ratio`` with a specific value.
    Raise ``HEALTH_WARN`` if any heartbeat between OSDs exceeds
    ``mon_warn_on_slow_ping_time`` milliseconds.  The default is 0 (disabled).
  default: 0
  services:
  - mgr
  see_also:
  - mon_warn_on_slow_ping_ratio
- name: mon_warn_on_slow_ping_ratio
  type: float
  level: advanced
  desc: Issue a health warning if heartbeat ping longer than percentage of osd_heartbeat_grace
  fmt_desc: Raise ``HEALTH_WARN`` when any heartbeat between OSDs exceeds
    ``mon_warn_on_slow_ping_ratio`` of ``osd_heartbeat_grace``.
  default: 0.05
  services:
  - mgr
  see_also:
  - osd_heartbeat_grace
  - mon_warn_on_slow_ping_time
- name: mon_max_snap_prune_per_epoch
  type: uint
  level: advanced
  desc: max number of pruned snaps we will process in a single OSDMap epoch
  default: 100
  services:
  - mon
- name: mon_min_osdmap_epochs
  type: int
  level: advanced
  desc: min number of OSDMaps to store
  fmt_desc: Minimum number of OSD map epochs to keep at all times.
  default: 500
  services:
  - mon
  with_legacy: true
- name: mon_max_log_epochs
  type: int
  level: advanced
  desc: max number of past cluster log epochs to store
  fmt_desc: Maximum number of Log epochs the monitor should keep.
  default: 500
  services:
  - mon
  with_legacy: true
- name: mon_max_mdsmap_epochs
  type: int
  level: advanced
  desc: max number of FSMaps/MDSMaps to store
  fmt_desc: The maximum number of mdsmap epochs to trim during a single proposal.
  default: 500
  services:
  - mon
  with_legacy: true
- name: mon_max_mgrmap_epochs
  type: int
  level: advanced
  desc: max number of MgrMaps to store
  default: 500
  services:
  - mon
- name: mon_max_osd
  type: int
  level: advanced
  desc: max number of OSDs in a cluster
  fmt_desc: The maximum number of OSDs allowed in the cluster.
  default: 10000
  services:
  - mon
  with_legacy: true
- name: mon_probe_timeout
  type: float
  level: advanced
  desc: timeout for querying other mons during bootstrap pre-election phase (seconds)
  fmt_desc: Number of seconds the monitor will wait to find peers before bootstrapping.
  default: 2
  services:
  - mon
  with_legacy: true
- name: mon_client_bytes
  type: size
  level: advanced
  desc: max bytes of outstanding client messages mon will read off the network
  fmt_desc: The amount of client message data allowed in memory (in bytes).
  default: 100_M
  services:
  - mon
  with_legacy: true
- name: mon_daemon_bytes
  type: size
  level: advanced
  desc: max bytes of outstanding mon messages mon will read off the network
  fmt_desc: The message memory cap for metadata server and OSD messages (in bytes).
  default: 400_M
  services:
  - mon
  with_legacy: true
- name: mon_mgr_proxy_client_bytes_ratio
  type: float
  level: dev
  desc: ratio of mon_client_bytes that can be consumed by proxied mgr commands before
    we error out to client
  default: 0.3
  services:
  - mon
- name: mon_log_max_summary
  type: uint
  level: advanced
  desc: number of recent cluster log messages to retain
  default: 50
  services:
  - mon
  with_legacy: true
- name: mon_max_log_entries_per_event
  type: int
  level: advanced
  desc: max cluster log entries per paxos event
  fmt_desc: The maximum number of log entries per event.
  default: 4096
  services:
  - mon
  with_legacy: true
# min pgs per osd for reweight-by-pg command
- name: mon_reweight_min_pgs_per_osd
  type: uint
  level: advanced
  default: 10
  services:
  - mgr
  with_legacy: true
# min bytes per osd for reweight-by-utilization command
- name: mon_reweight_min_bytes_per_osd
  type: size
  level: advanced
  default: 100_M
  services:
  - mgr
  with_legacy: true
# max osds to change per reweight-by-* command
- name: mon_reweight_max_osds
  type: int
  level: advanced
  default: 4
  services:
  - mgr
  with_legacy: true
- name: mon_reweight_max_change
  type: float
  level: advanced
  default: 0.05
  services:
  - mgr
  with_legacy: true
- name: mon_health_to_clog
  type: bool
  level: advanced
  desc: log monitor health to cluster log
  fmt_desc: Enable sending a health summary to the cluster log periodically.
  default: true
  services:
  - mon
  with_legacy: true
- name: mon_health_to_clog_interval
  type: int
  level: advanced
  desc: frequency to log monitor health to cluster log
  fmt_desc: How often (in seconds) the monitor sends a health summary to the cluster
    log (a non-positive number disables). Monitors will always
    send a summary to the cluster log whether or not it differs from
    the previous summary.
  default: 10_min
  services:
  - mon
  see_also:
  - mon_health_to_clog
  with_legacy: true
- name: mon_health_to_clog_tick_interval
  type: float
  level: dev
  fmt_desc: How often (in seconds) the monitor sends a health summary to the cluster
    log (a non-positive number disables). If current health summary
    is empty or identical to the last time, monitor will not send it
    to cluster log.
  default: 1_min
  services:
  - mon
  with_legacy: true
- name: mon_health_detail_to_clog
  type: bool
  level: dev
  desc: log health detail to cluster log
  default: true
  with_legacy: true
- name: mon_health_max_detail
  type: uint
  level: advanced
  desc: max detailed pgs to report in health detail
  default: 50
  services:
  - mon
- name: mon_health_log_update_period
  type: int
  level: dev
  desc: minimum time in seconds between log messages about each health check
  default: 5
  services:
  - mon
  min: 0
- name: mon_data_avail_crit
  type: int
  level: advanced
  desc: issue MON_DISK_CRIT health error when mon available space below this percentage
  fmt_desc: Raise ``HEALTH_ERR`` status when the filesystem that houses a
    monitor's data store reports that its available capacity is
    less than or equal to this percentage.
  default: 5
  services:
  - mon
  with_legacy: true
- name: mon_data_avail_warn
  type: int
  level: advanced
  desc: issue MON_DISK_LOW health warning when mon available space below this percentage
  fmt_desc: Raise ``HEALTH_WARN`` status when the filesystem that houses a
    monitor's data store reports that its available capacity is
    less than or equal to this percentage .
  default: 30
  services:
  - mon
  with_legacy: true
- name: mon_data_size_warn
  type: size
  level: advanced
  desc: issue MON_DISK_BIG health warning when mon database is above this size
  fmt_desc: Raise ``HEALTH_WARN`` status when a monitor's data
    store grows to be larger than this size, 15GB by default.
  default: 15_G
  services:
  - mon
  with_legacy: true
- name: mon_warn_pg_not_scrubbed_ratio
  type: float
  level: advanced
  desc: Percentage of the scrub max interval past the scrub max interval to warn
  default: 0.5
  see_also:
  - osd_scrub_max_interval
  min: 0
  with_legacy: true
- name: mon_warn_pg_not_deep_scrubbed_ratio
  type: float
  level: advanced
  desc: Percentage of the deep scrub interval past the deep scrub interval to warn
  default: 0.75
  see_also:
  - osd_deep_scrub_interval
  min: 0
  with_legacy: true
- name: mon_scrub_interval
  type: secs
  level: advanced
  desc: frequency for scrubbing mon database
  fmt_desc: How often the monitor scrubs its store by comparing
    the stored checksums with the computed ones for all stored
    keys. (0 disables it. dangerous, use with care)
  default: 1_day
  services:
  - mon
- name: mon_scrub_timeout
  type: int
  level: advanced
  desc: timeout to restart scrub of mon quorum participant does not respond for the
    latest chunk
  default: 5_min
  services:
  - mon
  with_legacy: true
- name: mon_scrub_max_keys
  type: int
  level: advanced
  desc: max keys per on scrub chunk/step
  fmt_desc: The maximum number of keys to scrub each time.
  default: 100
  services:
  - mon
  with_legacy: true
# probability of injected crc mismatch [0.0, 1.0]
- name: mon_scrub_inject_crc_mismatch
  type: float
  level: dev
  desc: probability for injecting crc mismatches into mon scrub
  default: 0
  services:
  - mon
  with_legacy: true
# probability of injected missing keys [0.0, 1.0]
- name: mon_scrub_inject_missing_keys
  type: float
  level: dev
  desc: probability for injecting missing keys into mon scrub
  default: 0
  services:
  - mon
  with_legacy: true
- name: mon_config_key_max_entry_size
  type: size
  level: advanced
  desc: Defines the number of bytes allowed to be held in a single config-key entry
  fmt_desc: The maximum size of config-key entry (in bytes)
  default: 64_K
  services:
  - mon
  with_legacy: true
- name: mon_sync_timeout
  type: float
  level: advanced
  desc: timeout before canceling sync if syncing mon does not respond
  fmt_desc: Number of seconds the monitor will wait for the next update
    message from its sync provider before it gives up and bootstrap
    again.
  default: 1_min
  services:
  - mon
  with_legacy: true
- name: mon_sync_max_payload_size
  type: size
  level: advanced
  desc: target max message payload for mon sync
  fmt_desc: The maximum size for a sync payload (in bytes).
  default: 1_M
  services:
  - mon
  with_legacy: true
- name: mon_sync_max_payload_keys
  type: int
  level: advanced
  desc: target max keys in message payload for mon sync
  default: 2000
  services:
  - mon
  with_legacy: true
- name: mon_sync_debug
  type: bool
  level: dev
  desc: enable extra debugging during mon sync
  default: false
  services:
  - mon
  with_legacy: true
- name: mon_inject_sync_get_chunk_delay
  type: float
  level: dev
  desc: inject delay during sync (seconds)
  default: 0
  services:
  - mon
  with_legacy: true
- name: mon_osd_min_down_reporters
  type: uint
  level: advanced
  desc: number of OSDs from different subtrees who need to report a down OSD for it
    to count
  fmt_desc: The minimum number of Ceph OSD Daemons required to report a
              ``down`` Ceph OSD Daemon.
  default: 2
  services:
  - mon
  see_also:
  - mon_osd_reporter_subtree_level
- name: mon_osd_reporter_subtree_level
  type: str
  level: advanced
  desc: in which level of parent bucket the reporters are counted
  fmt_desc: In which level of parent bucket the reporters are counted. The OSDs
              send failure reports to monitors if they find a peer that is not responsive.
              Monitors mark the reported ``OSD`` out and then ``down`` after a grace period.
  default: host
  services:
  - mon
  flags:
  - runtime
- name: mon_osd_snap_trim_queue_warn_on
  type: int
  level: advanced
  desc: Warn when snap trim queue is that large (or larger).
  long_desc: Warn when snap trim queue length for at least one PG crosses this value,
    as this is indicator of snap trimmer not keeping up, wasting disk space
  default: 32768
  services:
  - mon
  with_legacy: true
# force mon to trim maps to this point, regardless of min_last_epoch_clean (dangerous)
- name: mon_osd_force_trim_to
  type: int
  level: dev
  desc: force mons to trim osdmaps through this epoch
  fmt_desc: Force monitor to trim osdmaps to this point, even if there is
    PGs not clean at the specified epoch (0 disables it. dangerous,
    use with care)
  default: 0
  services:
  - mon
  with_legacy: true
# force mon to trim mdsmaps to this point (dangerous)
- name: mon_mds_force_trim_to
  type: int
  level: dev
  desc: force mons to trim mdsmaps/fsmaps through this epoch
  fmt_desc: Force monitor to trim mdsmaps to this point (0 disables it.
    dangerous, use with care)
  default: 0
  services:
  - mon
  with_legacy: true
# skip safety assertions on FSMap (in case of bugs where we want to continue anyway)
- name: mon_mds_skip_sanity
  type: bool
  level: advanced
  desc: skip sanity checks on fsmap/mdsmap
  fmt_desc: Skip safety assertions on FSMap (in case of bugs where we want to
    continue anyway). Monitor terminates if the FSMap sanity check
    fails, but we can disable it by enabling this option.
  default: false
  services:
  - mon
  with_legacy: true
- name: mon_debug_extra_checks
  type: bool
  level: dev
  desc: Enable some additional monitor checks
  long_desc: Enable some additional monitor checks that would be too expensive to
    run on production systems, or would only be relevant while testing or debugging.
  default: false
  services:
  - mon
- name: mon_debug_block_osdmap_trim
  type: bool
  level: dev
  desc: Block OSDMap trimming while the option is enabled.
  long_desc: Blocking OSDMap trimming may be quite helpful to easily reproduce states
    in which the monitor keeps (hundreds of) thousands of osdmaps.
  default: false
  services:
  - mon
- name: mon_debug_deprecated_as_obsolete
  type: bool
  level: dev
  desc: treat deprecated mon commands as obsolete
  default: false
  services:
  - mon
  with_legacy: true
- name: mon_debug_dump_transactions
  type: bool
  level: dev
  desc: dump paxos transactions to log
  default: false
  services:
  - mon
  see_also:
  - mon_debug_dump_location
  with_legacy: true
- name: mon_debug_dump_json
  type: bool
  level: dev
  desc: dump paxos transasctions to log as json
  default: false
  services:
  - mon
  see_also:
  - mon_debug_dump_transactions
  with_legacy: true
- name: mon_debug_dump_location
  type: str
  level: dev
  desc: file to dump paxos transactions to
  default: /var/log/ceph/$cluster-$name.tdump
  services:
  - mon
  see_also:
  - mon_debug_dump_transactions
  with_legacy: true
- name: mon_debug_no_require_pacific
  type: bool
  level: dev
  desc: do not set pacific feature for new mon clusters
  default: false
  services:
  - mon
  flags:
  - cluster_create
- name: mon_debug_no_require_quincy
  type: bool
  level: dev
  desc: do not set quincy feature for new mon clusters
  default: false
  services:
  - mon
  flags:
  - cluster_create
- name: mon_debug_no_require_bluestore_for_ec_overwrites
  type: bool
  level: dev
  desc: do not require bluestore OSDs to enable EC overwrites on a rados pool
  default: false
  services:
  - mon
  with_legacy: true
- name: mon_debug_no_initial_persistent_features
  type: bool
  level: dev
  desc: do not set any monmap features for new mon clusters
  default: false
  services:
  - mon
  flags:
  - cluster_create
  with_legacy: true
- name: mon_inject_transaction_delay_max
  type: float
  level: dev
  desc: max duration of injected delay in paxos
  default: 10
  services:
  - mon
  with_legacy: true
# range [0, 1]
- name: mon_inject_transaction_delay_probability
  type: float
  level: dev
  desc: probability of injecting a delay in paxos
  default: 0
  services:
  - mon
  with_legacy: true
- name: mon_inject_pg_merge_bounce_probability
  type: float
  level: dev
  desc: probability of failing and reverting a pg_num decrement
  default: 0
  services:
  - mon
# kill the sync provider at a specific point in the work flow
- name: mon_sync_provider_kill_at
  type: int
  level: dev
  desc: kill mon sync requester at specific point
  default: 0
  services:
  - mon
  with_legacy: true
# kill the sync requester at a specific point in the work flow
- name: mon_sync_requester_kill_at
  type: int
  level: dev
  desc: kill mon sync requestor at specific point
  default: 0
  services:
  - mon
  with_legacy: true
# force monitor to join quorum even if it has been previously removed from the map
- name: mon_force_quorum_join
  type: bool
  level: advanced
  desc: force mon to rejoin quorum even though it was just removed
  fmt_desc: Force monitor to join quorum even if it has been previously removed from the map
  default: false
  services:
  - mon
  with_legacy: true
# type of keyvaluedb backend
- name: mon_keyvaluedb
  type: str
  level: advanced
  desc: database backend to use for the mon database
  default: rocksdb
  services:
  - mon
  enum_values:
  - leveldb
  - rocksdb
  flags:
  - create
  with_legacy: true
# UNSAFE -- TESTING ONLY! Allows addition of a cache tier with preexisting snaps
- name: mon_debug_unsafe_allow_tier_with_nonempty_snaps
  type: bool
  level: dev
  default: false
  services:
  - mon
  with_legacy: true
- name: mon_osd_blocklist_default_expire
  type: float
  level: advanced
  desc: Duration in seconds that blocklist entries for clients remain in the OSD map
  default: 1_hr
  services:
  - mon
  with_legacy: true
- name: mon_mds_blocklist_interval
  type: float
  level: dev
  desc: Duration in seconds that blocklist entries for MDS daemons remain in the OSD
    map
  fmt_desc: The blocklist duration for failed MDSs in the OSD map. Note,
    this controls how long failed MDS daemons will stay in the
    OSDMap blocklist. It has no effect on how long something is
    blocklisted when the administrator blocklists it manually. For
    example, ``ceph osd blocklist add`` will still use the default
    blocklist time.
  default: 1_day
  services:
  - mon
  min: 1_hr
  flags:
  - runtime
- name: mon_mgr_blocklist_interval
  type: float
  level: dev
  desc: Duration in seconds that blocklist entries for mgr daemons remain in the OSD
    map
  default: 1_day
  services:
  - mon
  min: 1_hr
  flags:
  - runtime
- name: mon_osd_crush_smoke_test
  type: bool
  level: advanced
  desc: perform a smoke test on any new CRUSH map before accepting changes
  default: true
  services:
  - mon
  with_legacy: true
- name: mon_smart_report_timeout
  type: uint
  level: advanced
  desc: Timeout (in seconds) for smarctl to run, default is set to 5
  default: 5
  services:
  - mon
- name: mon_auth_validate_all_caps
  type: bool
  level: advanced
  desc: Whether to parse non-monitor capabilities set by the 'ceph auth ...' commands.
    Disabling this saves CPU on the monitor, but allows invalid capabilities to be
    set, and only be rejected later, when they are used.
  default: true
  services:
  - mon
  flags:
  - runtime
- name: mon_warn_on_older_version
  type: bool
  level: advanced
  desc: issue DAEMON_OLD_VERSION health warning if daemons are not all running the
    same version
  default: true
  services:
  - mon
- name: mon_warn_older_version_delay
  type: secs
  level: advanced
  desc: issue DAEMON_OLD_VERSION health warning after this amount of time has elapsed
  default: 7_day
  services:
  - mon
# how often (in commits) to stash a full copy of the PaxosService state
- name: paxos_stash_full_interval
  type: int
  level: advanced
  default: 25
  services:
  - mon
  fmt_desc: How often (in commits) to stash a full copy of the PaxosService state.
    Current this setting only affects ``mds``, ``mon``, ``auth`` and ``mgr``
    PaxosServices.
  with_legacy: true
# max paxos iterations before we must first sync the monitor stores
- name: paxos_max_join_drift
  type: int
  level: advanced
  default: 10
  services:
  - mon
  fmt_desc: The maximum Paxos iterations before we must first sync the
    monitor data stores. When a monitor finds that its peer is too
    far ahead of it, it will first sync with data stores before moving
    on.
  with_legacy: true
# gather updates for this long before proposing a map update
- name: paxos_propose_interval
  type: float
  level: advanced
  default: 1
  services:
  - mon
  fmt_desc: Gather updates for this time interval before proposing
    a map update.
  with_legacy: true
# min time to gather updates for after period of inactivity
- name: paxos_min_wait
  type: float
  level: advanced
  default: 0.05
  services:
  - mon
  fmt_desc: The minimum amount of time to gather updates after a period of
    inactivity.
  with_legacy: true
# minimum number of paxos states to keep around
- name: paxos_min
  type: int
  level: advanced
  default: 500
  services:
  - mon
  fmt_desc: The minimum number of Paxos states to keep around
  with_legacy: true
# number of extra proposals tolerated before trimming
- name: paxos_trim_min
  type: int
  level: advanced
  default: 250
  services:
  - mon
  fmt_desc: Number of extra proposals tolerated before trimming
  with_legacy: true
# maximum amount of versions to trim during a single proposal (0 disables it)
- name: paxos_trim_max
  type: int
  level: advanced
  default: 500
  services:
  - mon
  fmt_desc: The maximum number of extra proposals to trim at a time
  with_legacy: true
# minimum amount of versions to trigger a trim (0 disables it)
- name: paxos_service_trim_min
  type: uint
  level: advanced
  default: 250
  services:
  - mon
  fmt_desc: The minimum amount of versions to trigger a trim (0 disables it)
  with_legacy: true
# maximum amount of versions to trim during a single proposal (0 disables it)
- name: paxos_service_trim_max
  type: uint
  level: advanced
  default: 500
  services:
  - mon
  fmt_desc: The maximum amount of versions to trim during a single proposal (0 disables it)
  with_legacy: true
- name: paxos_service_trim_max_multiplier
  type: uint
  level: advanced
  desc: factor by which paxos_service_trim_max will be multiplied to get a new upper
    bound when trim sizes are high  (0 disables it)
  default: 20
  services:
  - mon
  min: 0
  flags:
  - runtime
- name: paxos_kill_at
  type: int
  level: dev
  default: 0
  services:
  - mon
  with_legacy: true
# required of mon, mds, osd daemons
- name: auth_cluster_required
  type: str
  level: advanced
  desc: authentication methods required by the cluster
  fmt_desc: If enabled, the Ceph Storage Cluster daemons (i.e., ``ceph-mon``,
   ``ceph-osd``, ``ceph-mds`` and ``ceph-mgr``) must authenticate with
   each other. Valid settings are ``cephx`` or ``none``.
  default: cephx
  with_legacy: true
# required by daemons of clients
- name: auth_service_required
  type: str
  level: advanced
  desc: authentication methods required by service daemons
  fmt_desc: If enabled, the Ceph Storage Cluster daemons require Ceph Clients
   to authenticate with the Ceph Storage Cluster in order to access
   Ceph services. Valid settings are ``cephx`` or ``none``.
  default: cephx
  with_legacy: true
# what clients require of daemons
- name: auth_client_required
  type: str
  level: advanced
  desc: authentication methods allowed by clients
  fmt_desc: If enabled, the Ceph Client requires the Ceph Storage Cluster to
   authenticate with the Ceph Client. Valid settings are ``cephx``
   or ``none``.
  default: cephx, none
  with_legacy: true
# deprecated; default value for above if they are not defined.
- name: auth_supported
  type: str
  level: advanced
  desc: authentication methods required (deprecated)
  with_legacy: true
- name: max_rotating_auth_attempts
  type: int
  level: advanced
  desc: number of attempts to initialize rotating keys before giving up
  default: 10
  with_legacy: true
- name: rotating_keys_bootstrap_timeout
  type: int
  level: advanced
  desc: timeout for obtaining rotating keys during bootstrap phase (seconds)
  default: 30
- name: rotating_keys_renewal_timeout
  type: int
  level: advanced
  desc: timeout for updating rotating keys (seconds)
  default: 10
- name: cephx_require_signatures
  type: bool
  level: advanced
  default: false
  fmt_desc: If set to ``true``, Ceph requires signatures on all message
   traffic between the Ceph Client and the Ceph Storage Cluster, and
   between daemons comprising the Ceph Storage Cluster.

   Ceph Argonaut and Linux kernel versions prior to 3.19 do
   not support signatures; if such clients are in use this
   option can be turned off to allow them to connect.
  with_legacy: true
- name: cephx_require_version
  type: int
  level: advanced
  desc: Cephx version required (1 = pre-mimic, 2 = mimic+)
  default: 2
  with_legacy: true
- name: cephx_cluster_require_signatures
  type: bool
  level: advanced
  default: false
  fmt_desc:    If set to ``true``, Ceph requires signatures on all message
   traffic between Ceph daemons comprising the Ceph Storage Cluster.
  with_legacy: true
- name: cephx_cluster_require_version
  type: int
  level: advanced
  desc: Cephx version required by the cluster from clients (1 = pre-mimic, 2 = mimic+)
  default: 2
  with_legacy: true
- name: cephx_service_require_signatures
  type: bool
  level: advanced
  default: false
  fmt_desc: If set to ``true``, Ceph requires signatures on all message
   traffic between Ceph Clients and the Ceph Storage Cluster.
  with_legacy: true
- name: cephx_service_require_version
  type: int
  level: advanced
  desc: Cephx version required from ceph services (1 = pre-mimic, 2 = mimic+)
  default: 2
  with_legacy: true
# Default to signing session messages if supported
- name: cephx_sign_messages
  type: bool
  level: advanced
  default: true
  fmt_desc: If the Ceph version supports message signing, Ceph will sign
   all messages so they are more difficult to spoof.
  with_legacy: true
- name: auth_mon_ticket_ttl
  type: float
  level: advanced
  default: 72_hr
  with_legacy: true
- name: auth_service_ticket_ttl
  type: float
  level: advanced
  default: 1_hr
  fmt_desc: When the Ceph Storage Cluster sends a Ceph Client a ticket for
   authentication, the Ceph Storage Cluster assigns the ticket a
   time to live.
  with_legacy: true
- name: auth_allow_insecure_global_id_reclaim
  type: bool
  level: advanced
  desc: Allow reclaiming global_id without presenting a valid ticket proving
    previous possession of that global_id
  long_desc: Allowing unauthorized global_id (re)use poses a security risk.
    Unfortunately, older clients may omit their ticket on reconnects and
    therefore rely on this being allowed for preserving their global_id for
    the lifetime of the client instance. Setting this value to false would
    immediately prevent new connections from those clients (assuming
    auth_expose_insecure_global_id_reclaim set to true) and eventually break
    existing sessions as well (regardless of auth_expose_insecure_global_id_reclaim
    setting).
  default: true
  see_also:
  - mon_warn_on_insecure_global_id_reclaim
  - mon_warn_on_insecure_global_id_reclaim_allowed
  - auth_expose_insecure_global_id_reclaim
  with_legacy: true
- name: auth_expose_insecure_global_id_reclaim
  type: bool
  level: advanced
  desc: Force older clients that may omit their ticket on reconnects to
    reconnect as part of establishing a session
  long_desc: 'In permissive mode (auth_allow_insecure_global_id_reclaim set
    to true), this helps with identifying clients that are not patched. In
    enforcing mode (auth_allow_insecure_global_id_reclaim set to false), this
    is a fail-fast mechanism: don''t establish a session that will almost
    inevitably be broken later.'
  default: true
  see_also:
  - mon_warn_on_insecure_global_id_reclaim
  - mon_warn_on_insecure_global_id_reclaim_allowed
  - auth_allow_insecure_global_id_reclaim
  with_legacy: true
# if true, assert when weird things happen
- name: auth_debug
  type: bool
  level: dev
  default: false
  with_legacy: true
# how many mons to try to connect to in parallel during hunt
- name: mon_client_hunt_parallel
  type: uint
  level: advanced
  default: 3
  with_legacy: true
# try new mon every N seconds until we connect
- name: mon_client_hunt_interval
  type: float
  level: advanced
  default: 3
  fmt_desc: The client will try a new monitor every ``N`` seconds until it
    establishes a connection.
  with_legacy: true
# send logs every N seconds
- name: mon_client_log_interval
  type: float
  level: advanced
  desc: How frequently we send queued cluster log messages to mon
  default: 1
  with_legacy: true
# ping every N seconds
- name: mon_client_ping_interval
  type: float
  level: advanced
  default: 10
  fmt_desc: The client will ping the monitor every ``N`` seconds.
  with_legacy: true
# fail if we don't hear back
- name: mon_client_ping_timeout
  type: float
  level: advanced
  default: 30
  with_legacy: true
- name: mon_client_hunt_interval_backoff
  type: float
  level: advanced
  default: 1.5
  with_legacy: true
- name: mon_client_hunt_interval_min_multiple
  type: float
  level: advanced
  default: 1
  with_legacy: true
- name: mon_client_hunt_interval_max_multiple
  type: float
  level: advanced
  default: 10
  with_legacy: true
- name: mon_client_max_log_entries_per_message
  type: int
  level: advanced
  default: 1000
  fmt_desc: The maximum number of log entries a monitor will generate
    per client message.
  with_legacy: true
- name: mon_client_directed_command_retry
  type: int
  level: dev
  desc: Number of times to try sending a command directed at a specific monitor
  default: 2
  with_legacy: true
- name: mon_max_pool_pg_num
  type: uint
  level: advanced
  default: 64_K
  fmt_desc: The maximum number of placement groups per pool.
- name: mon_pool_quota_warn_threshold
  type: int
  level: advanced
  desc: percent of quota at which to issue warnings
  default: 0
  services:
  - mgr
- name: mon_pool_quota_crit_threshold
  type: int
  level: advanced
  desc: percent of quota at which to issue errors
  default: 0
  services:
  - mgr
# whitespace-separated list of key=value pairs describing crush location
- name: crush_location
  type: str
  level: advanced
  with_legacy: true
- name: crush_location_hook
  type: str
  level: advanced
  with_legacy: true
- name: crush_location_hook_timeout
  type: int
  level: advanced
  default: 10
  with_legacy: true
- name: objecter_tick_interval
  type: float
  level: dev
  default: 5
  with_legacy: true
# before we ask for a map
- name: objecter_timeout
  type: float
  level: advanced
  desc: Seconds before in-flight op is considered 'laggy' and we query mon for the
    latest OSDMap
  default: 10
  with_legacy: true
- name: objecter_inflight_op_bytes
  type: size
  level: advanced
  desc: Max in-flight data in bytes (both directions)
  default: 100_M
  with_legacy: true
- name: objecter_inflight_ops
  type: uint
  level: advanced
  desc: Max in-flight operations
  default: 1_K
  with_legacy: true
# num of completion locks per each session, for serializing same object responses
- name: objecter_completion_locks_per_session
  type: uint
  level: dev
  default: 32
  with_legacy: true
# suppress watch pings
- name: objecter_inject_no_watch_ping
  type: bool
  level: dev
  default: false
  with_legacy: true
# ignore the first reply for each write, and resend the osd op instead
- name: objecter_retry_writes_after_first_reply
  type: bool
  level: dev
  default: false
  with_legacy: true
- name: objecter_debug_inject_relock_delay
  type: bool
  level: dev
  default: false
  with_legacy: true
- name: filer_max_purge_ops
  type: uint
  level: advanced
  desc: Max in-flight operations for purging a striped range (e.g., MDS journal)
  default: 10
  with_legacy: true
- name: filer_max_truncate_ops
  type: uint
  level: advanced
  desc: Max in-flight operations for truncating/deleting a striped sequence (e.g.,
    MDS journal)
  default: 128
  with_legacy: true
- name: journaler_write_head_interval
  type: int
  level: advanced
  desc: Interval in seconds between journal header updates (to help bound replay time)
  default: 15
# * journal object size
- name: journaler_prefetch_periods
  type: uint
  level: advanced
  desc: Number of striping periods to prefetch while reading MDS journal
  default: 10
  # we need at least 2 periods to make progress.
  min: 2
# * journal object size
- name: journaler_prezero_periods
  type: uint
  level: advanced
  desc: Number of striping periods to zero head of MDS journal write position
  default: 5
  # we need to zero at least two periods, minimum, to ensure that we
  # have a full empty object/period in front of us.
  min: 2
- name: osd_calc_pg_upmaps_aggressively
  type: bool
  level: advanced
  desc: try to calculate PG upmaps more aggressively, e.g., by doing a fairly exhaustive
    search of existing PGs that can be unmapped or upmapped
  default: true
  flags:
  - runtime
- name: osd_calc_pg_upmaps_local_fallback_retries
  type: uint
  level: advanced
  desc: 'Maximum number of PGs we can attempt to unmap or upmap for a specific overfull
    or underfull osd per iteration '
  default: 100
  flags:
  - runtime
- name: osd_numa_prefer_iface
  type: bool
  level: advanced
  desc: prefer IP on network interface on same numa node as storage
  default: true
  see_also:
  - osd_numa_auto_affinity
  flags:
  - startup
- name: osd_numa_auto_affinity
  type: bool
  level: advanced
  desc: automatically set affinity to numa node when storage and network match
  default: true
  flags:
  - startup
- name: osd_numa_node
  type: int
  level: advanced
  desc: set affinity to a numa node (-1 for none)
  default: -1
  see_also:
  - osd_numa_auto_affinity
  flags:
  - startup
- name: osd_smart_report_timeout
  type: uint
  level: advanced
  desc: Timeout (in seconds) for smarctl to run, default is set to 5
  default: 5
# verify backend can support configured max object name length
- name: osd_check_max_object_name_len_on_startup
  type: bool
  level: dev
  default: true
  with_legacy: true
- name: osd_max_backfills
  type: uint
  level: advanced
  desc: Maximum number of concurrent local and remote backfills or recoveries per
    OSD
  long_desc: There can be osd_max_backfills local reservations AND the same remote
    reservations per OSD. So a value of 1 lets this OSD participate as 1 PG primary
    in recovery and 1 shard of another recovering PG.
  fmt_desc: The maximum number of backfills allowed to or from a single OSD.
    Note that this is applied separately for read and write operations.
  default: 1
  flags:
  - runtime
  with_legacy: true
# Minimum recovery priority (255 = max, smaller = lower)
- name: osd_min_recovery_priority
  type: int
  level: advanced
  desc: Minimum priority below which recovery is not performed
  long_desc: The purpose here is to prevent the cluster from doing *any* lower priority
    work (e.g., rebalancing) below this threshold and focus solely on higher priority
    work (e.g., replicating degraded objects).
  default: 0
  with_legacy: true
- name: osd_backfill_retry_interval
  type: float
  level: advanced
  desc: how frequently to retry backfill reservations after being denied (e.g., due
    to a full OSD)
  fmt_desc: The number of seconds to wait before retrying backfill requests.
  default: 30
  with_legacy: true
- name: osd_recovery_retry_interval
  type: float
  level: advanced
  desc: how frequently to retry recovery reservations after being denied (e.g., due
    to a full OSD)
  default: 30
  with_legacy: true
- name: osd_agent_max_ops
  type: int
  level: advanced
  desc: maximum concurrent tiering operations for tiering agent
  fmt_desc: The maximum number of simultaneous flushing ops per tiering agent
    in the high speed mode.
  default: 4
  with_legacy: true
- name: osd_agent_max_low_ops
  type: int
  level: advanced
  desc: maximum concurrent low-priority tiering operations for tiering agent
  fmt_desc: The maximum number of simultaneous flushing ops per tiering agent
    in the low speed mode.
  default: 2
  with_legacy: true
- name: osd_agent_min_evict_effort
  type: float
  level: advanced
  desc: minimum effort to expend evicting clean objects
  default: 0.1
  min: 0
  max: 0.99
  with_legacy: true
- name: osd_agent_quantize_effort
  type: float
  level: advanced
  desc: size of quantize unit for eviction effort
  default: 0.1
  with_legacy: true
- name: osd_agent_delay_time
  type: float
  level: advanced
  desc: how long agent should sleep if it has no work to do
  default: 5
  with_legacy: true
- name: osd_find_best_info_ignore_history_les
  type: bool
  level: dev
  desc: ignore last_epoch_started value when peering AND PROBABLY LOSE DATA
  long_desc: THIS IS AN EXTREMELY DANGEROUS OPTION THAT SHOULD ONLY BE USED AT THE
    DIRECTION OF A DEVELOPER.  It makes peering ignore the last_epoch_started value
    when peering, which can allow the OSD to believe an OSD has an authoritative view
    of a PG's contents even when it is in fact old and stale, typically leading to
    data loss (by believing a stale PG is up to date).
  default: false
  with_legacy: true
# decay atime and hist histograms after how many objects go by
- name: osd_agent_hist_halflife
  type: int
  level: advanced
  desc: halflife of agent atime and temp histograms
  default: 1000
  with_legacy: true
# decay atime and hist histograms after how many objects go by
- name: osd_agent_slop
  type: float
  level: advanced
  desc: slop factor to avoid switching tiering flush and eviction mode
  default: 0.02
  with_legacy: true
- name: osd_uuid
  type: uuid
  level: advanced
  desc: uuid label for a new OSD
  fmt_desc: The universally unique identifier (UUID) for the Ceph OSD Daemon.
  note: The ``osd_uuid`` applies to a single Ceph OSD Daemon. The ``fsid``
    applies to the entire cluster.
  flags:
  - create
  with_legacy: true
- name: osd_data
  type: str
  level: advanced
  desc: path to OSD data
  fmt_desc: The path to the OSDs data. You must create the directory when
    deploying Ceph. You should mount a drive for OSD data at this
    mount point. We do not recommend changing the default.
  default: /var/lib/ceph/osd/$cluster-$id
  flags:
  - no_mon_update
  with_legacy: true
- name: osd_journal
  type: str
  level: advanced
  desc: path to OSD journal (when FileStore backend is in use)
  fmt_desc: The path to the OSD's journal. This may be a path to a file or a
    block device (such as a partition of an SSD). If it is a file,
    you must create the directory to contain it. We recommend using a
    separate fast device when the ``osd_data`` drive is an HDD.
  default: /var/lib/ceph/osd/$cluster-$id/journal
  flags:
  - no_mon_update
  with_legacy: true
- name: osd_journal_size
  type: size
  level: advanced
  desc: size of FileStore journal (in MiB)
  fmt_desc: The size of the journal in megabytes.
  default: 5_K
  flags:
  - create
  with_legacy: true
- name: osd_journal_flush_on_shutdown
  type: bool
  level: advanced
  desc: flush FileStore journal contents during clean OSD shutdown
  default: true
  with_legacy: true
- name: osd_compact_on_start
  type: bool
  level: advanced
  desc: compact OSD's object store's OMAP on start
  default: false
# flags for specific control purpose during osd mount() process.
# e.g., can be 1 to skip over replaying journal
# or 2 to skip over mounting omap or 3 to skip over both.
# This might be helpful in case the journal is totally corrupted
# and we still want to bring the osd daemon back normally, etc.
- name: osd_os_flags
  type: uint
  level: dev
  desc: flags to skip filestore omap or journal initialization
  default: 0
- name: osd_max_write_size
  type: size
  level: advanced
  desc: Maximum size of a RADOS write operation in megabytes
  long_desc: This setting prevents clients from doing very large writes to RADOS.  If
    you set this to a value below what clients expect, they will receive an error
    when attempting to write to the cluster.
  fmt_desc: The maximum size of a write in megabytes.
  default: 90
  min: 4
  with_legacy: true
- name: osd_max_pgls
  type: uint
  level: advanced
  desc: maximum number of results when listing objects in a pool
  fmt_desc: The maximum number of placement groups to list. A client
    requesting a large number can tie up the Ceph OSD Daemon.
  default: 1_K
  with_legacy: true
- name: osd_client_message_size_cap
  type: size
  level: advanced
  desc: maximum memory to devote to in-flight client requests
  long_desc: If this value is exceeded, the OSD will not read any new client data
    off of the network until memory is freed.
  fmt_desc: The largest client data message allowed in memory.
  default: 500_M
  with_legacy: true
- name: osd_client_message_cap
  type: uint
  level: advanced
  desc: maximum number of in-flight client requests
  default: 0
  with_legacy: true
- name: osd_crush_update_weight_set
  type: bool
  level: advanced
  desc: update CRUSH weight-set weights when updating weights
  long_desc: If this setting is true, we will update the weight-set weights when adjusting
    an item's weight, effectively making changes take effect immediately, and discarding
    any previous optimization in the weight-set value.  Setting this value to false
    will leave it to the balancer to (slowly, presumably) adjust weights to approach
    the new target value.
  default: true
  with_legacy: true
# 1 = host
- name: osd_crush_chooseleaf_type
  type: int
  level: dev
  desc: default chooseleaf type for osdmaptool --create
  fmt_desc: The bucket type to use for ``chooseleaf`` in a CRUSH rule. Uses
    ordinal rank rather than name.
  default: 1
  flags:
  - cluster_create
  with_legacy: true
# try to use gmt for hitset archive names if all osds in cluster support it
- name: osd_pool_use_gmt_hitset
  type: bool
  level: dev
  desc: use UTC for hitset timestamps
  long_desc: This setting only exists for compatibility with hammer (and older) clusters.
  default: true
  with_legacy: true
- name: osd_crush_update_on_start
  type: bool
  level: advanced
  desc: update OSD CRUSH location on startup
  default: true
  with_legacy: true
- name: osd_class_update_on_start
  type: bool
  level: advanced
  desc: set OSD device class on startup
  default: true
  with_legacy: true
- name: osd_crush_initial_weight
  type: float
  level: advanced
  desc: if >= 0, initial CRUSH weight for newly created OSDs
  long_desc: If this value is negative, the size of the OSD in TiB is used.
  fmt_desc: The initial CRUSH weight for newly added OSDs. The default
    value of this option is ``the size of a newly added OSD in TB``. By default,
    the initial CRUSH weight for a newly added OSD is set to its device size in
    TB. See `Weighting Bucket Items`_ for details.
  default: -1
  with_legacy: true
# whether turn on fast read on the pool or not
- name: osd_pool_default_ec_fast_read
  type: bool
  level: advanced
  desc: set ec_fast_read for new erasure-coded pools
  fmt_desc: Whether to turn on fast read on the pool or not. It will be used as
    the default setting of newly created erasure coded pools if ``fast_read``
    is not specified at create time.
  default: false
  services:
  - mon
  with_legacy: true
- name: osd_pool_default_crush_rule
  type: int
  level: advanced
  desc: CRUSH rule for newly created pools
  fmt_desc: The default CRUSH rule to use when creating a replicated pool. The
    default value of ``-1`` means "pick the rule with the lowest numerical ID and
    use that".  This is to make pool creation work in the absence of rule 0.
  default: -1
  services:
  - mon
- name: osd_pool_erasure_code_stripe_unit
  type: size
  level: advanced
  desc: the amount of data (in bytes) in a data chunk, per stripe
  fmt_desc: Sets the default size, in bytes, of a chunk of an object
    stripe for erasure coded pools. Every object of size S
    will be stored as N stripes, with each data chunk
    receiving ``stripe unit`` bytes. Each stripe of ``N *
    stripe unit`` bytes will be encoded/decoded
    individually. This option can is overridden by the
    ``stripe_unit`` setting in an erasure code profile.
  default: 4_K
  services:
  - mon
- name: osd_pool_default_size
  type: uint
  level: advanced
  desc: the number of copies of an object for new replicated pools
  fmt_desc: Sets the number of replicas for objects in the pool. The default
    value is the same as
    ``ceph osd pool set {pool-name} size {size}``.
  default: 3
  services:
  - mon
  min: 0
  max: 10
  flags:
  - runtime
- name: osd_pool_default_min_size
  type: uint
  level: advanced
  desc: the minimal number of copies allowed to write to a degraded pool for new replicated
    pools
  long_desc: 0 means no specific default; ceph will use size-size/2
  fmt_desc: Sets the minimum number of written replicas for objects in the
    pool in order to acknowledge an I/O operation to the client.  If
    minimum is not met, Ceph will not acknowledge the I/O to the
    client, **which may result in data loss**. This setting ensures
    a minimum number of replicas when operating in ``degraded`` mode.
    The default value is ``0`` which means no particular minimum. If ``0``,
    minimum is ``size - (size / 2)``.
  default: 0
  services:
  - mon
  see_also:
  - osd_pool_default_size
  min: 0
  max: 255
  flags:
  - runtime
- name: osd_pool_default_pg_num
  type: uint
  level: advanced
  desc: number of PGs for new pools
  fmt_desc: The default number of placement groups for a pool. The default
    value is the same as ``pg_num`` with ``mkpool``.
  default: 32
  services:
  - mon
  flags:
  - runtime
- name: osd_pool_default_pgp_num
  type: uint
  level: advanced
  desc: number of PGs for placement purposes (0 to match pg_num)
  fmt_desc: The default number of placement groups for placement for a pool.
    The default value is the same as ``pgp_num`` with ``mkpool``.
    PG and PGP should be equal (for now).
  default: 0
  services:
  - mon
  see_also:
  - osd_pool_default_pg_num
  flags:
  - runtime
- name: osd_pool_default_type
  type: str
  level: advanced
  desc: default type of pool to create
  default: replicated
  services:
  - mon
  enum_values:
  - replicated
  - erasure
  flags:
  - runtime
- name: osd_pool_default_erasure_code_profile
  type: str
  level: advanced
  desc: default erasure code profile for new erasure-coded pools
  default: plugin=jerasure technique=reed_sol_van k=2 m=2
  services:
  - mon
  flags:
  - runtime
- name: osd_erasure_code_plugins
  type: str
  level: advanced
  desc: erasure code plugins to load
  default: @osd_erasure_code_plugins@
  services:
  - mon
  - osd
  flags:
  - startup
  with_legacy: true
# Allows the "peered" state for recovery and backfill below min_size
- name: osd_allow_recovery_below_min_size
  type: bool
  level: dev
  desc: allow replicated pools to recover with < min_size active members
  default: true
  services:
  - osd
  with_legacy: true
- name: osd_pool_default_flags
  type: int
  level: dev
  desc: (integer) flags to set on new pools
  fmt_desc: The default flags for new pools.
  default: 0
  services:
  - mon
  with_legacy: true
# use new pg hashing to prevent pool/pg overlap
- name: osd_pool_default_flag_hashpspool
  type: bool
  level: advanced
  desc: set hashpspool (better hashing scheme) flag on new pools
  default: true
  services:
  - mon
  with_legacy: true
# pool can't be deleted
- name: osd_pool_default_flag_nodelete
  type: bool
  level: advanced
  desc: set nodelete flag on new pools
  fmt_desc: Set the ``nodelete`` flag on new pools, which prevents pool removal.
  default: false
  services:
  - mon
  with_legacy: true
# pool's pg and pgp num can't be changed
- name: osd_pool_default_flag_nopgchange
  type: bool
  level: advanced
  desc: set nopgchange flag on new pools
  fmt_desc: Set the ``nopgchange`` flag on new pools. Does not allow the number of PGs to be changed.
  default: false
  services:
  - mon
  with_legacy: true
# pool's size and min size can't be changed
- name: osd_pool_default_flag_nosizechange
  type: bool
  level: advanced
  desc: set nosizechange flag on new pools
  fmt_desc: Set the ``nosizechange`` flag on new pools. Does not allow the ``size`` to be changed.
  default: false
  services:
  - mon
  with_legacy: true
- name: osd_pool_default_hit_set_bloom_fpp
  type: float
  level: advanced
  default: 0.05
  services:
  - mon
  see_also:
  - osd_tier_default_cache_hit_set_type
  with_legacy: true
- name: osd_pool_default_cache_target_dirty_ratio
  type: float
  level: advanced
  default: 0.4
  with_legacy: true
- name: osd_pool_default_cache_target_dirty_high_ratio
  type: float
  level: advanced
  default: 0.6
  with_legacy: true
- name: osd_pool_default_cache_target_full_ratio
  type: float
  level: advanced
  default: 0.8
  with_legacy: true
# seconds
- name: osd_pool_default_cache_min_flush_age
  type: int
  level: advanced
  default: 0
  with_legacy: true
# seconds
- name: osd_pool_default_cache_min_evict_age
  type: int
  level: advanced
  default: 0
  with_legacy: true
# max size to check for eviction
- name: osd_pool_default_cache_max_evict_check_size
  type: int
  level: advanced
  default: 10
  with_legacy: true
- name: osd_pool_default_pg_autoscale_mode
  type: str
  level: advanced
  desc: Default PG autoscaling behavior for new pools
  default: 'on'
  enum_values:
  - 'off'
  - 'warn'
  - 'on'
  flags:
  - runtime
- name: osd_pool_default_read_lease_ratio
  type: float
  level: dev
  desc: Default read_lease_ratio for a pool, as a multiple of osd_heartbeat_grace
  long_desc: This should be <= 1.0 so that the read lease will have expired by the
    time we decide to mark a peer OSD down.
  default: 0.8
  see_also:
  - osd_heartbeat_grace
  flags:
  - runtime
  with_legacy: true
# min target size for a HitSet
- name: osd_hit_set_min_size
  type: int
  level: advanced
  default: 1000
  with_legacy: true
# max target size for a HitSet
- name: osd_hit_set_max_size
  type: int
  level: advanced
  default: 100000
  with_legacy: true
# rados namespace for hit_set tracking
- name: osd_hit_set_namespace
  type: str
  level: advanced
  default: .ceph-internal
  with_legacy: true
# conservative default throttling values
- name: osd_tier_promote_max_objects_sec
  type: uint
  level: advanced
  default: 25
  with_legacy: true
- name: osd_tier_promote_max_bytes_sec
  type: size
  level: advanced
  default: 5_M
  with_legacy: true
- name: osd_tier_default_cache_mode
  type: str
  level: advanced
  default: writeback
  enum_values:
  - none
  - writeback
  - forward
  - readonly
  - readforward
  - readproxy
  - proxy
  flags:
  - runtime
- name: osd_tier_default_cache_hit_set_count
  type: uint
  level: advanced
  default: 4
- name: osd_tier_default_cache_hit_set_period
  type: uint
  level: advanced
  default: 1200
- name: osd_tier_default_cache_hit_set_type
  type: str
  level: advanced
  default: bloom
  enum_values:
  - bloom
  - explicit_hash
  - explicit_object
  flags:
  - runtime
- name: osd_tier_default_cache_min_read_recency_for_promote
  type: uint
  level: advanced
  desc: number of recent HitSets the object must appear in to be promoted (on read)
  default: 1
- name: osd_tier_default_cache_min_write_recency_for_promote
  type: uint
  level: advanced
  desc: number of recent HitSets the object must appear in to be promoted (on write)
  default: 1
- name: osd_tier_default_cache_hit_set_grade_decay_rate
  type: uint
  level: advanced
  default: 20
- name: osd_tier_default_cache_hit_set_search_last_n
  type: uint
  level: advanced
  default: 1
- name: osd_objecter_finishers
  type: int
  level: advanced
  default: 1
  flags:
  - startup
  with_legacy: true
- name: osd_map_dedup
  type: bool
  level: advanced
  default: true
  fmt_desc: Enable removing duplicates in the OSD map.
  with_legacy: true
- name: osd_map_cache_size
  type: int
  level: advanced
  default: 50
  fmt_desc: The number of OSD maps to keep cached.
  with_legacy: true
- name: osd_map_message_max
  type: int
  level: advanced
  desc: maximum number of OSDMaps to include in a single message
  fmt_desc: The maximum map entries allowed per MOSDMap message.
  default: 40
  with_legacy: true
- name: osd_map_message_max_bytes
  type: size
  level: advanced
  desc: maximum number of bytes worth of OSDMaps to include in a single message
  default: 10_M
  with_legacy: true
# cap on # of inc maps we send to peers, clients
- name: osd_map_share_max_epochs
  type: int
  level: advanced
  default: 40
  with_legacy: true
- name: osd_pg_epoch_max_lag_factor
  type: float
  level: advanced
  desc: Max multiple of the map cache that PGs can lag before we throttle map injest
  default: 2
  see_also:
  - osd_map_cache_size
- name: osd_inject_bad_map_crc_probability
  type: float
  level: dev
  default: 0
  with_legacy: true
- name: osd_inject_failure_on_pg_removal
  type: bool
  level: dev
  default: false
  with_legacy: true
# shutdown the OSD if stuatus flipping more than max_markdown_count times in recent max_markdown_period seconds
- name: osd_max_markdown_period
  type: int
  level: advanced
  default: 10_min
  with_legacy: true
- name: osd_max_markdown_count
  type: int
  level: advanced
  default: 5
  with_legacy: true
- name: osd_op_pq_max_tokens_per_priority
  type: uint
  level: advanced
  default: 4_M
  with_legacy: true
- name: osd_op_pq_min_cost
  type: size
  level: advanced
  default: 64_K
  with_legacy: true
# preserve clone_overlap during recovery/migration
- name: osd_recover_clone_overlap
  type: bool
  level: advanced
  default: true
  fmt_desc: Preserves clone overlap during recovery. Should always be set
    to ``true``.
  with_legacy: true
- name: osd_num_cache_shards
  type: size
  level: advanced
  desc: The number of cache shards to use in the object store.
  default: 32
  flags:
  - startup
- name: osd_op_num_threads_per_shard
  type: int
  level: advanced
  default: 0
  flags:
  - startup
  with_legacy: true
- name: osd_op_num_threads_per_shard_hdd
  type: int
  level: advanced
  default: 1
  see_also:
  - osd_op_num_threads_per_shard
  flags:
  - startup
  with_legacy: true
- name: osd_op_num_threads_per_shard_ssd
  type: int
  level: advanced
  default: 2
  see_also:
  - osd_op_num_threads_per_shard
  flags:
  - startup
  with_legacy: true
- name: osd_op_num_shards
  type: int
  level: advanced
  fmt_desc: The number of shards allocated for a given OSD. Each shard has its own processing queue.
    PGs on the OSD are distributed evenly in the shard. This setting overrides _ssd and _hdd if
    non-zero.
  default: 0
  flags:
  - startup
  with_legacy: true
- name: osd_op_num_shards_hdd
  type: int
  level: advanced
  fmt_desc: the number of shards allocated for a given OSD (for rotational media).
  default: 5
  see_also:
  - osd_op_num_shards
  flags:
  - startup
  with_legacy: true
- name: osd_op_num_shards_ssd
  type: int
  level: advanced
  fmt_desc: the number of shards allocated for a given OSD (for solid state media).
  default: 8
  see_also:
  - osd_op_num_shards
  flags:
  - startup
  with_legacy: true
- name: osd_skip_data_digest
  type: bool
  level: dev
  desc: Do not store full-object checksums if the backend (bluestore) does its own
    checksums.  Only usable with all BlueStore OSDs.
  default: false
# PrioritzedQueue (prio), Weighted Priority Queue (wpq ; default),
# mclock_opclass, mclock_client, or debug_random. "mclock_opclass"
# and "mclock_client" are based on the mClock/dmClock algorithm
# (Gulati, et al. 2010). "mclock_opclass" prioritizes based on the
# class the operation belongs to. "mclock_client" does the same but
# also works to ienforce fairness between clients. "debug_random"
# chooses among all four with equal probability.
- name: osd_op_queue
  type: str
  level: advanced
  desc: which operation priority queue algorithm to use
  long_desc: which operation priority queue algorithm to use
  fmt_desc: This sets the type of queue to be used for prioritizing ops
    within each OSD. Both queues feature a strict sub-queue which is
    dequeued before the normal queue. The normal queue is different
    between implementations. The WeightedPriorityQueue (``wpq``)
    dequeues operations in relation to their priorities to prevent
    starvation of any queue. WPQ should help in cases where a few OSDs
    are more overloaded than others. The mClockQueue
    (``mclock_scheduler``) prioritizes operations based on which class
    they belong to (recovery, scrub, snaptrim, client op, osd subop).
    See `QoS Based on mClock`_. Requires a restart.
  default: mclock_scheduler
  see_also:
  - osd_op_queue_cut_off
  enum_values:
  - wpq
  - mclock_scheduler
  - debug_random
  with_legacy: true
# Min priority to go to strict queue. (low, high)
- name: osd_op_queue_cut_off
  type: str
  level: advanced
  desc: the threshold between high priority ops and low priority ops
  long_desc: the threshold between high priority ops that use strict priority ordering
    and low priority ops that use a fairness algorithm that may or may not incorporate
    priority
  fmt_desc: This selects which priority ops will be sent to the strict
    queue verses the normal queue. The ``low`` setting sends all
    replication ops and higher to the strict queue, while the ``high``
    option sends only replication acknowledgment ops and higher to
    the strict queue. Setting this to ``high`` should help when a few
    OSDs in the cluster are very busy especially when combined with
    ``wpq`` in the ``osd_op_queue`` setting. OSDs that are very busy
    handling replication traffic could starve primary client traffic
    on these OSDs without these settings. Requires a restart.
  default: high
  see_also:
  - osd_op_queue
  enum_values:
  - low
  - high
  - debug_random
  with_legacy: true
- name: osd_mclock_scheduler_client_res
  type: uint
  level: advanced
  desc: IO proportion reserved for each client (default)
  long_desc: Only considered for osd_op_queue = mclock_scheduler
  fmt_desc: IO proportion reserved for each client (default).
  default: 1
  see_also:
  - osd_op_queue
- name: osd_mclock_scheduler_client_wgt
  type: uint
  level: advanced
  desc: IO share for each client (default) over reservation
  long_desc: Only considered for osd_op_queue = mclock_scheduler
  fmt_desc: IO share for each client (default) over reservation.
  default: 1
  see_also:
  - osd_op_queue
- name: osd_mclock_scheduler_client_lim
  type: uint
  level: advanced
  desc: IO limit for each client (default) over reservation
  long_desc: Only considered for osd_op_queue = mclock_scheduler
  fmt_desc: IO limit for each client (default) over reservation.
  default: 999999
  see_also:
  - osd_op_queue
- name: osd_mclock_scheduler_background_recovery_res
  type: uint
  level: advanced
  desc: IO proportion reserved for background recovery (default)
  long_desc: Only considered for osd_op_queue = mclock_scheduler
  fmt_desc: IO proportion reserved for background recovery (default).
  default: 1
  see_also:
  - osd_op_queue
- name: osd_mclock_scheduler_background_recovery_wgt
  type: uint
  level: advanced
  desc: IO share for each background recovery over reservation
  long_desc: Only considered for osd_op_queue = mclock_scheduler
  fmt_desc: IO share for each background recovery over reservation.
  default: 1
  see_also:
  - osd_op_queue
- name: osd_mclock_scheduler_background_recovery_lim
  type: uint
  level: advanced
  desc: IO limit for background recovery over reservation
  long_desc: Only considered for osd_op_queue = mclock_scheduler
  fmt_desc: IO limit for background recovery over reservation.
  default: 999999
  see_also:
  - osd_op_queue
- name: osd_mclock_scheduler_background_best_effort_res
  type: uint
  level: advanced
  desc: IO proportion reserved for background best_effort (default)
  long_desc: Only considered for osd_op_queue = mclock_scheduler
  fmt_desc: IO proportion reserved for background best_effort (default).
  default: 1
  see_also:
  - osd_op_queue
- name: osd_mclock_scheduler_background_best_effort_wgt
  type: uint
  level: advanced
  desc: IO share for each background best_effort over reservation
  long_desc: Only considered for osd_op_queue = mclock_scheduler
  fmt_desc: IO share for each background best_effort over reservation.
  default: 1
  see_also:
  - osd_op_queue
- name: osd_mclock_scheduler_background_best_effort_lim
  type: uint
  level: advanced
  desc: IO limit for background best_effort over reservation
  long_desc: Only considered for osd_op_queue = mclock_scheduler
  fmt_desc: IO limit for background best_effort over reservation.
  default: 999999
  see_also:
  - osd_op_queue
- name: osd_mclock_scheduler_anticipation_timeout
  type: float
  level: advanced
  desc: mclock anticipation timeout in seconds
  long_desc: the amount of time that mclock waits until the unused resource is forfeited
  default: 0
- name: osd_mclock_cost_per_io_usec
  type: float
  level: dev
  desc: Cost per IO in microseconds to consider per OSD (overrides _ssd and _hdd if
    non-zero)
  long_desc: This option specifies the cost factor to consider in usec per OSD. This
    is considered by the mclock scheduler to set an additional cost factor in QoS
    calculations. Only considered for osd_op_queue = mclock_scheduler
  fmt_desc: Cost per IO in microseconds to consider per OSD (overrides _ssd
    and _hdd if non-zero)
  default: 0
  flags:
  - runtime
- name: osd_mclock_cost_per_io_usec_hdd
  type: float
  level: dev
  desc: Cost per IO in microseconds to consider per OSD (for rotational media)
  long_desc: This option specifies the cost factor to consider in usec per OSD for
    rotational device type. This is considered by the mclock_scheduler to set an additional
    cost factor in QoS calculations. Only considered for osd_op_queue = mclock_scheduler
  fmt_desc: Cost per IO in microseconds to consider per OSD (for rotational
    media)
  default: 25000
  flags:
  - runtime
- name: osd_mclock_cost_per_io_usec_ssd
  type: float
  level: dev
  desc: Cost per IO in microseconds to consider per OSD (for solid state media)
  long_desc: This option specifies the cost factor to consider in usec per OSD for
    solid state device type. This is considered by the mclock_scheduler to set an
    additional cost factor in QoS calculations. Only considered for osd_op_queue =
    mclock_scheduler
  fmt_desc: Cost per IO in microseconds to consider per OSD (for solid state
    media)
  default: 50
  flags:
  - runtime
- name: osd_mclock_cost_per_byte_usec
  type: float
  level: dev
  desc: Cost per byte in microseconds to consider per OSD (overrides _ssd and _hdd
    if non-zero)
  long_desc: This option specifies the cost per byte to consider in microseconds per
    OSD. This is considered by the mclock scheduler to set an additional cost factor
    in QoS calculations. Only considered for osd_op_queue = mclock_scheduler
  fmt_desc: Cost per byte in microseconds to consider per OSD (overrides _ssd
    and _hdd if non-zero)
  default: 0
  flags:
  - runtime
- name: osd_mclock_cost_per_byte_usec_hdd
  type: float
  level: dev
  desc: Cost per byte in microseconds to consider per OSD (for rotational media)
  long_desc: This option specifies the cost per byte to consider in microseconds per
    OSD for rotational device type. This is considered by the mclock_scheduler to
    set an additional cost factor in QoS calculations. Only considered for osd_op_queue
    = mclock_scheduler
  fmt_desc: Cost per byte in microseconds to consider per OSD (for rotational
    media)
  default: 5.2
  flags:
  - runtime
- name: osd_mclock_cost_per_byte_usec_ssd
  type: float
  level: dev
  desc: Cost per byte in microseconds to consider per OSD (for solid state media)
  long_desc: This option specifies the cost per byte to consider in microseconds per
    OSD for solid state device type. This is considered by the mclock_scheduler to
    set an additional cost factor in QoS calculations. Only considered for osd_op_queue
    = mclock_scheduler
  fmt_desc: Cost per byte in microseconds to consider per OSD (for solid state
    media)
  default: 0.011
  flags:
  - runtime
- name: osd_mclock_max_capacity_iops
  type: float
  level: basic
  desc: Max IOPs capacity (at 4KiB block size) to consider per OSD (overrides _ssd
    and _hdd if non-zero)
  long_desc: This option specifies the max osd capacity in iops per OSD. Helps in
    QoS calculations when enabling a dmclock profile. Only considered for osd_op_queue
    = mclock_scheduler
  fmt_desc: Max IOPS capacity (at 4KiB block size) to consider per OSD
    (overrides _ssd and _hdd if non-zero)
  default: 0
  flags:
  - runtime
- name: osd_mclock_max_capacity_iops_hdd
  type: float
  level: basic
  desc: Max IOPs capacity (at 4KiB block size) to consider per OSD (for rotational
    media)
  long_desc: This option specifies the max OSD capacity in iops per OSD. Helps in
    QoS calculations when enabling a dmclock profile. Only considered for osd_op_queue
    = mclock_scheduler
  fmt_desc: Max IOPS capacity (at 4KiB block size) to consider per OSD (for
    rotational media)
  default: 315
  flags:
  - runtime
- name: osd_mclock_max_capacity_iops_ssd
  type: float
  level: basic
  desc: Max IOPs capacity (at 4KiB block size) to consider per OSD (for solid state
    media)
  long_desc: This option specifies the max OSD capacity in iops per OSD. Helps in
    QoS calculations when enabling a dmclock profile. Only considered for osd_op_queue
    = mclock_scheduler
  fmt_desc: Max IOPS capacity (at 4KiB block size) to consider per OSD (for
    solid state media)
  default: 21500
  flags:
  - runtime
- name: osd_mclock_profile
  type: str
  level: advanced
  desc: Which mclock profile to use
  long_desc: This option specifies the mclock profile to enable - one among the set
    of built-in profiles or a custom profile. Only considered for osd_op_queue = mclock_scheduler
  fmt_desc: |
    This sets the type of mclock profile to use for providing QoS
    based on operations belonging to different classes (background
    recovery, scrub, snaptrim, client op, osd subop). Once a built-in
    profile is enabled, the lower level mclock resource control
    parameters [*reservation, weight, limit*] and some Ceph
    configuration parameters are set transparently. Note that the
    above does not apply for the *custom* profile.
  default: high_client_ops
  see_also:
  - osd_op_queue
  enum_values:
  - balanced
  - high_recovery_ops
  - high_client_ops
  - custom
  flags:
  - runtime
# do not assert on divergent_prior entries which aren't in the log and whose on-disk objects are newer
- name: osd_ignore_stale_divergent_priors
  type: bool
  level: advanced
  default: false
  with_legacy: true
# Set to true for testing.  Users should NOT set this.
# If set to true even after reading enough shards to
# decode the object, any error will be reported.
- name: osd_read_ec_check_for_errors
  type: bool
  level: advanced
  default: false
  with_legacy: true
# Only use clone_overlap for recovery if there are fewer than
# osd_recover_clone_overlap_limit entries in the overlap set
- name: osd_recover_clone_overlap_limit
  type: uint
  level: advanced
  default: 10
  flags:
  - runtime
- name: osd_debug_feed_pullee
  type: int
  level: dev
  desc: Feed a pullee, and force primary to pull a currently missing object from it
  default: -1
  with_legacy: true
- name: osd_backfill_scan_min
  type: int
  level: advanced
  default: 64
  fmt_desc: The minimum number of objects per backfill scan.
  with_legacy: true
- name: osd_backfill_scan_max
  type: int
  level: advanced
  default: 512
  fmt_desc: The maximum number of objects per backfill scan.p
  with_legacy: true
- name: osd_op_thread_timeout
  type: int
  level: advanced
  default: 15
  fmt_desc: The Ceph OSD Daemon operation thread timeout in seconds.
  with_legacy: true
- name: osd_op_thread_suicide_timeout
  type: int
  level: advanced
  default: 150
  with_legacy: true
- name: osd_recovery_sleep
  type: float
  level: advanced
  desc: Time in seconds to sleep before next recovery or backfill op
  fmt_desc: Time in seconds to sleep before the next recovery or backfill op.
    Increasing this value will slow down recovery operation while
    client operations will be less impacted.
  default: 0
  flags:
  - runtime
  with_legacy: true
- name: osd_recovery_sleep_hdd
  type: float
  level: advanced
  desc: Time in seconds to sleep before next recovery or backfill op for HDDs
  fmt_desc: Time in seconds to sleep before next recovery or backfill op
    for HDDs.
  default: 0.1
  flags:
  - runtime
  with_legacy: true
- name: osd_recovery_sleep_ssd
  type: float
  level: advanced
  desc: Time in seconds to sleep before next recovery or backfill op for SSDs
  fmt_desc: Time in seconds to sleep before the next recovery or backfill op
    for SSDs.
  default: 0
  see_also:
  - osd_recovery_sleep
  flags:
  - runtime
  with_legacy: true
- name: osd_recovery_sleep_hybrid
  type: float
  level: advanced
  desc: Time in seconds to sleep before next recovery or backfill op when data is
    on HDD and journal is on SSD
  fmt_desc: Time in seconds to sleep before the next recovery or backfill op
    when OSD data is on HDD and OSD journal / WAL+DB is on SSD.
  default: 0.025
  see_also:
  - osd_recovery_sleep
  flags:
  - runtime
- name: osd_snap_trim_sleep
  type: float
  level: advanced
  desc: Time in seconds to sleep before next snap trim (overrides values below)
  fmt_desc: Time in seconds to sleep before next snap trim op.
    Increasing this value will slow down snap trimming.
    This option overrides backend specific variants.
  default: 0
  flags:
  - runtime
  with_legacy: true
- name: osd_snap_trim_sleep_hdd
  type: float
  level: advanced
  desc: Time in seconds to sleep before next snap trim for HDDs
  default: 5
  flags:
  - runtime
- name: osd_snap_trim_sleep_ssd
  type: float
  level: advanced
  desc: Time in seconds to sleep before next snap trim for SSDs
  fmt_desc: Time in seconds to sleep before next snap trim op
    for SSD OSDs (including NVMe).
  default: 0
  flags:
  - runtime
- name: osd_snap_trim_sleep_hybrid
  type: float
  level: advanced
  desc: Time in seconds to sleep before next snap trim when data is on HDD and journal
    is on SSD
  fmt_desc: Time in seconds to sleep before next snap trim op
    when OSD data is on an HDD and the OSD journal or WAL+DB is on an SSD.
  default: 2
  flags:
  - runtime
- name: osd_scrub_invalid_stats
  type: bool
  level: advanced
  default: true
  with_legacy: true
- name: osd_command_thread_timeout
  type: int
  level: advanced
  default: 10_min
  fmt_desc: The maximum time in seconds before timing out a command thread.
  with_legacy: true
- name: osd_command_thread_suicide_timeout
  type: int
  level: advanced
  default: 15_min
  with_legacy: true
- name: osd_heartbeat_interval
  type: int
  level: dev
  desc: Interval (in seconds) between peer pings
  fmt_desc: How often an Ceph OSD Daemon pings its peers (in seconds).
  default: 6
  min: 1
  max: 1_min
  with_legacy: true
# (seconds) how long before we decide a peer has failed
# This setting is read by the MONs and OSDs and has to be set to a equal value in both settings of the configuration
- name: osd_heartbeat_grace
  type: int
  level: advanced
  default: 20
  fmt_desc: The elapsed time when a Ceph OSD Daemon hasn't shown a heartbeat
              that the Ceph Storage Cluster considers it ``down``.
              This setting must be set in both the [mon] and [osd] or [global]
              sections so that it is read by both monitor and OSD daemons.
  with_legacy: true
- name: osd_heartbeat_stale
  type: int
  level: advanced
  desc: Interval (in seconds) we mark an unresponsive heartbeat peer as stale.
  long_desc: Automatically mark unresponsive heartbeat sessions as stale and tear
    them down. The primary benefit is that OSD doesn't need to keep a flood of blocked
    heartbeat messages around in memory.
  default: 10_min
# minimum number of peers
- name: osd_heartbeat_min_peers
  type: int
  level: advanced
  default: 10
  with_legacy: true
# prio the heartbeat tcp socket and set dscp as CS6 on it if true
- name: osd_heartbeat_use_min_delay_socket
  type: bool
  level: advanced
  default: false
  with_legacy: true
# the minimum size of OSD heartbeat messages to send
- name: osd_heartbeat_min_size
  type: size
  level: advanced
  desc: Minimum heartbeat packet size in bytes. Will add dummy payload if heartbeat
    packet is smaller than this.
  default: 2000
  with_legacy: true
# max number of parallel snap trims/pg
- name: osd_pg_max_concurrent_snap_trims
  type: uint
  level: advanced
  default: 2
  with_legacy: true
# max number of trimming pgs
- name: osd_max_trimming_pgs
  type: uint
  level: advanced
  default: 2
  with_legacy: true
# minimum number of peers that must be reachable to mark ourselves
# back up after being wrongly marked down.
- name: osd_heartbeat_min_healthy_ratio
  type: float
  level: advanced
  default: 0.33
  with_legacy: true
# (seconds) how often to ping monitor if no peers
- name: osd_mon_heartbeat_interval
  type: int
  level: advanced
  default: 30
  fmt_desc: How often the Ceph OSD Daemon pings a Ceph Monitor if it has no
              Ceph OSD Daemon peers.
  with_legacy: true
- name: osd_mon_heartbeat_stat_stale
  type: int
  level: advanced
  desc: Stop reporting on heartbeat ping times not updated for this many seconds.
  long_desc: Stop reporting on old heartbeat information unless this is set to zero
  fmt_desc: Stop reporting on heartbeat ping times which haven't been updated for
              this many seconds.  Set to zero to disable this action.
  default: 1_hr
# failures, up_thru, boot.
- name: osd_mon_report_interval
  type: int
  level: advanced
  desc: Frequency of OSD reports to mon for peer failures, fullness status changes
  fmt_desc: The number of seconds a Ceph OSD Daemon may wait
              from startup or another reportable event before reporting
              to a Ceph Monitor.
  default: 5
  with_legacy: true
# max updates in flight
- name: osd_mon_report_max_in_flight
  type: int
  level: advanced
  default: 2
  with_legacy: true
# (second) how often to send beacon message to monitor
- name: osd_beacon_report_interval
  type: int
  level: advanced
  default: 5_min
  with_legacy: true
# report pg stats for any given pg at least this often
- name: osd_pg_stat_report_interval_max
  type: int
  level: advanced
  default: 500
  with_legacy: true
# Max number of snap intervals to report to mgr in pg_stat_t
- name: osd_max_snap_prune_intervals_per_epoch
  type: uint
  level: dev
  desc: Max number of snap intervals to report to mgr in pg_stat_t
  default: 512
  with_legacy: true
- name: osd_default_data_pool_replay_window
  type: int
  level: advanced
  default: 45
  fmt_desc: The time (in seconds) for an OSD to wait for a client to replay
    a request.
- name: osd_auto_mark_unfound_lost
  type: bool
  level: advanced
  default: false
  with_legacy: true
- name: osd_recovery_delay_start
  type: float
  level: advanced
  default: 0
  fmt_desc: After peering completes, Ceph will delay for the specified number
    of seconds before starting to recover RADOS objects.
  with_legacy: true
- name: osd_recovery_max_active
  type: uint
  level: advanced
  desc: Number of simultaneous active recovery operations per OSD (overrides _ssd
    and _hdd if non-zero)
  fmt_desc: The number of active recovery requests per OSD at one time. More
    requests will accelerate recovery, but the requests places an
    increased load on the cluster.
  note: This value is only used if it is non-zero. Normally it
    is ``0``, which means that the ``hdd`` or ``ssd`` values
    (below) are used, depending on the type of the primary
    device backing the OSD.
  default: 0
  see_also:
  - osd_recovery_max_active_hdd
  - osd_recovery_max_active_ssd
  flags:
  - runtime
  with_legacy: true
- name: osd_recovery_max_active_hdd
  type: uint
  level: advanced
  desc: Number of simultaneous active recovery operations per OSD (for rotational
    devices)
  fmt_desc: The number of active recovery requests per OSD at one time, if the
    primary device is rotational.
  default: 3
  see_also:
  - osd_recovery_max_active
  - osd_recovery_max_active_ssd
  flags:
  - runtime
  with_legacy: true
- name: osd_recovery_max_active_ssd
  type: uint
  level: advanced
  desc: Number of simultaneous active recovery operations per OSD (for non-rotational
    solid state devices)
  fmt_desc: The number of active recovery requests per OSD at one time, if the
    primary device is non-rotational (i.e., an SSD).
  default: 10
  see_also:
  - osd_recovery_max_active
  - osd_recovery_max_active_hdd
  flags:
  - runtime
  with_legacy: true
- name: osd_recovery_max_single_start
  type: uint
  level: advanced
  default: 1
  fmt_desc: The maximum number of recovery operations per OSD that will be
    newly started when an OSD is recovering.
  with_legacy: true
# max size of push chunk
- name: osd_recovery_max_chunk
  type: size
  level: advanced
  default: 8_M
  fmt_desc: the maximum total size of data chunks a recovery op can carry.
  with_legacy: true
# max number of omap entries per chunk; 0 to disable limit
- name: osd_recovery_max_omap_entries_per_chunk
  type: uint
  level: advanced
  default: 8096
  with_legacy: true
# max size of a COPYFROM chunk
- name: osd_copyfrom_max_chunk
  type: size
  level: advanced
  default: 8_M
  with_legacy: true
# push cost per object
- name: osd_push_per_object_cost
  type: size
  level: advanced
  default: 1000
  fmt_desc: the overhead for serving a push op
  with_legacy: true
# max size of push message
- name: osd_max_push_cost
  type: size
  level: advanced
  default: 8_M
  with_legacy: true
# max objects in single push op
- name: osd_max_push_objects
  type: uint
  level: advanced
  default: 10
  with_legacy: true
- name: osd_max_scrubs
  type: int
  level: advanced
  desc: Maximum concurrent scrubs on a single OSD
  fmt_desc: The maximum number of simultaneous scrub operations for
    a Ceph OSD Daemon.
  default: 1
  with_legacy: true
- name: osd_scrub_during_recovery
  type: bool
  level: advanced
  desc: Allow scrubbing when PGs on the OSD are undergoing recovery
  fmt_desc: Allow scrub during recovery. Setting this to ``false`` will disable
    scheduling new scrub (and deep--scrub) while there is active recovery.
    Already running scrubs will be continued. This might be useful to reduce
    load on busy clusters.
  default: false
  with_legacy: true
- name: osd_repair_during_recovery
  type: bool
  level: advanced
  desc: Allow requested repairing when PGs on the OSD are undergoing recovery
  default: false
  with_legacy: true
- name: osd_scrub_begin_hour
  type: int
  level: advanced
  desc: Restrict scrubbing to this hour of the day or later
  long_desc: Use osd_scrub_begin_hour=0 and osd_scrub_end_hour=0 for the entire day.
  fmt_desc: This restricts scrubbing to this hour of the day or later.
    Use ``osd_scrub_begin_hour = 0`` and ``osd_scrub_end_hour = 0``
    to allow scrubbing the entire day.  Along with ``osd_scrub_end_hour``, they define a time
    window, in which the scrubs can happen.
    But a scrub will be performed
    no matter whether the time window allows or not, as long as the placement
    group's scrub interval exceeds ``osd_scrub_max_interval``.
  default: 0
  see_also:
  - osd_scrub_end_hour
  min: 0
  max: 23
  with_legacy: true
- name: osd_scrub_end_hour
  type: int
  level: advanced
  desc: Restrict scrubbing to hours of the day earlier than this
  long_desc: Use osd_scrub_begin_hour=0 and osd_scrub_end_hour=0 for the entire day.
  fmt_desc: This restricts scrubbing to the hour earlier than this.
    Use ``osd_scrub_begin_hour = 0`` and ``osd_scrub_end_hour = 0`` to allow scrubbing
    for the entire day.  Along with ``osd_scrub_begin_hour``, they define a time
    window, in which the scrubs can happen. But a scrub will be performed
    no matter whether the time window allows or not, as long as the placement
    group's scrub interval exceeds ``osd_scrub_max_interval``.
  default: 0
  see_also:
  - osd_scrub_begin_hour
  min: 0
  max: 23
  with_legacy: true
- name: osd_scrub_begin_week_day
  type: int
  level: advanced
  desc: Restrict scrubbing to this day of the week or later
  long_desc: 0 = Sunday, 1 = Monday, etc. Use osd_scrub_begin_week_day=0 osd_scrub_end_week_day=0
    for the entire week.
  fmt_desc: This restricts scrubbing to this day of the week or later.
    0  = Sunday, 1 = Monday, etc. Use ``osd_scrub_begin_week_day = 0``
    and ``osd_scrub_end_week_day = 0`` to allow scrubbing for the entire week.
    Along with ``osd_scrub_end_week_day``, they define a time window in which
    scrubs can happen. But a scrub will be performed
    no matter whether the time window allows or not, when the PG's
    scrub interval exceeds ``osd_scrub_max_interval``.
  default: 0
  see_also:
  - osd_scrub_end_week_day
  min: 0
  max: 6
  with_legacy: true
- name: osd_scrub_end_week_day
  type: int
  level: advanced
  desc: Restrict scrubbing to days of the week earlier than this
  long_desc: 0 = Sunday, 1 = Monday, etc. Use osd_scrub_begin_week_day=0 osd_scrub_end_week_day=0
    for the entire week.
  fmt_desc: This restricts scrubbing to days of the week earlier than this.
    0 = Sunday, 1 = Monday, etc.  Use ``osd_scrub_begin_week_day = 0``
    and ``osd_scrub_end_week_day = 0`` to allow scrubbing for the entire week.
    Along with ``osd_scrub_begin_week_day``, they define a time
    window, in which the scrubs can happen. But a scrub will be performed
    no matter whether the time window allows or not, as long as the placement
    group's scrub interval exceeds ``osd_scrub_max_interval``.
  default: 0
  see_also:
  - osd_scrub_begin_week_day
  min: 0
  max: 6
  with_legacy: true
- name: osd_scrub_load_threshold
  type: float
  level: advanced
  desc: Allow scrubbing when system load divided by number of CPUs is below this value
  fmt_desc: The normalized maximum load. Ceph will not scrub when the system load
    (as defined by ``getloadavg() / number of online CPUs``) is higher than this number.
    Default is ``0.5``.
  default: 0.5
  with_legacy: true
# if load is low
- name: osd_scrub_min_interval
  type: float
  level: advanced
  desc: Scrub each PG no more often than this interval
  fmt_desc: The minimal interval in seconds for scrubbing the Ceph OSD Daemon
    when the Ceph Storage Cluster load is low.
  default: 1_day
  see_also:
  - osd_scrub_max_interval
  with_legacy: true
# regardless of load
- name: osd_scrub_max_interval
  type: float
  level: advanced
  desc: Scrub each PG no less often than this interval
  fmt_desc: The maximum interval in seconds for scrubbing the Ceph OSD Daemon
    irrespective of cluster load.
  default: 7_day
  see_also:
  - osd_scrub_min_interval
  with_legacy: true
# randomize the scheduled scrub in the span of [min,min*(1+randomize_ratio))
- name: osd_scrub_interval_randomize_ratio
  type: float
  level: advanced
  desc: Ratio of scrub interval to randomly vary
  long_desc: This prevents a scrub 'stampede' by randomly varying the scrub intervals
    so that they are soon uniformly distributed over the week
  fmt_desc: Add a random delay to ``osd_scrub_min_interval`` when scheduling
    the next scrub job for a PG. The delay is a random
    value less than ``osd_scrub_min_interval`` \*
    ``osd_scrub_interval_randomized_ratio``. The default setting
    spreads scrubs throughout the allowed time
    window of ``[1, 1.5]`` \* ``osd_scrub_min_interval``.
  default: 0.5
  see_also:
  - osd_scrub_min_interval
  with_legacy: true
# the probability to back off the scheduled scrub
- name: osd_scrub_backoff_ratio
  type: float
  level: dev
  desc: Backoff ratio for scheduling scrubs
  long_desc: This is the precentage of ticks that do NOT schedule scrubs, 66% means
    that 1 out of 3 ticks will schedule scrubs
  default: 0.66
  with_legacy: true
- name: osd_scrub_chunk_min
  type: int
  level: advanced
  desc: Minimum number of objects to scrub in a single chunk
  fmt_desc: The minimal number of object store chunks to scrub during single operation.
    Ceph blocks writes to single chunk during scrub.
  default: 5
  see_also:
  - osd_scrub_chunk_max
  with_legacy: true
- name: osd_scrub_chunk_max
  type: int
  level: advanced
  desc: Maximum number of objects to scrub in a single chunk
  fmt_desc: The maximum number of object store chunks to scrub during single operation.
  default: 25
  see_also:
  - osd_scrub_chunk_min
  with_legacy: true
# sleep between [deep]scrub ops
- name: osd_scrub_sleep
  type: float
  level: advanced
  desc: Duration to inject a delay during scrubbing
  fmt_desc: Time to sleep before scrubbing the next group of chunks. Increasing this value will slow
    down the overall rate of scrubbing so that client operations will be less impacted.
  default: 0
  flags:
  - runtime
  with_legacy: true
# more sleep between [deep]scrub ops
- name: osd_scrub_extended_sleep
  type: float
  level: advanced
  desc: Duration to inject a delay during scrubbing out of scrubbing hours
  default: 0
  see_also:
  - osd_scrub_begin_hour
  - osd_scrub_end_hour
  - osd_scrub_begin_week_day
  - osd_scrub_end_week_day
  with_legacy: true
# whether auto-repair inconsistencies upon deep-scrubbing
- name: osd_scrub_auto_repair
  type: bool
  level: advanced
  desc: Automatically repair damaged objects detected during scrub
  fmt_desc: Setting this to ``true`` will enable automatic PG repair when errors
    are found by scrubs or deep-scrubs.  However, if more than
    ``osd_scrub_auto_repair_num_errors`` errors are found a repair is NOT performed.
  default: false
  with_legacy: true
# only auto-repair when number of errors is below this threshold
- name: osd_scrub_auto_repair_num_errors
  type: uint
  level: advanced
  desc: Maximum number of detected errors to automatically repair
  fmt_desc: Auto repair will not occur if more than this many errors are found.
  default: 5
  see_also:
  - osd_scrub_auto_repair
  with_legacy: true
- name: osd_scrub_max_preemptions
  type: uint
  level: advanced
  desc: Set the maximum number of times we will preempt a deep scrub due to a client
    operation before blocking client IO to complete the scrub
  default: 5
  min: 0
  max: 30
- name: osd_deep_scrub_interval
  type: float
  level: advanced
  desc: Deep scrub each PG (i.e., verify data checksums) at least this often
  fmt_desc: The interval for "deep" scrubbing (fully reading all data). The
    ``osd_scrub_load_threshold`` does not affect this setting.
  default: 7_day
  with_legacy: true
- name: osd_deep_scrub_randomize_ratio
  type: float
  level: advanced
  desc: Scrubs will randomly become deep scrubs at this rate (0.15 -> 15% of scrubs
    are deep)
  long_desc: This prevents a deep scrub 'stampede' by spreading deep scrubs so they
    are uniformly distributed over the week
  default: 0.15
  with_legacy: true
- name: osd_deep_scrub_stride
  type: size
  level: advanced
  desc: Number of bytes to read from an object at a time during deep scrub
  fmt_desc: Read size when doing a deep scrub.
  default: 512_K
  with_legacy: true
- name: osd_deep_scrub_keys
  type: int
  level: advanced
  desc: Number of keys to read from an object at a time during deep scrub
  default: 1024
  with_legacy: true
# objects must be this old (seconds) before we update the whole-object digest on scrub
- name: osd_deep_scrub_update_digest_min_age
  type: int
  level: advanced
  desc: Update overall object digest only if object was last modified longer ago than
    this
  default: 2_hr
  with_legacy: true
- name: osd_deep_scrub_large_omap_object_key_threshold
  type: uint
  level: advanced
  desc: Warn when we encounter an object with more omap keys than this
  default: 200000
  services:
  - osd
  see_also:
  - osd_deep_scrub_large_omap_object_value_sum_threshold
  with_legacy: true
- name: osd_deep_scrub_large_omap_object_value_sum_threshold
  type: size
  level: advanced
  desc: Warn when we encounter an object with more omap key bytes than this
  default: 1_G
  services:
  - osd
  see_also:
  - osd_deep_scrub_large_omap_object_key_threshold
  with_legacy: true
# where rados plugins are stored
- name: osd_class_dir
  type: str
  level: advanced
  default: @CMAKE_INSTALL_LIBDIR@/rados-classes
  fmt_desc: The class path for RADOS class plug-ins.
  with_legacy: true
- name: osd_open_classes_on_start
  type: bool
  level: advanced
  default: true
  with_legacy: true
# list of object classes allowed to be loaded (allow all: *)
- name: osd_class_load_list
  type: str
  level: advanced
  default: cephfs hello journal lock log numops otp rbd refcount rgw rgw_gc timeindex
    user version cas cmpomap queue 2pc_queue fifo
  with_legacy: true
# list of object classes with default execute perm (allow all: *)
- name: osd_class_default_list
  type: str
  level: advanced
  default: cephfs hello journal lock log numops otp rbd refcount rgw rgw_gc timeindex
    user version cas cmpomap queue 2pc_queue fifo
  with_legacy: true
- name: osd_check_for_log_corruption
  type: bool
  level: advanced
  default: false
  fmt_desc: Check log files for corruption. Can be computationally expensive.
  with_legacy: true
- name: osd_use_stale_snap
  type: bool
  level: advanced
  default: false
  with_legacy: true
- name: osd_rollback_to_cluster_snap
  type: str
  level: advanced
  with_legacy: true
- name: osd_default_notify_timeout
  type: uint
  level: advanced
  desc: default number of seconds after which notify propagation times out. used if
    a client has not specified other value
  fmt_desc: The OSD default notification timeout (in seconds).
  default: 30
  with_legacy: true
- name: osd_kill_backfill_at
  type: int
  level: dev
  default: 0
  with_legacy: true
# Bounds how infrequently a new map epoch will be persisted for a pg
# make this < map_cache_size!
- name: osd_pg_epoch_persisted_max_stale
  type: uint
  level: advanced
  default: 40
  with_legacy: true
- name: osd_target_pg_log_entries_per_osd
  type: uint
  level: dev
  desc: target number of PG entries total on an OSD - limited per pg by the min and
    max options below
  default: 300000
  see_also:
  - osd_max_pg_log_entries
  - osd_min_pg_log_entries
  with_legacy: true
- name: osd_min_pg_log_entries
  type: uint
  level: dev
  desc: minimum number of entries to maintain in the PG log
  fmt_desc: The minimum number of placement group logs to maintain
    when trimming log files.
  default: 250
  services:
  - osd
  see_also:
  - osd_max_pg_log_entries
  - osd_pg_log_dups_tracked
  - osd_target_pg_log_entries_per_osd
  with_legacy: true
- name: osd_max_pg_log_entries
  type: uint
  level: dev
  desc: maximum number of entries to maintain in the PG log
  fmt_desc: The maximum number of placement group logs to maintain
    when trimming log files.
  default: 10000
  services:
  - osd
  see_also:
  - osd_min_pg_log_entries
  - osd_pg_log_dups_tracked
  - osd_target_pg_log_entries_per_osd
  with_legacy: true
- name: osd_pg_log_dups_tracked
  type: uint
  level: dev
  desc: how many versions back to track in order to detect duplicate ops; this is
    combined with both the regular pg log entries and additional minimal dup detection
    entries
  default: 3000
  services:
  - osd
  see_also:
  - osd_min_pg_log_entries
  - osd_max_pg_log_entries
  with_legacy: true
- name: osd_object_clean_region_max_num_intervals
  type: int
  level: dev
  desc: number of intervals in clean_offsets
  long_desc: partial recovery uses multiple intervals to record the clean part of
    the objectwhen the number of intervals is greater than osd_object_clean_region_max_num_intervals,
    minimum interval will be trimmed(0 will recovery the entire object data interval)
  default: 10
  services:
  - osd
  with_legacy: true
# max entries factor before force recovery
- name: osd_force_recovery_pg_log_entries_factor
  type: float
  level: dev
  default: 1.3
  with_legacy: true
- name: osd_pg_log_trim_min
  type: uint
  level: dev
  desc: Minimum number of log entries to trim at once. This lets us trim in larger
    batches rather than with each write.
  default: 100
  see_also:
  - osd_max_pg_log_entries
  - osd_min_pg_log_entries
  with_legacy: true
- name: osd_force_auth_primary_missing_objects
  type: uint
  level: advanced
  desc: Approximate missing objects above which to force auth_log_shard to be primary
    temporarily
  default: 100
- name: osd_async_recovery_min_cost
  type: uint
  level: advanced
  desc: A mixture measure of number of current log entries difference and historical
    missing objects,  above which we switch to use asynchronous recovery when appropriate
  default: 100
  flags:
  - runtime
- name: osd_max_pg_per_osd_hard_ratio
  type: float
  level: advanced
  desc: Maximum number of PG per OSD, a factor of 'mon_max_pg_per_osd'
  long_desc: OSD will refuse to instantiate PG if the number of PG it serves exceeds
    this number.
  fmt_desc: The ratio of number of PGs per OSD allowed by the cluster before the
    OSD refuses to create new PGs. An OSD stops creating new PGs if the number
    of PGs it serves exceeds
    ``osd_max_pg_per_osd_hard_ratio`` \* ``mon_max_pg_per_osd``.
  default: 3
  see_also:
  - mon_max_pg_per_osd
  min: 1
- name: osd_pg_log_trim_max
  type: uint
  level: advanced
  desc: maximum number of entries to remove at once from the PG log
  default: 10000
  services:
  - osd
  see_also:
  - osd_min_pg_log_entries
  - osd_max_pg_log_entries
  with_legacy: true
# how many seconds old makes an op complaint-worthy
- name: osd_op_complaint_time
  type: float
  level: advanced
  default: 30
  fmt_desc: An operation becomes complaint worthy after the specified number
    of seconds have elapsed.
  with_legacy: true
- name: osd_command_max_records
  type: int
  level: advanced
  default: 256
  fmt_desc: Limits the number of lost objects to return.
  with_legacy: true
# max peer osds to report that are blocking our progress
- name: osd_max_pg_blocked_by
  type: uint
  level: advanced
  default: 16
  with_legacy: true
- name: osd_op_log_threshold
  type: int
  level: advanced
  default: 5
  fmt_desc: How many operations logs to display at once.
  with_legacy: true
- name: osd_backoff_on_unfound
  type: bool
  level: advanced
  default: true
  with_legacy: true
# [mainly for debug?] object unreadable/writeable
- name: osd_backoff_on_degraded
  type: bool
  level: advanced
  default: false
  with_legacy: true
# [debug] pg peering
- name: osd_backoff_on_peering
  type: bool
  level: advanced
  default: false
  with_legacy: true
- name: osd_debug_shutdown
  type: bool
  level: dev
  desc: Turn up debug levels during shutdown
  default: false
  with_legacy: true
# crash osd if client ignores a backoff; useful for debugging
- name: osd_debug_crash_on_ignored_backoff
  type: bool
  level: dev
  default: false
  with_legacy: true
- name: osd_debug_inject_dispatch_delay_probability
  type: float
  level: dev
  default: 0
  with_legacy: true
- name: osd_debug_inject_dispatch_delay_duration
  type: float
  level: dev
  default: 0.1
  with_legacy: true
- name: osd_debug_drop_ping_probability
  type: float
  level: dev
  default: 0
  with_legacy: true
- name: osd_debug_drop_ping_duration
  type: int
  level: dev
  default: 0
  with_legacy: true
- name: osd_debug_op_order
  type: bool
  level: dev
  default: false
  with_legacy: true
- name: osd_debug_verify_missing_on_start
  type: bool
  level: dev
  default: false
  with_legacy: true
- name: osd_debug_verify_snaps
  type: bool
  level: dev
  default: false
  with_legacy: true
- name: osd_debug_verify_stray_on_activate
  type: bool
  level: dev
  default: false
  with_legacy: true
- name: osd_debug_skip_full_check_in_backfill_reservation
  type: bool
  level: dev
  default: false
  with_legacy: true
- name: osd_debug_reject_backfill_probability
  type: float
  level: dev
  default: 0
  with_legacy: true
# inject failure during copyfrom completion
- name: osd_debug_inject_copyfrom_error
  type: bool
  level: dev
  default: false
  with_legacy: true
- name: osd_debug_misdirected_ops
  type: bool
  level: dev
  default: false
  with_legacy: true
- name: osd_debug_skip_full_check_in_recovery
  type: bool
  level: dev
  default: false
  with_legacy: true
- name: osd_debug_random_push_read_error
  type: float
  level: dev
  default: 0
  with_legacy: true
- name: osd_debug_verify_cached_snaps
  type: bool
  level: dev
  default: false
  with_legacy: true
- name: osd_debug_deep_scrub_sleep
  type: float
  level: dev
  desc: Inject an expensive sleep during deep scrub IO to make it easier to induce
    preemption
  default: 0
  with_legacy: true
- name: osd_debug_no_acting_change
  type: bool
  level: dev
  default: false
  with_legacy: true
- name: osd_debug_no_purge_strays
  type: bool
  level: dev
  default: false
  with_legacy: truen
- name: osd_debug_pretend_recovery_active
  type: bool
  level: dev
  default: false
  with_legacy: true
# enable/disable OSD op tracking
- name: osd_enable_op_tracker
  type: bool
  level: advanced
  default: true
  with_legacy: true
# The number of shards for holding the ops
- name: osd_num_op_tracker_shard
  type: uint
  level: advanced
  default: 32
  with_legacy: true
# Max number of completed ops to track
- name: osd_op_history_size
  type: uint
  level: advanced
  default: 20
  fmt_desc: The maximum number of completed operations to track.
  with_legacy: true
# Oldest completed op to track
- name: osd_op_history_duration
  type: uint
  level: advanced
  default: 600
  fmt_desc: The oldest completed operation to track.
  with_legacy: true
# Max number of slow ops to track
- name: osd_op_history_slow_op_size
  type: uint
  level: advanced
  default: 20
  with_legacy: true
# track the op if over this threshold
- name: osd_op_history_slow_op_threshold
  type: float
  level: advanced
  default: 10
  with_legacy: true
# to adjust various transactions that batch smaller items
- name: osd_target_transaction_size
  type: int
  level: advanced
  default: 30
  with_legacy: true
- name: osd_delete_sleep
  type: float
  level: advanced
  desc: Time in seconds to sleep before next removal transaction (overrides values
    below)
  fmt_desc: Time in seconds to sleep before the next removal transaction. This
    throttles the PG deletion process.
  default: 0
  flags:
  - runtime
- name: osd_delete_sleep_hdd
  type: float
  level: advanced
  desc: Time in seconds to sleep before next removal transaction for HDDs
  default: 5
  flags:
  - runtime
- name: osd_delete_sleep_ssd
  type: float
  level: advanced
  desc: Time in seconds to sleep before next removal transaction for SSDs
  default: 1
  flags:
  - runtime
- name: osd_delete_sleep_hybrid
  type: float
  level: advanced
  desc: Time in seconds to sleep before next removal transaction when OSD data is on HDD
    and OSD journal or WAL+DB is on SSD
  default: 1
  flags:
  - runtime
# what % full makes an OSD "full" (failsafe)
- name: osd_failsafe_full_ratio
  type: float
  level: advanced
  default: 0.97
  with_legacy: true
- name: osd_fast_shutdown
  type: bool
  level: advanced
  desc: Fast, immediate shutdown
  long_desc: Setting this to false makes the OSD do a slower teardown of all state
    when it receives a SIGINT or SIGTERM or when shutting down for any other reason.  That
    slow shutdown is primarilyy useful for doing memory leak checking with valgrind.
  default: true
  with_legacy: true
- name: osd_fast_shutdown_notify_mon
  type: bool
  level: advanced
  desc: Tell mon about OSD shutdown on immediate shutdown
  long_desc: Tell the monitor the OSD is shutting down on immediate shutdown. This
    helps with cluster log messages from other OSDs reporting it immediately failed.
  default: false
  see_also:
  - osd_fast_shutdown
  - osd_mon_shutdown_timeout
  with_legacy: true
# immediately mark OSDs as down once they refuse to accept connections
- name: osd_fast_fail_on_connection_refused
  type: bool
  level: advanced
  default: true
  fmt_desc: If this option is enabled, crashed OSDs are marked down
    immediately by connected peers and MONs (assuming that the
    crashed OSD host survives). Disable it to restore old
    behavior, at the expense of possible long I/O stalls when
    OSDs crash in the middle of I/O operations.
  with_legacy: true
- name: osd_pg_object_context_cache_count
  type: int
  level: advanced
  default: 64
  with_legacy: true
# true if LTTng-UST tracepoints should be enabled
- name: osd_tracing
  type: bool
  level: advanced
  default: false
  with_legacy: true
# true if function instrumentation should use LTTng
- name: osd_function_tracing
  type: bool
  level: advanced
  default: false
  with_legacy: true
# use fast info attr, if we can
- name: osd_fast_info
  type: bool
  level: advanced
  default: true
  with_legacy: true
# determines whether PGLog::check() compares written out log to stored log
- name: osd_debug_pg_log_writeout
  type: bool
  level: dev
  default: false
  with_legacy: true
# Max number of loop before we reset thread-pool's handle
- name: osd_loop_before_reset_tphandle
  type: uint
  level: advanced
  default: 64
  with_legacy: true
# default timeout while caling WaitInterval on an empty queue
- name: threadpool_default_timeout
  type: int
  level: advanced
  default: 1_min
  with_legacy: true
# default wait time for an empty queue before pinging the hb timeout
- name: threadpool_empty_queue_max_wait
  type: int
  level: advanced
  default: 2
  with_legacy: true
- name: leveldb_log_to_ceph_log
  type: bool
  level: advanced
  default: true
  with_legacy: true
- name: leveldb_write_buffer_size
  type: size
  level: advanced
  default: 8_M
  with_legacy: true
- name: leveldb_cache_size
  type: size
  level: advanced
  default: 128_M
  with_legacy: true
- name: leveldb_block_size
  type: size
  level: advanced
  default: 0
  with_legacy: true
- name: leveldb_bloom_size
  type: int
  level: advanced
  default: 0
  with_legacy: true
- name: leveldb_max_open_files
  type: int
  level: advanced
  default: 0
  with_legacy: true
- name: leveldb_compression
  type: bool
  level: advanced
  default: true
  with_legacy: true
- name: leveldb_paranoid
  type: bool
  level: advanced
  default: false
  with_legacy: true
- name: leveldb_log
  type: str
  level: advanced
  default: /dev/null
  with_legacy: true
- name: leveldb_compact_on_mount
  type: bool
  level: advanced
  default: false
  with_legacy: true
- name: rocksdb_log_to_ceph_log
  type: bool
  level: advanced
  default: true
  with_legacy: true
- name: rocksdb_cache_size
  type: size
  level: advanced
  default: 512_M
  flags:
  - runtime
  with_legacy: true
# ratio of cache for row (vs block)
- name: rocksdb_cache_row_ratio
  type: float
  level: advanced
  default: 0
  with_legacy: true
# rocksdb block cache shard bits, 4 bit -> 16 shards
- name: rocksdb_cache_shard_bits
  type: int
  level: advanced
  default: 4
  with_legacy: true
# 'lru' or 'clock'
- name: rocksdb_cache_type
  type: str
  level: advanced
  default: binned_lru
  with_legacy: true
- name: rocksdb_block_size
  type: size
  level: advanced
  default: 4_K
  with_legacy: true
# Enabling this will have 5-10% impact on performance for the stats collection
- name: rocksdb_perf
  type: bool
  level: advanced
  default: false
  with_legacy: true
# For rocksdb, this behavior will be an overhead of 5%~10%, collected only rocksdb_perf is enabled.
- name: rocksdb_collect_compaction_stats
  type: bool
  level: advanced
  default: false
  with_legacy: true
# For rocksdb, this behavior will be an overhead of 5%~10%, collected only rocksdb_perf is enabled.
- name: rocksdb_collect_extended_stats
  type: bool
  level: advanced
  default: false
  with_legacy: true
# For rocksdb, this behavior will be an overhead of 5%~10%, collected only rocksdb_perf is enabled.
- name: rocksdb_collect_memory_stats
  type: bool
  level: advanced
  default: false
  with_legacy: true
- name: rocksdb_delete_range_threshold
  type: uint
  level: advanced
  desc: The number of keys required to invoke DeleteRange when deleting muliple keys.
  default: 1_M
- name: rocksdb_bloom_bits_per_key
  type: uint
  level: advanced
  desc: Number of bits per key to use for RocksDB's bloom filters.
  long_desc: 'RocksDB bloom filters can be used to quickly answer the question of
    whether or not a key may exist or definitely does not exist in a given RocksDB
    SST file without having to read all keys into memory.  Using a higher bit value
    decreases the likelihood of false positives at the expense of additional disk
    space and memory consumption when the filter is loaded into RAM.  The current
    default value of 20 was found to provide significant performance gains when getattr
    calls are made (such as during new object creation in bluestore) without significant
    memory overhead or cache pollution when combined with rocksdb partitioned index
    filters.  See: https://github.com/facebook/rocksdb/wiki/Partitioned-Index-Filters
    for more information.'
  default: 20
- name: rocksdb_cache_index_and_filter_blocks
  type: bool
  level: dev
  desc: Whether to cache indices and filters in block cache
  long_desc: By default RocksDB will load an SST file's index and bloom filters into
    memory when it is opened and remove them from memory when an SST file is closed.  Thus,
    memory consumption by indices and bloom filters is directly tied to the number
    of concurrent SST files allowed to be kept open.  This option instead stores cached
    indicies and filters in the block cache where they directly compete with other
    cached data.  By default we set this option to true to better account for and
    bound rocksdb memory usage and keep filters in memory even when an SST file is
    closed.
  default: true
- name: rocksdb_cache_index_and_filter_blocks_with_high_priority
  type: bool
  level: dev
  desc: Whether to cache indices and filters in the block cache with high priority
  long_desc: A downside of setting rocksdb_cache_index_and_filter_blocks to true is
    that regular data can push indices and filters out of memory.  Setting this option
    to true means they are cached with higher priority than other data and should
    typically stay in the block cache.
  default: false
- name: rocksdb_pin_l0_filter_and_index_blocks_in_cache
  type: bool
  level: dev
  desc: Whether to pin Level 0 indices and bloom filters in the block cache
  long_desc: A downside of setting rocksdb_cache_index_and_filter_blocks to true is
    that regular data can push indices and filters out of memory.  Setting this option
    to true means that level 0 SST files will always have their indices and filters
    pinned in the block cache.
  default: false
- name: rocksdb_index_type
  type: str
  level: dev
  desc: 'Type of index for SST files: binary_search, hash_search, two_level'
  long_desc: 'This option controls the table index type.  binary_search is a space
    efficient index block that is optimized for block-search-based index. hash_search
    may improve prefix lookup performance at the expense of higher disk and memory
    usage and potentially slower compactions.  two_level is an experimental index
    type that uses two binary search indexes and works in conjunction with partition
    filters.  See: http://rocksdb.org/blog/2017/05/12/partitioned-index-filter.html'
  default: binary_search
- name: rocksdb_partition_filters
  type: bool
  level: dev
  desc: (experimental) partition SST index/filters into smaller blocks
  long_desc: 'This is an experimental option for rocksdb that works in conjunction
    with two_level indices to avoid having to keep the entire filter/index in cache
    when cache_index_and_filter_blocks is true.  The idea is to keep a much smaller
    top-level index in heap/cache and then opportunistically cache the lower level
    indices.  See: https://github.com/facebook/rocksdb/wiki/Partitioned-Index-Filters'
  default: false
- name: rocksdb_metadata_block_size
  type: size
  level: dev
  desc: The block size for index partitions. (0 = rocksdb default)
  default: 4_K
- name: mon_rocksdb_options
  type: str
  level: advanced
  default: write_buffer_size=33554432,compression=kNoCompression,level_compaction_dynamic_level_bytes=true
  with_legacy: true
# osd_*_priority adjust the relative priority of client io, recovery io,
# snaptrim io, etc
#
# osd_*_priority determines the ratio of available io between client and
# recovery.  Each option may be set between
# 1..63.
- name: osd_client_op_priority
  type: uint
  level: advanced
  default: 63
  fmt_desc: The priority set for client operations.  This value is relative
    to that of ``osd_recovery_op_priority`` below.  The default
    strongly favors client ops over recovery.
  with_legacy: true
- name: osd_recovery_op_priority
  type: uint
  level: advanced
  desc: Priority to use for recovery operations if not specified for the pool
  fmt_desc: The priority of recovery operations vs client operations, if not specified by the
    pool's ``recovery_op_priority``.  The default value prioritizes client
    ops (see above) over recovery ops.  You may adjust the tradeoff of client
    impact against the time to restore cluster health by lowering this value
    for increased prioritization of client ops, or by increasing it to favor
    recovery.
  default: 3
  with_legacy: true
- name: osd_peering_op_priority
  type: uint
  level: dev
  default: 255
  with_legacy: true
- name: osd_snap_trim_priority
  type: uint
  level: advanced
  default: 5
  fmt_desc: The priority set for the snap trim work queue.
  with_legacy: true
- name: osd_snap_trim_cost
  type: size
  level: advanced
  default: 1_M
  with_legacy: true
- name: osd_pg_delete_priority
  type: uint
  level: advanced
  default: 5
  with_legacy: true
- name: osd_pg_delete_cost
  type: size
  level: advanced
  default: 1_M
  with_legacy: true
- name: osd_scrub_priority
  type: uint
  level: advanced
  desc: Priority for scrub operations in work queue
  fmt_desc: The default work queue priority for scheduled scrubs when the
    pool doesn't specify a value of ``scrub_priority``.  This can be
    boosted to the value of ``osd_client_op_priority`` when scrubs are
    blocking client operations.
  default: 5
  with_legacy: true
- name: osd_scrub_cost
  type: size
  level: advanced
  desc: Cost for scrub operations in work queue
  default: 50_M
  with_legacy: true
# set requested scrub priority higher than scrub priority to make the
# requested scrubs jump the queue of scheduled scrubs
- name: osd_requested_scrub_priority
  type: uint
  level: advanced
  default: 120
  fmt_desc: The priority set for user requested scrub on the work queue.  If
    this value were to be smaller than ``osd_client_op_priority`` it
    can be boosted to the value of ``osd_client_op_priority`` when
    scrub is blocking client operations.
  with_legacy: true
- name: osd_recovery_priority
  type: uint
  level: advanced
  desc: Priority of recovery in the work queue
  long_desc: Not related to a pool's recovery_priority
  fmt_desc: The default priority set for recovery work queue.  Not
    related to a pool's ``recovery_priority``.
  default: 5
  with_legacy: true
# set default cost equal to 20MB io
- name: osd_recovery_cost
  type: size
  level: advanced
  default: 20_M
  with_legacy: true
# osd_recovery_op_warn_multiple scales the normal warning threshold,
# osd_op_complaint_time, so that slow recovery ops won't cause noise
- name: osd_recovery_op_warn_multiple
  type: uint
  level: advanced
  default: 16
  with_legacy: true
# Max time to wait between notifying mon of shutdown and shutting down
- name: osd_mon_shutdown_timeout
  type: float
  level: advanced
  default: 5
  with_legacy: true
# crash if the OSD has stray PG refs on shutdown
- name: osd_shutdown_pgref_assert
  type: bool
  level: advanced
  default: false
  with_legacy: true
# OSD's maximum object size
- name: osd_max_object_size
  type: size
  level: advanced
  default: 128_M
  fmt_desc: The maximum size of a RADOS object in bytes.
  with_legacy: true
# max rados object name len
- name: osd_max_object_name_len
  type: uint
  level: advanced
  default: 2_K
  with_legacy: true
# max rados object namespace len
- name: osd_max_object_namespace_len
  type: uint
  level: advanced
  default: 256
  with_legacy: true
# max rados attr name len; cannot go higher than 100 chars for file system backends
- name: osd_max_attr_name_len
  type: uint
  level: advanced
  default: 100
  with_legacy: true
- name: osd_max_attr_size
  type: uint
  level: advanced
  default: 0
  with_legacy: true
- name: osd_max_omap_entries_per_request
  type: uint
  level: advanced
  default: 1_K
  with_legacy: true
- name: osd_max_omap_bytes_per_request
  type: size
  level: advanced
  default: 1_G
  with_legacy: true
# osd_recovery_op_warn_multiple scales the normal warning threshold,
# osd_op_complaint_time, so that slow recovery ops won't cause noise
- name: osd_max_write_op_reply_len
  type: size
  level: advanced
  desc: Max size of the per-op payload for requests with the RETURNVEC flag set
  long_desc: This value caps the amount of data (per op; a request may have many ops)
    that will be sent back to the client and recorded in the PG log.
  default: 32
  with_legacy: true
- name: osd_objectstore
  type: str
  level: advanced
  desc: backend type for an OSD (like filestore or bluestore)
  default: bluestore
  enum_values:
  - bluestore
  - filestore
  - memstore
  - kstore
  - seastore
  flags:
  - create
  with_legacy: true
# true if LTTng-UST tracepoints should be enabled
- name: osd_objectstore_tracing
  type: bool
  level: advanced
  default: false
  with_legacy: true
- name: osd_objectstore_fuse
  type: bool
  level: advanced
  default: false
  with_legacy: true
- name: osd_bench_small_size_max_iops
  type: uint
  level: advanced
  default: 100
  with_legacy: true
- name: osd_bench_large_size_max_throughput
  type: size
  level: advanced
  default: 100_M
  with_legacy: true
- name: osd_bench_max_block_size
  type: size
  level: advanced
  default: 64_M
  with_legacy: true
# duration of 'osd bench', capped at 30s to avoid triggering timeouts
- name: osd_bench_duration
  type: uint
  level: advanced
  default: 30
  with_legacy: true
# create a blkin trace for all osd requests
- name: osd_blkin_trace_all
  type: bool
  level: advanced
  default: false
  with_legacy: true
# create a blkin trace for all objecter requests
- name: osdc_blkin_trace_all
  type: bool
  level: advanced
  default: false
  with_legacy: true
- name: osd_discard_disconnected_ops
  type: bool
  level: advanced
  default: true
  with_legacy: true
- name: osd_memory_target
  type: size
  level: basic
  desc: When tcmalloc and cache autotuning is enabled, try to keep this many bytes
    mapped in memory.
  long_desc: The minimum value must be at least equal to osd_memory_base + osd_memory_cache_min.
  fmt_desc: |
    When TCMalloc is available and cache autotuning is enabled, try to
    keep this many bytes mapped in memory. Note: This may not exactly
    match the RSS memory usage of the process.  While the total amount
    of heap memory mapped by the process should usually be close
    to this target, there is no guarantee that the kernel will actually
    reclaim  memory that has been unmapped.  During initial development,
    it was found that some kernels result in the OSD's RSS memory
    exceeding the mapped memory by up to 20%.  It is hypothesised
    however, that the kernel generally may be more aggressive about
    reclaiming unmapped memory when there is a high amount of memory
    pressure.  Your mileage may vary.
  default: 4_G
  see_also:
  - bluestore_cache_autotune
  - osd_memory_cache_min
  - osd_memory_base
  min: 896_M
  flags:
  - runtime
- name: osd_memory_target_cgroup_limit_ratio
  type: float
  level: advanced
  desc: Set the default value for osd_memory_target to the cgroup memory limit (if
    set) times this value
  long_desc: A value of 0 disables this feature.
  default: 0.8
  see_also:
  - osd_memory_target
  min: 0
  max: 1
- name: osd_memory_base
  type: size
  level: dev
  desc: When tcmalloc and cache autotuning is enabled, estimate the minimum amount
    of memory in bytes the OSD will need.
  fmt_desc: When TCMalloc and cache autotuning are enabled, estimate the minimum
    amount of memory in bytes the OSD will need.  This is used to help
    the autotuner estimate the expected aggregate memory consumption of
    the caches.
  default: 768_M
  see_also:
  - bluestore_cache_autotune
  flags:
  - runtime
- name: osd_memory_expected_fragmentation
  type: float
  level: dev
  desc: When tcmalloc and cache autotuning is enabled, estimate the percent of memory
    fragmentation.
  fmt_desc: When TCMalloc and cache autotuning is enabled, estimate the
    percentage of memory fragmentation.  This is used to help the
    autotuner estimate the expected aggregate memory consumption
    of the caches.
  default: 0.15
  see_also:
  - bluestore_cache_autotune
  min: 0
  max: 1
  flags:
  - runtime
- name: osd_memory_cache_min
  type: size
  level: dev
  desc: When tcmalloc and cache autotuning is enabled, set the minimum amount of memory
    used for caches.
  fmt_desc: |
    When TCMalloc and cache autotuning are enabled, set the minimum
    amount of memory used for caches. Note: Setting this value too
    low can result in significant cache thrashing.
  default: 128_M
  see_also:
  - bluestore_cache_autotune
  min: 128_M
  flags:
  - runtime
- name: osd_memory_cache_resize_interval
  type: float
  level: dev
  desc: When tcmalloc and cache autotuning is enabled, wait this many seconds between
    resizing caches.
  fmt_desc: When TCMalloc and cache autotuning are enabled, wait this many
    seconds between resizing caches.  This setting changes the total
    amount of memory available for BlueStore to use for caching.  Note
    that setting this interval too small can result in memory allocator
    thrashing and lower performance.
  default: 1
  see_also:
  - bluestore_cache_autotune
- name: memstore_device_bytes
  type: size
  level: advanced
  default: 1_G
  with_legacy: true
- name: memstore_page_set
  type: bool
  level: advanced
  default: false
  with_legacy: true
- name: memstore_page_size
  type: size
  level: advanced
  default: 64_K
  with_legacy: true
- name: memstore_debug_omit_block_device_write
  type: bool
  level: dev
  desc: write metadata only
  default: false
  see_also:
  - bluestore_debug_omit_block_device_write
  with_legacy: true
- name: objectstore_blackhole
  type: bool
  level: advanced
  default: false
  with_legacy: true
- name: bdev_debug_inflight_ios
  type: bool
  level: dev
  default: false
  with_legacy: true
# if N>0, then ~ 1/N IOs will complete before we crash on flush
- name: bdev_inject_crash
  type: int
  level: dev
  default: 0
  with_legacy: true
# wait N more seconds on flush
- name: bdev_inject_crash_flush_delay
  type: int
  level: dev
  default: 2
  with_legacy: true
- name: bdev_aio
  type: bool
  level: advanced
  default: true
  with_legacy: true
# milliseconds
- name: bdev_aio_poll_ms
  type: int
  level: advanced
  default: 250
  with_legacy: true
- name: bdev_aio_max_queue_depth
  type: int
  level: advanced
  default: 1024
  with_legacy: true
- name: bdev_aio_reap_max
  type: int
  level: advanced
  default: 16
  with_legacy: true
- name: bdev_block_size
  type: size
  level: advanced
  default: 4_K
  with_legacy: true
- name: bdev_debug_aio
  type: bool
  level: dev
  default: false
  with_legacy: true
- name: bdev_debug_aio_suicide_timeout
  type: float
  level: dev
  default: 1_min
  with_legacy: true
- name: bdev_debug_aio_log_age
  type: float
  level: dev
  default: 5
  with_legacy: true
# if yes, osd will unbind all NVMe devices from kernel driver and bind them
# to the uio_pci_generic driver. The purpose is to prevent the case where
# NVMe driver is loaded while osd is running.
- name: bdev_nvme_unbind_from_kernel
  type: bool
  level: advanced
  default: false
  with_legacy: true
- name: bdev_enable_discard
  type: bool
  level: advanced
  default: false
  with_legacy: true
- name: bdev_async_discard
  type: bool
  level: advanced
  default: false
  with_legacy: true
- name: bdev_flock_retry_interval
  type: float
  level: advanced
  desc: interval to retry the flock
  default: 0.1
- name: bdev_flock_retry
  type: uint
  level: advanced
  desc: times to retry the flock
  long_desc: The number of times to retry on getting the block device lock. Programs
    such as systemd-udevd may compete with Ceph for this lock. 0 means 'unlimited'.
  default: 3
- name: bluefs_alloc_size
  type: size
  level: advanced
  desc: Allocation unit size for DB and WAL devices
  default: 1_M
  with_legacy: true
- name: bluefs_shared_alloc_size
  type: size
  level: advanced
  desc: Allocation unit size for primary/shared device
  default: 64_K
  with_legacy: true
- name: bluefs_max_prefetch
  type: size
  level: advanced
  default: 1_M
  with_legacy: true
# alloc when we get this low
- name: bluefs_min_log_runway
  type: size
  level: advanced
  default: 1_M
  with_legacy: true
# alloc this much at a time
- name: bluefs_max_log_runway
  type: size
  level: advanced
  default: 4_M
  with_legacy: true
# before we consider
- name: bluefs_log_compact_min_ratio
  type: float
  level: advanced
  default: 5
  with_legacy: true
# before we consider
- name: bluefs_log_compact_min_size
  type: size
  level: advanced
  default: 16_M
  with_legacy: true
# ignore flush until its this big
- name: bluefs_min_flush_size
  type: size
  level: advanced
  default: 512_K
  with_legacy: true
# sync or async log compaction
- name: bluefs_compact_log_sync
  type: bool
  level: advanced
  default: false
  with_legacy: true
- name: bluefs_buffered_io
  type: bool
  level: advanced
  desc: Enabled buffered IO for bluefs reads.
  long_desc: When this option is enabled, bluefs will in some cases perform buffered
    reads.  This allows the kernel page cache to act as a secondary cache for things
    like RocksDB block reads.  For example, if the rocksdb block cache isn't large
    enough to hold all blocks during OMAP iteration, it may be possible to read them
    from page cache instead of from the disk.  This can dramatically improve
    performance when the osd_memory_target is too small to hold all entries in block
    cache but it does come with downsides.  It has been reported to occasionally
    cause excessive kernel swapping (and associated stalls) under certain workloads.
    Currently the best and most consistent performing combination appears to be
    enabling bluefs_buffered_io and disabling system level swap.  It is possible
    that this recommendation may change in the future however.
  default: true
  with_legacy: true
- name: bluefs_sync_write
  type: bool
  level: advanced
  default: false
  with_legacy: true
- name: bluefs_allocator
  type: str
  level: dev
  default: hybrid
  enum_values:
  - bitmap
  - stupid
  - avl
  - hybrid
  with_legacy: true
- name: bluefs_log_replay_check_allocations
  type: bool
  level: advanced
  desc: Enables checks for allocations consistency during log replay
  default: true
  with_legacy: true
- name: bluefs_replay_recovery
  type: bool
  level: dev
  desc: Attempt to read bluefs log so large that it became unreadable.
  long_desc: If BlueFS log grows to extreme sizes (200GB+) it is likely that it becames
    unreadable. This options enables heuristics that scans devices for missing data.
    DO NOT ENABLE BY DEFAULT
  default: false
  with_legacy: true
- name: bluefs_replay_recovery_disable_compact
  type: bool
  level: advanced
  default: false
  with_legacy: true
- name: bluefs_check_for_zeros
  type: bool
  level: dev
  desc: Check data read for suspicious pages
  long_desc: Looks into data read to check if there is a 4K block entirely filled
    with zeros. If this happens, we re-read data. If there is difference, we print
    error to log.
  default: false
  see_also:
  - bluestore_retry_disk_reads
  flags:
  - runtime
  with_legacy: true
- name: bluestore_bluefs
  type: bool
  level: dev
  desc: Use BlueFS to back rocksdb
  long_desc: BlueFS allows rocksdb to share the same physical device(s) as the rest
    of BlueStore.  It should be used in all cases unless testing/developing an alternative
    metadata database for BlueStore.
  default: true
  flags:
  - create
  with_legacy: true
# mirror to normal Env for debug
- name: bluestore_bluefs_env_mirror
  type: bool
  level: dev
  desc: Mirror bluefs data to file system for testing/validation
  default: false
  flags:
  - create
  with_legacy: true
- name: bluestore_bluefs_max_free
  type: size
  level: advanced
  default: 10_G
  desc: Maximum free space allocated to BlueFS
- name: bluestore_bluefs_alloc_failure_dump_interval
  type: float
  level: advanced
  desc: How frequently (in seconds) to dump allocator onBlueFS space allocation failure
  default: 0
  with_legacy: true
- name: bluestore_spdk_mem
  type: size
  level: dev
  desc: Amount of dpdk memory size in MB
  long_desc: If running multiple SPDK instances per node, you must specify the amount
    of dpdk memory size in MB each instance will use, to make sure each instance uses
    its own dpdk memory
  default: 512
- name: bluestore_spdk_coremask
  type: str
  level: dev
  desc: A hexadecimal bit mask of the cores to run on. Note the core numbering can
    change between platforms and should be determined beforehand
  default: '0x1'
- name: bluestore_spdk_max_io_completion
  type: uint
  level: dev
  desc: Maximal I/Os to be batched completed while checking queue pair completions,
    0 means let spdk library determine it
  default: 0
- name: bluestore_spdk_io_sleep
  type: uint
  level: dev
  desc: Time period to wait if there is no completed I/O from polling
  default: 5
# If you want to use spdk driver, you need to specify NVMe serial number here
# with "spdk:" prefix.
# Users can use 'lspci -vvv -d 8086:0953 | grep "Device Serial Number"' to
# get the serial number of Intel(R) Fultondale NVMe controllers.
# Example:
# bluestore_block_path = spdk:55cd2e404bd73932
- name: bluestore_block_path
  type: str
  level: dev
  desc: Path to block device/file
  flags:
  - create
  with_legacy: true
- name: bluestore_block_size
  type: size
  level: dev
  desc: Size of file to create for backing bluestore
  default: 100_G
  flags:
  - create
  with_legacy: true
- name: bluestore_block_create
  type: bool
  level: dev
  desc: Create bluestore_block_path if it doesn't exist
  default: true
  see_also:
  - bluestore_block_path
  - bluestore_block_size
  flags:
  - create
  with_legacy: true
- name: bluestore_block_db_path
  type: str
  level: dev
  desc: Path for db block device
  flags:
  - create
  with_legacy: true
# rocksdb ssts (hot/warm)
- name: bluestore_block_db_size
  type: uint
  level: dev
  desc: Size of file to create for bluestore_block_db_path
  default: 0
  flags:
  - create
  with_legacy: true
- name: bluestore_block_db_create
  type: bool
  level: dev
  desc: Create bluestore_block_db_path if it doesn't exist
  default: false
  see_also:
  - bluestore_block_db_path
  - bluestore_block_db_size
  flags:
  - create
  with_legacy: true
- name: bluestore_block_wal_path
  type: str
  level: dev
  desc: Path to block device/file backing bluefs wal
  flags:
  - create
  with_legacy: true
# rocksdb wal
- name: bluestore_block_wal_size
  type: size
  level: dev
  desc: Size of file to create for bluestore_block_wal_path
  default: 96_M
  flags:
  - create
  with_legacy: true
- name: bluestore_block_wal_create
  type: bool
  level: dev
  desc: Create bluestore_block_wal_path if it doesn't exist
  default: false
  see_also:
  - bluestore_block_wal_path
  - bluestore_block_wal_size
  flags:
  - create
  with_legacy: true
# whether preallocate space if block/db_path/wal_path is file rather that block device.
- name: bluestore_block_preallocate_file
  type: bool
  level: dev
  desc: Preallocate file created via bluestore_block*_create
  default: false
  flags:
  - create
  with_legacy: true
- name: bluestore_ignore_data_csum
  type: bool
  level: dev
  desc: Ignore checksum errors on read and do not generate an EIO error
  default: false
  flags:
  - runtime
  with_legacy: true
- name: bluestore_csum_type
  type: str
  level: advanced
  desc: Default checksum algorithm to use
  long_desc: crc32c, xxhash32, and xxhash64 are available.  The _16 and _8 variants
    use only a subset of the bits for more compact (but less reliable) checksumming.
  fmt_desc: The default checksum algorithm to use.
  default: crc32c
  enum_values:
  - none
  - crc32c
  - crc32c_16
  - crc32c_8
  - xxhash32
  - xxhash64
  flags:
  - runtime
  with_legacy: true
- name: bluestore_retry_disk_reads
  type: uint
  level: advanced
  desc: Number of read retries on checksum validation error
  long_desc: Retries to read data from the disk this many times when checksum validation
    fails to handle spurious read errors gracefully.
  default: 3
  min: 0
  max: 255
  flags:
  - runtime
  with_legacy: true
- name: bluestore_min_alloc_size
  type: uint
  level: advanced
  desc: Minimum allocation size to allocate for an object
  long_desc: A smaller allocation size generally means less data is read and then
    rewritten when a copy-on-write operation is triggered (e.g., when writing to something
    that was recently snapshotted).  Similarly, less data is journaled before performing
    an overwrite (writes smaller than min_alloc_size must first pass through the BlueStore
    journal).  Larger values of min_alloc_size reduce the amount of metadata required
    to describe the on-disk layout and reduce overall fragmentation.
  default: 0
  flags:
  - create
  with_legacy: true
- name: bluestore_min_alloc_size_hdd
  type: size
  level: advanced
  desc: Default min_alloc_size value for rotational media
  default: 4_K
  see_also:
  - bluestore_min_alloc_size
  flags:
  - create
  with_legacy: true
- name: bluestore_min_alloc_size_ssd
  type: size
  level: advanced
  desc: Default min_alloc_size value for non-rotational (solid state)  media
  default: 4_K
  see_also:
  - bluestore_min_alloc_size
  flags:
  - create
  with_legacy: true
- name: bluestore_max_alloc_size
  type: size
  level: advanced
  desc: Maximum size of a single allocation (0 for no max)
  default: 0
  flags:
  - create
  with_legacy: true
- name: bluestore_prefer_deferred_size
  type: size
  level: advanced
  desc: Writes smaller than this size will be written to the journal and then asynchronously
    written to the device.  This can be beneficial when using rotational media where
    seeks are expensive, and is helpful both with and without solid state journal/wal
    devices.
  default: 0
  flags:
  - runtime
  with_legacy: true
- name: bluestore_prefer_deferred_size_hdd
  type: size
  level: advanced
  desc: Default bluestore_prefer_deferred_size for rotational media
  default: 64_K
  see_also:
  - bluestore_prefer_deferred_size
  flags:
  - runtime
  with_legacy: true
- name: bluestore_prefer_deferred_size_ssd
  type: size
  level: advanced
  desc: Default bluestore_prefer_deferred_size for non-rotational (solid state) media
  default: 0
  see_also:
  - bluestore_prefer_deferred_size
  flags:
  - runtime
  with_legacy: true
- name: bluestore_compression_mode
  type: str
  level: advanced
  desc: Default policy for using compression when pool does not specify
  long_desc: '''none'' means never use compression.  ''passive'' means use compression
    when clients hint that data is compressible.  ''aggressive'' means use compression
    unless clients hint that data is not compressible.  This option is used when the
    per-pool property for the compression mode is not present.'
  fmt_desc: The default policy for using compression if the per-pool property
    ``compression_mode`` is not set. ``none`` means never use
    compression. ``passive`` means use compression when
    :c:func:`clients hint <rados_set_alloc_hint>` that data is
    compressible.  ``aggressive`` means use compression unless
    clients hint that data is not compressible.  ``force`` means use
    compression under all circumstances even if the clients hint that
    the data is not compressible.
  default: none
  enum_values:
  - none
  - passive
  - aggressive
  - force
  flags:
  - runtime
  with_legacy: true
- name: bluestore_compression_algorithm
  type: str
  level: advanced
  desc: Default compression algorithm to use when writing object data
  long_desc: This controls the default compressor to use (if any) if the per-pool
    property is not set.  Note that zstd is *not* recommended for bluestore due to
    high CPU overhead when compressing small amounts of data.
  fmt_desc: The default compressor to use (if any) if the per-pool property
    ``compression_algorithm`` is not set. Note that ``zstd`` is *not*
    recommended for BlueStore due to high CPU overhead when
    compressing small amounts of data.
  default: snappy
  enum_values:
  - ''
  - snappy
  - zlib
  - zstd
  - lz4
  flags:
  - runtime
  with_legacy: true
- name: bluestore_compression_min_blob_size
  type: size
  level: advanced
  desc: Maximum chunk size to apply compression to when random access is expected
    for an object.
  long_desc: Chunks larger than this are broken into smaller chunks before being compressed
  fmt_desc: Chunks smaller than this are never compressed.
    The per-pool property ``compression_min_blob_size`` overrides
    this setting.
  default: 0
  flags:
  - runtime
  with_legacy: true
- name: bluestore_compression_min_blob_size_hdd
  type: size
  level: advanced
  desc: Default value of bluestore_compression_min_blob_size for rotational media
  fmt_desc: Default value of ``bluestore compression min blob size``
    for rotational media.
  default: 8_K
  see_also:
  - bluestore_compression_min_blob_size
  flags:
  - runtime
  with_legacy: true
- name: bluestore_compression_min_blob_size_ssd
  type: size
  level: advanced
  desc: Default value of bluestore_compression_min_blob_size for non-rotational (solid
    state) media
  fmt_desc: Default value of ``bluestore compression min blob size``
    for non-rotational (solid state) media.
  default: 8_K
  see_also:
  - bluestore_compression_min_blob_size
  flags:
  - runtime
  with_legacy: true
- name: bluestore_compression_max_blob_size
  type: size
  level: advanced
  desc: Maximum chunk size to apply compression to when non-random access is expected
    for an object.
  long_desc: Chunks larger than this are broken into smaller chunks before being compressed
  fmt_desc: Chunks larger than this value are broken into smaller blobs of at most
    ``bluestore_compression_max_blob_size`` bytes before being compressed.
    The per-pool property ``compression_max_blob_size`` overrides
    this setting.
  default: 0
  flags:
  - runtime
  with_legacy: true
- name: bluestore_compression_max_blob_size_hdd
  type: size
  level: advanced
  desc: Default value of bluestore_compression_max_blob_size for rotational media
  fmt_desc: Default value of ``bluestore compression max blob size``
    for rotational media.
  default: 64_K
  see_also:
  - bluestore_compression_max_blob_size
  flags:
  - runtime
  with_legacy: true
- name: bluestore_compression_max_blob_size_ssd
  type: size
  level: advanced
  desc: Default value of bluestore_compression_max_blob_size for non-rotational (solid
    state) media
  fmt_desc: Default value of ``bluestore compression max blob size``
    for non-rotational (SSD, NVMe) media.
  default: 64_K
  see_also:
  - bluestore_compression_max_blob_size
  flags:
  - runtime
  with_legacy: true
# Specifies minimum expected amount of saved allocation units
# per single blob to enable compressed blobs garbage collection
- name: bluestore_gc_enable_blob_threshold
  type: int
  level: dev
  default: 0
  flags:
  - runtime
  with_legacy: true
# Specifies minimum expected amount of saved allocation units
# per all blobsb to enable compressed blobs garbage collection
- name: bluestore_gc_enable_total_threshold
  type: int
  level: dev
  default: 0
  flags:
  - runtime
  with_legacy: true
- name: bluestore_max_blob_size
  type: size
  level: dev
  long_desc: Bluestore blobs are collections of extents (ie on-disk data) originating
    from one or more objects.  Blobs can be compressed, typically have checksum data,
    may be overwritten, may be shared (with an extent ref map), or split.  This setting
    controls the maximum size a blob is allowed to be.
  default: 0
  flags:
  - runtime
  with_legacy: true
- name: bluestore_max_blob_size_hdd
  type: size
  level: dev
  default: 64_K
  see_also:
  - bluestore_max_blob_size
  flags:
  - runtime
  with_legacy: true
- name: bluestore_max_blob_size_ssd
  type: size
  level: dev
  default: 64_K
  see_also:
  - bluestore_max_blob_size
  flags:
  - runtime
  with_legacy: true
# Require the net gain of compression at least to be at this ratio,
# otherwise we don't compress.
# And ask for compressing at least 12.5%(1/8) off, by default.
- name: bluestore_compression_required_ratio
  type: float
  level: advanced
  desc: Compression ratio required to store compressed data
  long_desc: If we compress data and get less than this we discard the result and
    store the original uncompressed data.
  fmt_desc: The ratio of the size of the data chunk after
    compression relative to the original size must be at
    least this small in order to store the compressed
    version.
  default: 0.875
  flags:
  - runtime
  with_legacy: true
- name: bluestore_extent_map_shard_max_size
  type: size
  level: dev
  desc: Max size (bytes) for a single extent map shard before splitting
  default: 1200
  with_legacy: true
- name: bluestore_extent_map_shard_target_size
  type: size
  level: dev
  desc: Target size (bytes) for a single extent map shard
  default: 500
  with_legacy: true
- name: bluestore_extent_map_shard_min_size
  type: size
  level: dev
  desc: Min size (bytes) for a single extent map shard before merging
  default: 150
  with_legacy: true
- name: bluestore_extent_map_shard_target_size_slop
  type: float
  level: dev
  desc: Ratio above/below target for a shard when trying to align to an existing extent
    or blob boundary
  default: 0.2
  with_legacy: true
- name: bluestore_extent_map_inline_shard_prealloc_size
  type: size
  level: dev
  desc: Preallocated buffer for inline shards
  default: 256
  with_legacy: true
- name: bluestore_cache_trim_interval
  type: float
  level: advanced
  desc: How frequently we trim the bluestore cache
  default: 0.05
  with_legacy: true
- name: bluestore_cache_trim_max_skip_pinned
  type: uint
  level: dev
  desc: Max pinned cache entries we consider before giving up
  default: 1000
  with_legacy: true
- name: bluestore_cache_type
  type: str
  level: dev
  desc: Cache replacement algorithm
  default: 2q
  enum_values:
  - 2q
  - lru
  with_legacy: true
- name: bluestore_2q_cache_kin_ratio
  type: float
  level: dev
  desc: 2Q paper suggests .5
  default: 0.5
  with_legacy: true
- name: bluestore_2q_cache_kout_ratio
  type: float
  level: dev
  desc: 2Q paper suggests .5
  default: 0.5
  with_legacy: true
- name: bluestore_cache_size
  type: size
  level: dev
  desc: Cache size (in bytes) for BlueStore
  long_desc: This includes data and metadata cached by BlueStore as well as memory
    devoted to rocksdb's cache(s).
  fmt_desc: The amount of memory BlueStore will use for its cache.  If zero,
    ``bluestore_cache_size_hdd`` or ``bluestore_cache_size_ssd`` will
    be used instead.
  default: 0
  with_legacy: true
- name: bluestore_cache_size_hdd
  type: size
  level: dev
  desc: Default bluestore_cache_size for rotational media
  fmt_desc: The default amount of memory BlueStore will use for its cache when
    backed by an HDD.
  default: 1_G
  see_also:
  - bluestore_cache_size
  with_legacy: true
- name: bluestore_cache_size_ssd
  type: size
  level: dev
  desc: Default bluestore_cache_size for non-rotational (solid state) media
  fmt_desc: The default amount of memory BlueStore will use for its cache when
    backed by an SSD.
  default: 3_G
  see_also:
  - bluestore_cache_size
  with_legacy: true
- name: bluestore_cache_meta_ratio
  type: float
  level: dev
  desc: Ratio of bluestore cache to devote to metadata
  default: 0.45
  see_also:
  - bluestore_cache_size
  with_legacy: true
- name: bluestore_cache_kv_ratio
  type: float
  level: dev
  desc: Ratio of bluestore cache to devote to key/value database (RocksDB)
  default: 0.45
  see_also:
  - bluestore_cache_size
  with_legacy: true
- name: bluestore_cache_kv_onode_ratio
  type: float
  level: dev
  desc: Ratio of bluestore cache to devote to kv onode column family (rocksdb)
  default: 0.04
  see_also:
  - bluestore_cache_size
- name: bluestore_cache_autotune
  type: bool
  level: dev
  desc: Automatically tune the ratio of caches while respecting min values.
  fmt_desc: Automatically tune the space ratios assigned to various BlueStore
    caches while respecting minimum values.
  default: true
  see_also:
  - bluestore_cache_size
  - bluestore_cache_meta_ratio
- name: bluestore_cache_autotune_interval
  type: float
  level: dev
  desc: The number of seconds to wait between rebalances when cache autotune is enabled.
  fmt_desc: |
    The number of seconds to wait between rebalances when cache autotune
    is enabled.  This setting changes how quickly the allocation ratios of
    various caches are recomputed.  Note:  Setting this interval too small
    can result in high CPU usage and lower performance.
  default: 5
  see_also:
  - bluestore_cache_autotune
- name: bluestore_alloc_stats_dump_interval
  type: float
  level: dev
  desc: The period (in second) for logging allocation statistics.
  default: 1_day
  with_legacy: true
- name: bluestore_kvbackend
  type: str
  level: dev
  desc: Key value database to use for bluestore
  default: rocksdb
  flags:
  - create
  with_legacy: true
- name: bluestore_allocator
  type: str
  level: advanced
  desc: Allocator policy
  long_desc: Allocator to use for bluestore.  Stupid should only be used for testing.
  default: hybrid
  enum_values:
  - bitmap
  - stupid
  - avl
  - hybrid
  - zoned
  with_legacy: true
- name: bluestore_freelist_blocks_per_key
  type: size
  level: dev
  desc: Block (and bits) per database key
  default: 128
  with_legacy: true
- name: bluestore_bitmapallocator_blocks_per_zone
  type: size
  level: dev
  default: 1_K
  with_legacy: true
- name: bluestore_bitmapallocator_span_size
  type: size
  level: dev
  default: 1_K
  with_legacy: true
- name: bluestore_max_deferred_txc
  type: uint
  level: advanced
  desc: Max transactions with deferred writes that can accumulate before we force
    flush deferred writes
  default: 32
  with_legacy: true
- name: bluestore_max_defer_interval
  type: float
  level: advanced
  desc: max duration to force deferred submit
  default: 3
  with_legacy: true
- name: bluestore_rocksdb_options
  type: str
  level: advanced
  desc: Full set of rocksdb settings to override
  default: compression=kNoCompression,max_write_buffer_number=4,min_write_buffer_number_to_merge=1,recycle_log_file_num=4,write_buffer_size=268435456,writable_file_max_buffer_size=0,compaction_readahead_size=2097152,max_background_compactions=2,max_total_wal_size=1073741824
  with_legacy: true
- name: bluestore_rocksdb_options_annex
  type: str
  level: advanced
  desc: An addition to bluestore_rocksdb_options. Allows setting rocksdb options without
    repeating the existing defaults.
  with_legacy: true
- name: bluestore_rocksdb_cf
  type: bool
  level: advanced
  desc: Enable use of rocksdb column families for bluestore metadata
  fmt_desc: Enables sharding of BlueStore's RocksDB.
    When ``true``, ``bluestore_rocksdb_cfs`` is used.
    Only applied when OSD is doing ``--mkfs``.
  default: true
  verbatim: |
    #ifdef WITH_SEASTAR
    // This is necessary as the Seastar's allocator imposes restrictions
    // on the number of threads that entered malloc/free/*. Unfortunately,
    // RocksDB sharding in BlueStore dramatically lifted the number of
    // threads spawn during RocksDB's init.
    .set_validator([](std::string *value, std::string *error_message) {
      if (const bool parsed_value = strict_strtob(value->c_str(), error_message);
        error_message->empty() && parsed_value) {
        *error_message = "invalid BlueStore sharding configuration."
                         " Be aware any change takes effect only on mkfs!";
        return -EINVAL;
      } else {
        return 0;
      }
    })
    #endif
- name: bluestore_rocksdb_cfs
  type: str
  level: dev
  desc: Definition of column families and their sharding
  long_desc: 'Space separated list of elements: column_def [ ''='' rocksdb_options
    ]. column_def := column_name [ ''('' shard_count [ '','' hash_begin ''-'' [ hash_end
    ] ] '')'' ]. Example: ''I=write_buffer_size=1048576 O(6) m(7,10-)''. Interval
    [hash_begin..hash_end) defines characters to use for hash calculation. Recommended
    hash ranges: O(0-13) P(0-8) m(0-16). Sharding of S,T,C,M,B prefixes is inadvised'
  fmt_desc: Definition of BlueStore's RocksDB sharding.
    The optimal value depends on multiple factors, and modification is invadvisable.
    This setting is used only when OSD is doing ``--mkfs``.
    Next runs of OSD retrieve sharding from disk.
  default: m(3) p(3,0-12) O(3,0-13)=block_cache={type=binned_lru} L P
- name: bluestore_fsck_on_mount
  type: bool
  level: dev
  desc: Run fsck at mount
  default: false
  with_legacy: true
- name: bluestore_fsck_on_mount_deep
  type: bool
  level: dev
  desc: Run deep fsck at mount when bluestore_fsck_on_mount is set to true
  default: false
  with_legacy: true
- name: bluestore_fsck_quick_fix_on_mount
  type: bool
  level: dev
  desc: Do quick-fix for the store at mount
  default: false
  with_legacy: true
- name: bluestore_fsck_on_umount
  type: bool
  level: dev
  desc: Run fsck at umount
  default: false
  with_legacy: true
- name: bluestore_fsck_on_umount_deep
  type: bool
  level: dev
  desc: Run deep fsck at umount when bluestore_fsck_on_umount is set to true
  default: false
  with_legacy: true
- name: bluestore_fsck_on_mkfs
  type: bool
  level: dev
  desc: Run fsck after mkfs
  default: true
  with_legacy: true
- name: bluestore_fsck_on_mkfs_deep
  type: bool
  level: dev
  desc: Run deep fsck after mkfs
  default: false
  with_legacy: true
- name: bluestore_sync_submit_transaction
  type: bool
  level: dev
  desc: Try to submit metadata transaction to rocksdb in queuing thread context
  default: false
  with_legacy: true
- name: bluestore_fsck_read_bytes_cap
  type: size
  level: advanced
  desc: Maximum bytes read at once by deep fsck
  default: 64_M
  flags:
  - runtime
  with_legacy: true
- name: bluestore_fsck_quick_fix_threads
  type: int
  level: advanced
  desc: Number of additional threads to perform quick-fix (shallow fsck) command
  default: 2
  with_legacy: true
- name: bluestore_throttle_bytes
  type: size
  level: advanced
  desc: Maximum bytes in flight before we throttle IO submission
  default: 64_M
  flags:
  - runtime
  with_legacy: true
- name: bluestore_throttle_deferred_bytes
  type: size
  level: advanced
  desc: Maximum bytes for deferred writes before we throttle IO submission
  default: 128_M
  flags:
  - runtime
  with_legacy: true
- name: bluestore_throttle_cost_per_io
  type: size
  level: advanced
  desc: Overhead added to transaction cost (in bytes) for each IO
  default: 0
  flags:
  - runtime
  with_legacy: true
- name: bluestore_throttle_cost_per_io_hdd
  type: uint
  level: advanced
  desc: Default bluestore_throttle_cost_per_io for rotational media
  default: 670000
  see_also:
  - bluestore_throttle_cost_per_io
  flags:
  - runtime
  with_legacy: true
- name: bluestore_throttle_cost_per_io_ssd
  type: uint
  level: advanced
  desc: Default bluestore_throttle_cost_per_io for non-rotation (solid state) media
  default: 4000
  see_also:
  - bluestore_throttle_cost_per_io
  flags:
  - runtime
  with_legacy: true
- name: bluestore_deferred_batch_ops
  type: uint
  level: advanced
  desc: Max number of deferred writes before we flush the deferred write queue
  default: 0
  min: 0
  max: 65535
  flags:
  - runtime
  with_legacy: true
- name: bluestore_deferred_batch_ops_hdd
  type: uint
  level: advanced
  desc: Default bluestore_deferred_batch_ops for rotational media
  default: 64
  see_also:
  - bluestore_deferred_batch_ops
  min: 0
  max: 65535
  flags:
  - runtime
  with_legacy: true
- name: bluestore_deferred_batch_ops_ssd
  type: uint
  level: advanced
  desc: Default bluestore_deferred_batch_ops for non-rotational (solid state) media
  default: 16
  see_also:
  - bluestore_deferred_batch_ops
  min: 0
  max: 65535
  flags:
  - runtime
  with_legacy: true
- name: bluestore_nid_prealloc
  type: int
  level: dev
  desc: Number of unique object ids to preallocate at a time
  default: 1024
  with_legacy: true
- name: bluestore_blobid_prealloc
  type: uint
  level: dev
  desc: Number of unique blob ids to preallocate at a time
  default: 10_K
  with_legacy: true
- name: bluestore_clone_cow
  type: bool
  level: advanced
  desc: Use copy-on-write when cloning objects (versus reading and rewriting them
    at clone time)
  default: true
  flags:
  - runtime
  with_legacy: true
- name: bluestore_default_buffered_read
  type: bool
  level: advanced
  desc: Cache read results by default (unless hinted NOCACHE or WONTNEED)
  default: true
  flags:
  - runtime
  with_legacy: true
- name: bluestore_default_buffered_write
  type: bool
  level: advanced
  desc: Cache writes by default (unless hinted NOCACHE or WONTNEED)
  default: false
  flags:
  - runtime
  with_legacy: true
- name: bluestore_debug_no_reuse_blocks
  type: bool
  level: dev
  default: false
  with_legacy: true
- name: bluestore_debug_small_allocations
  type: int
  level: dev
  default: 0
  with_legacy: true
- name: bluestore_debug_too_many_blobs_threshold
  type: int
  level: dev
  default: 24576
  with_legacy: true
- name: bluestore_debug_freelist
  type: bool
  level: dev
  default: false
  with_legacy: true
- name: bluestore_debug_prefill
  type: float
  level: dev
  desc: simulate fragmentation
  default: 0
  with_legacy: true
- name: bluestore_debug_prefragment_max
  type: size
  level: dev
  default: 1_M
  with_legacy: true
- name: bluestore_debug_inject_read_err
  type: bool
  level: dev
  default: false
  with_legacy: true
- name: bluestore_debug_randomize_serial_transaction
  type: int
  level: dev
  default: 0
  with_legacy: true
- name: bluestore_debug_omit_block_device_write
  type: bool
  level: dev
  default: false
  with_legacy: true
- name: bluestore_debug_fsck_abort
  type: bool
  level: dev
  default: false
  with_legacy: true
- name: bluestore_debug_omit_kv_commit
  type: bool
  level: dev
  default: false
  with_legacy: true
- name: bluestore_debug_permit_any_bdev_label
  type: bool
  level: dev
  default: false
  with_legacy: true
- name: bluestore_debug_random_read_err
  type: float
  level: dev
  default: 0
  with_legacy: true
- name: bluestore_debug_inject_bug21040
  type: bool
  level: dev
  default: false
  with_legacy: true
- name: bluestore_debug_inject_csum_err_probability
  type: float
  level: dev
  desc: inject crc verification errors into bluestore device reads
  default: 0
  with_legacy: true
- name: bluestore_fsck_error_on_no_per_pool_stats
  type: bool
  level: advanced
  desc: Make fsck error (instead of warn) when bluestore lacks per-pool stats, e.g.,
    after an upgrade
  default: false
  with_legacy: true
- name: bluestore_warn_on_bluefs_spillover
  type: bool
  level: advanced
  desc: Enable health indication on bluefs slow device usage
  default: true
  with_legacy: true
- name: bluestore_warn_on_legacy_statfs
  type: bool
  level: advanced
  desc: Enable health indication on lack of per-pool statfs reporting from bluestore
  default: true
  with_legacy: true
- name: bluestore_warn_on_spurious_read_errors
  type: bool
  level: advanced
  desc: Enable health indication when spurious read errors are observed by OSD
  default: true
  with_legacy: true
- name: bluestore_fsck_error_on_no_per_pool_omap
  type: bool
  level: advanced
  desc: Make fsck error (instead of warn) when objects without per-pool omap are found
  default: false
  with_legacy: true
- name: bluestore_fsck_error_on_no_per_pg_omap
  type: bool
  level: advanced
  desc: Make fsck error (instead of warn) when objects without per-pg omap are found
  default: false
  with_legacy: true
- name: bluestore_warn_on_no_per_pool_omap
  type: bool
  level: advanced
  desc: Enable health indication on lack of per-pool omap
  default: true
  with_legacy: true
- name: bluestore_warn_on_no_per_pg_omap
  type: bool
  level: advanced
  desc: Enable health indication on lack of per-pg omap
  default: false
  with_legacy: true
- name: bluestore_log_op_age
  type: float
  level: advanced
  desc: log operation if it's slower than this age (seconds)
  default: 5
  with_legacy: true
- name: bluestore_log_omap_iterator_age
  type: float
  level: advanced
  desc: log omap iteration operation if it's slower than this age (seconds)
  default: 5
  with_legacy: true
- name: bluestore_log_collection_list_age
  type: float
  level: advanced
  desc: log collection list operation if it's slower than this age (seconds)
  default: 1_min
  with_legacy: true
- name: bluestore_debug_enforce_settings
  type: str
  level: dev
  desc: Enforces specific hw profile settings
  long_desc: '''hdd'' enforces settings intended for BlueStore above a rotational
    drive. ''ssd'' enforces settings intended for BlueStore above a solid drive. ''default''
    - using settings for the actual hardware.'
  default: default
  enum_values:
  - default
  - hdd
  - ssd
  with_legacy: true
- name: bluestore_avl_alloc_bf_threshold
  type: uint
  level: dev
  desc: Sets threshold at which shrinking max free chunk size triggers enabling best-fit
    mode.
  long_desc: 'AVL allocator works in two modes: near-fit and best-fit. By default,
    it uses very fast near-fit mode, in which it tries to fit a new block near the
    last allocated block of similar size. The second mode is much slower best-fit
    mode, in which it tries to find an exact match for the requested allocation. This
    mode is used when either the device gets fragmented or when it is low on free
    space. When the largest free block is smaller than ''bluestore_avl_alloc_bf_threshold'',
    best-fit mode is used.'
  default: 128_K
  see_also:
  - bluestore_avl_alloc_bf_free_pct
- name: bluestore_avl_alloc_bf_free_pct
  type: uint
  level: dev
  desc: Sets threshold at which shrinking free space (in %, integer) triggers enabling
    best-fit mode.
  long_desc: 'AVL allocator works in two modes: near-fit and best-fit. By default,
    it uses very fast near-fit mode, in which it tries to fit a new block near the
    last allocated block of similar size. The second mode is much slower best-fit
    mode, in which it tries to find an exact match for the requested allocation. This
    mode is used when either the device gets fragmented or when it is low on free
    space. When free space is smaller than ''bluestore_avl_alloc_bf_free_pct'', best-fit
    mode is used.'
  default: 4
  see_also:
  - bluestore_avl_alloc_bf_threshold
- name: bluestore_hybrid_alloc_mem_cap
  type: uint
  level: dev
  desc: Maximum RAM hybrid allocator should use before enabling bitmap supplement
  default: 64_M
- name: bluestore_volume_selection_policy
  type: str
  level: dev
  desc: Determines bluefs volume selection policy
  long_desc: Determines bluefs volume selection policy. 'use_some_extra' policy allows
    to override RocksDB level granularity and put high level's data to faster device
    even when the level doesn't completely fit there. 'fit_to_fast' policy enables
    using 100% of faster disk capacity and allows the user to turn on 'level_compaction_dynamic_level_bytes'
    option in RocksDB options.
  default: use_some_extra
  enum_values:
  - rocksdb_original
  - use_some_extra
  - fit_to_fast
  with_legacy: true
- name: bluestore_volume_selection_reserved_factor
  type: float
  level: advanced
  desc: DB level size multiplier. Determines amount of space at DB device to bar from
    the usage when 'use some extra' policy is in action. Reserved size is determined
    as sum(L_max_size[0], L_max_size[L-1]) + L_max_size[L] * this_factor
  default: 2
  flags:
  - startup
  with_legacy: true
- name: bluestore_volume_selection_reserved
  type: int
  level: advanced
  desc: Space reserved at DB device and not allowed for 'use some extra' policy usage.
    Overrides 'bluestore_volume_selection_reserved_factor' setting and introduces
    straightforward limit.
  default: 0
  flags:
  - startup
  with_legacy: true
- name: bdev_ioring
  type: bool
  level: advanced
  desc: Enables Linux io_uring API instead of libaio
  default: false
- name: bdev_ioring_hipri
  type: bool
  level: advanced
  desc: Enables Linux io_uring API Use polled IO completions
  default: false
- name: bdev_ioring_sqthread_poll
  type: bool
  level: advanced
  desc: Enables Linux io_uring API Offload submission/completion to kernel thread
  default: false
- name: bluestore_kv_sync_util_logging_s
  type: float
  level: advanced
  desc: KV sync thread utilization logging period
  long_desc: How often (in seconds) to print KV sync thread utilization, not logged
    when set to 0 or when utilization is 0%
  default: 10
  flags:
  - runtime
  with_legacy: true
- name: kstore_max_ops
  type: uint
  level: advanced
  default: 512
  with_legacy: true
- name: kstore_max_bytes
  type: size
  level: advanced
  default: 64_M
  with_legacy: true
- name: kstore_backend
  type: str
  level: advanced
  default: rocksdb
  with_legacy: true
- name: kstore_rocksdb_options
  type: str
  level: advanced
  desc: Options to pass through when RocksDB is used as the KeyValueDB for kstore.
  default: compression=kNoCompression
  with_legacy: true
- name: kstore_fsck_on_mount
  type: bool
  level: advanced
  desc: Whether or not to run fsck on mount for kstore.
  default: false
  with_legacy: true
- name: kstore_fsck_on_mount_deep
  type: bool
  level: advanced
  desc: Whether or not to run deep fsck on mount for kstore
  default: true
  with_legacy: true
- name: kstore_nid_prealloc
  type: uint
  level: advanced
  default: 1_K
  with_legacy: true
- name: kstore_sync_transaction
  type: bool
  level: advanced
  default: false
  with_legacy: true
- name: kstore_sync_submit_transaction
  type: bool
  level: advanced
  default: false
  with_legacy: true
- name: kstore_onode_map_size
  type: uint
  level: advanced
  default: 1_K
  with_legacy: true
- name: kstore_default_stripe_size
  type: size
  level: advanced
  default: 64_K
  with_legacy: true
# rocksdb options that will be used for omap(if omap_backend is rocksdb)
- name: filestore_rocksdb_options
  type: str
  level: dev
  desc: Options to pass through when RocksDB is used as the KeyValueDB for filestore.
  default: max_background_jobs=10,compaction_readahead_size=2097152,compression=kNoCompression
  with_legacy: true
- name: filestore_omap_backend
  type: str
  level: dev
  desc: The KeyValueDB to use for filestore metadata (ie omap).
  default: rocksdb
  enum_values:
  - leveldb
  - rocksdb
  with_legacy: true
- name: filestore_omap_backend_path
  type: str
  level: dev
  desc: The path where the filestore KeyValueDB should store it's database(s).
  with_legacy: true
# filestore wb throttle limits
- name: filestore_wbthrottle_enable
  type: bool
  level: advanced
  desc: Enabling throttling of operations to backing file system
  default: true
  with_legacy: true
- name: filestore_wbthrottle_btrfs_bytes_start_flusher
  type: size
  level: advanced
  desc: Start flushing (fsyncing) when this many bytes are written(btrfs)
  default: 40_M
  with_legacy: true
- name: filestore_wbthrottle_btrfs_bytes_hard_limit
  type: size
  level: advanced
  desc: Block writes when this many bytes haven't been flushed (fsynced) (btrfs)
  default: 400_M
  with_legacy: true
- name: filestore_wbthrottle_btrfs_ios_start_flusher
  type: uint
  level: advanced
  desc: Start flushing (fsyncing) when this many IOs are written (brtrfs)
  default: 500
  with_legacy: true
- name: filestore_wbthrottle_btrfs_ios_hard_limit
  type: uint
  level: advanced
  desc: Block writes when this many IOs haven't been flushed (fsynced) (btrfs)
  default: 5000
  with_legacy: true
- name: filestore_wbthrottle_btrfs_inodes_start_flusher
  type: uint
  level: advanced
  desc: Start flushing (fsyncing) when this many distinct inodes have been modified
    (btrfs)
  default: 500
  with_legacy: true
- name: filestore_wbthrottle_xfs_bytes_start_flusher
  type: size
  level: advanced
  desc: Start flushing (fsyncing) when this many bytes are written(xfs)
  default: 40_M
  with_legacy: true
- name: filestore_wbthrottle_xfs_bytes_hard_limit
  type: size
  level: advanced
  desc: Block writes when this many bytes haven't been flushed (fsynced) (xfs)
  default: 400_M
  with_legacy: true
- name: filestore_wbthrottle_xfs_ios_start_flusher
  type: uint
  level: advanced
  desc: Start flushing (fsyncing) when this many IOs are written (xfs)
  default: 500
  with_legacy: true
- name: filestore_wbthrottle_xfs_ios_hard_limit
  type: uint
  level: advanced
  desc: Block writes when this many IOs haven't been flushed (fsynced) (xfs)
  default: 5000
  with_legacy: true
- name: filestore_wbthrottle_xfs_inodes_start_flusher
  type: uint
  level: advanced
  desc: Start flushing (fsyncing) when this many distinct inodes have been modified
    (xfs)
  default: 500
  with_legacy: true
# These must be less than the fd limit
- name: filestore_wbthrottle_btrfs_inodes_hard_limit
  type: uint
  level: advanced
  desc: Block writing when this many inodes have outstanding writes (btrfs)
  default: 5000
  with_legacy: true
- name: filestore_wbthrottle_xfs_inodes_hard_limit
  type: uint
  level: advanced
  desc: Block writing when this many inodes have outstanding writes (xfs)
  default: 5000
  with_legacy: true
# Introduce a O_DSYNC write in the filestore
- name: filestore_odsync_write
  type: bool
  level: dev
  desc: Write with O_DSYNC
  default: false
  with_legacy: true
# Tests index failure paths
- name: filestore_index_retry_probability
  type: float
  level: dev
  default: 0
  with_legacy: true
# Allow object read error injection
- name: filestore_debug_inject_read_err
  type: bool
  level: dev
  default: false
  with_legacy: true
- name: filestore_debug_random_read_err
  type: float
  level: dev
  default: 0
  with_legacy: true
# Expensive debugging check on sync
- name: filestore_debug_omap_check
  type: bool
  level: dev
  default: false
  with_legacy: true
- name: filestore_omap_header_cache_size
  type: size
  level: dev
  default: 1_K
  with_legacy: true
# Use omap for xattrs for attrs over
# filestore_max_inline_xattr_size or
- name: filestore_max_inline_xattr_size
  type: size
  level: dev
  default: 0
  with_legacy: true
- name: filestore_max_inline_xattr_size_xfs
  type: size
  level: dev
  default: 64_K
  with_legacy: true
- name: filestore_max_inline_xattr_size_btrfs
  type: size
  level: dev
  default: 2_K
  with_legacy: true
- name: filestore_max_inline_xattr_size_other
  type: size
  level: dev
  default: 512
  with_legacy: true
# for more than filestore_max_inline_xattrs attrs
- name: filestore_max_inline_xattrs
  type: uint
  level: dev
  default: 0
  with_legacy: true
- name: filestore_max_inline_xattrs_xfs
  type: uint
  level: dev
  default: 10
  with_legacy: true
- name: filestore_max_inline_xattrs_btrfs
  type: uint
  level: dev
  default: 10
  with_legacy: true
- name: filestore_max_inline_xattrs_other
  type: uint
  level: dev
  default: 2
  with_legacy: true
- name: filestore_max_xattr_value_size
  type: size
  level: dev
  default: 0
  with_legacy: true
- name: filestore_max_xattr_value_size_xfs
  type: size
  level: dev
  default: 64_K
  with_legacy: true
- name: filestore_max_xattr_value_size_btrfs
  type: size
  level: dev
  default: 64_K
  with_legacy: true
# ext4 allows 4k xattrs total including some smallish extra fields and the
# keys.  We're allowing 2 512 inline attrs in addition some some filestore
# replay attrs.  After accounting for those, we still need to fit up to
# two attrs of this value.  That means we need this value to be around 1k
# to be safe.  This is hacky, but it's not worth complicating the code
# to work around ext4's total xattr limit.
- name: filestore_max_xattr_value_size_other
  type: size
  level: dev
  default: 1_K
  with_legacy: true
# track sloppy crcs
- name: filestore_sloppy_crc
  type: bool
  level: dev
  default: false
  with_legacy: true
- name: filestore_sloppy_crc_block_size
  type: size
  level: dev
  default: 64_K
  with_legacy: true
- name: filestore_max_alloc_hint_size
  type: size
  level: dev
  default: 1_M
  with_legacy: true
# seconds
- name: filestore_max_sync_interval
  type: float
  level: advanced
  desc: Period between calls to syncfs(2) and journal trims (seconds)
  default: 5
  with_legacy: true
# seconds
- name: filestore_min_sync_interval
  type: float
  level: dev
  desc: Minimum period between calls to syncfs(2)
  default: 0.01
  with_legacy: true
- name: filestore_btrfs_snap
  type: bool
  level: dev
  default: true
  with_legacy: true
- name: filestore_btrfs_clone_range
  type: bool
  level: advanced
  desc: Use btrfs clone_range ioctl to efficiently duplicate objects
  default: true
  with_legacy: true
# zfsonlinux is still unstable
- name: filestore_zfs_snap
  type: bool
  level: dev
  default: false
  with_legacy: true
- name: filestore_fsync_flushes_journal_data
  type: bool
  level: dev
  default: false
  with_legacy: true
# (try to) use fiemap
- name: filestore_fiemap
  type: bool
  level: advanced
  desc: Use fiemap ioctl(2) to determine which parts of objects are sparse
  default: false
  with_legacy: true
- name: filestore_punch_hole
  type: bool
  level: advanced
  desc: Use fallocate(2) FALLOC_FL_PUNCH_HOLE to efficiently zero ranges of objects
  default: false
  with_legacy: true
# (try to) use seek_data/hole
- name: filestore_seek_data_hole
  type: bool
  level: advanced
  desc: Use lseek(2) SEEK_HOLE and SEEK_DATA to determine which parts of objects are
    sparse
  default: false
  with_legacy: true
- name: filestore_splice
  type: bool
  level: advanced
  desc: Use splice(2) to more efficiently copy data between files
  default: false
  with_legacy: true
- name: filestore_fadvise
  type: bool
  level: advanced
  desc: Use posix_fadvise(2) to pass hints to file system
  default: true
  with_legacy: true
# collect device partition information for management application to use
- name: filestore_collect_device_partition_information
  type: bool
  level: advanced
  desc: Collect metadata about the backing file system on OSD startup
  default: true
  with_legacy: true
# (try to) use extsize for alloc hint NOTE: extsize seems to trigger
# data corruption in xfs prior to kernel 3.5.  filestore will
# implicitly disable this if it cannot confirm the kernel is newer
# than that.
# NOTE: This option involves a tradeoff: When disabled, fragmentation is
# worse, but large sequential writes are faster. When enabled, large
# sequential writes are slower, but fragmentation is reduced.
- name: filestore_xfs_extsize
  type: bool
  level: advanced
  desc: Use XFS extsize ioctl(2) to hint allocator about expected write sizes
  default: false
  with_legacy: true
- name: filestore_journal_parallel
  type: bool
  level: dev
  default: false
  with_legacy: true
- name: filestore_journal_writeahead
  type: bool
  level: dev
  default: false
  with_legacy: true
- name: filestore_journal_trailing
  type: bool
  level: dev
  default: false
  with_legacy: true
- name: filestore_queue_max_ops
  type: uint
  level: advanced
  desc: Max IO operations in flight
  default: 50
  with_legacy: true
- name: filestore_queue_max_bytes
  type: size
  level: advanced
  desc: Max (written) bytes in flight
  default: 100_M
  with_legacy: true
- name: filestore_caller_concurrency
  type: int
  level: dev
  default: 10
  with_legacy: true
# Expected filestore throughput in B/s
- name: filestore_expected_throughput_bytes
  type: float
  level: advanced
  desc: Expected throughput of backend device (aids throttling calculations)
  default: 209715200
  with_legacy: true
# Expected filestore throughput in ops/s
- name: filestore_expected_throughput_ops
  type: float
  level: advanced
  desc: Expected through of backend device in IOPS (aids throttling calculations)
  default: 200
  with_legacy: true
# Filestore max delay multiple.  Defaults to 0 (disabled)
- name: filestore_queue_max_delay_multiple
  type: float
  level: dev
  default: 0
  with_legacy: true
# Filestore high delay multiple.  Defaults to 0 (disabled)
- name: filestore_queue_high_delay_multiple
  type: float
  level: dev
  default: 0
  with_legacy: true
# Filestore max delay multiple ops.  Defaults to 0 (disabled)
- name: filestore_queue_max_delay_multiple_bytes
  type: float
  level: dev
  default: 0
  with_legacy: true
# Filestore high delay multiple bytes.  Defaults to 0 (disabled)
- name: filestore_queue_high_delay_multiple_bytes
  type: float
  level: dev
  default: 0
  with_legacy: true
# Filestore max delay multiple ops.  Defaults to 0 (disabled)
- name: filestore_queue_max_delay_multiple_ops
  type: float
  level: dev
  default: 0
  with_legacy: true
# Filestore high delay multiple ops.  Defaults to 0 (disabled)
- name: filestore_queue_high_delay_multiple_ops
  type: float
  level: dev
  default: 0
  with_legacy: true
- name: filestore_queue_low_threshhold
  type: float
  level: dev
  default: 0.3
  with_legacy: true
- name: filestore_queue_high_threshhold
  type: float
  level: dev
  with_legacy: true
  default: 0.9
- name: filestore_op_threads
  type: int
  level: advanced
  desc: Threads used to apply changes to backing file system
  default: 2
  with_legacy: true
- name: filestore_op_thread_timeout
  type: int
  level: advanced
  desc: Seconds before a worker thread is considered stalled
  default: 1_min
  with_legacy: true
- name: filestore_op_thread_suicide_timeout
  type: int
  level: advanced
  desc: Seconds before a worker thread is considered dead
  default: 3_min
  with_legacy: true
- name: filestore_commit_timeout
  type: float
  level: advanced
  desc: Seconds before backing file system is considered hung
  default: 10_min
  with_legacy: true
- name: filestore_fiemap_threshold
  type: size
  level: dev
  default: 4_K
  with_legacy: true
- name: filestore_merge_threshold
  type: int
  level: dev
  default: -10
  with_legacy: true
- name: filestore_split_multiple
  type: int
  level: dev
  default: 2
  with_legacy: true
- name: filestore_split_rand_factor
  type: uint
  level: dev
  default: 20
  with_legacy: true
- name: filestore_update_to
  type: int
  level: dev
  default: 1000
  with_legacy: true
- name: filestore_blackhole
  type: bool
  level: dev
  default: false
  with_legacy: true
- name: filestore_fd_cache_size
  type: int
  level: dev
  default: 128
  with_legacy: true
- name: filestore_fd_cache_shards
  type: int
  level: dev
  default: 16
  with_legacy: true
- name: filestore_ondisk_finisher_threads
  type: int
  level: dev
  default: 1
  with_legacy: true
- name: filestore_apply_finisher_threads
  type: int
  level: dev
  default: 1
  with_legacy: true
# file onto which store transaction dumps
- name: filestore_dump_file
  type: str
  level: dev
  with_legacy: true
# inject a failure at the n'th opportunity
- name: filestore_kill_at
  type: int
  level: dev
  default: 0
  with_legacy: true
# artificially stall for N seconds in op queue thread
- name: filestore_inject_stall
  type: int
  level: dev
  default: 0
  with_legacy: true
# fail/crash on EIO
- name: filestore_fail_eio
  type: bool
  level: dev
  default: true
  with_legacy: true
- name: filestore_debug_verify_split
  type: bool
  level: dev
  default: false
  with_legacy: true
- name: journal_dio
  type: bool
  level: dev
  default: true
  fmt_desc: Enables direct i/o to the journal. Requires ``journal block
   align`` set to ``true``.
  with_legacy: true
- name: journal_aio
  type: bool
  level: dev
  default: true
  fmt_desc: Enables using ``libaio`` for asynchronous writes to the journal.
   Requires ``journal dio`` set to ``true``. Version 0.61 and later, ``true``.
   Version 0.60 and earlier, ``false``.
  with_legacy: true
- name: journal_force_aio
  type: bool
  level: dev
  default: false
  with_legacy: true
- name: journal_block_size
  type: size
  level: dev
  default: 4_K
  with_legacy: true
- name: journal_block_align
  type: bool
  level: dev
  default: true
  fmt_desc: Block aligns write operations. Required for ``dio`` and ``aio``.
  with_legacy: true
- name: journal_write_header_frequency
  type: uint
  level: dev
  default: 0
  with_legacy: true
- name: journal_max_write_bytes
  type: size
  level: advanced
  desc: Max bytes in flight to journal
  fmt_desc: The maximum number of bytes the journal will write at
   any one time.
  default: 10_M
  with_legacy: true
- name: journal_max_write_entries
  type: int
  level: advanced
  desc: Max IOs in flight to journal
  fmt_desc: The maximum number of entries the journal will write at
   any one time.
  default: 100
  with_legacy: true
# Target range for journal fullness
- name: journal_throttle_low_threshhold
  type: float
  level: dev
  default: 0.6
  with_legacy: true
- name: journal_throttle_high_threshhold
  type: float
  level: dev
  default: 0.9
  with_legacy: true
# Multiple over expected at high_threshhold. Defaults to 0 (disabled).
- name: journal_throttle_high_multiple
  type: float
  level: dev
  default: 0
  with_legacy: true
# Multiple over expected at max.  Defaults to 0 (disabled).
- name: journal_throttle_max_multiple
  type: float
  level: dev
  default: 0
  with_legacy: true
# align data payloads >= this.
- name: journal_align_min_size
  type: size
  level: dev
  default: 64_K
  fmt_desc: Align data payloads greater than the specified minimum.
  with_legacy: true
- name: journal_replay_from
  type: int
  level: dev
  default: 0
  with_legacy: true
- name: mgr_stats_threshold
  type: int
  level: advanced
  desc: Lowest perfcounter priority collected by mgr
  long_desc: Daemons only set perf counter data to the manager daemon if the counter
    has a priority higher than this.
  default: 5
  min: 0
  max: 11
- name: journal_zero_on_create
  type: bool
  level: dev
  default: false
  fmt_desc: Causes the file store to overwrite the entire journal with
   ``0``'s during ``mkfs``.
  with_legacy: true
# assume journal is not corrupt
- name: journal_ignore_corruption
  type: bool
  level: dev
  default: false
  with_legacy: true
# using ssd disk as journal, whether support discard nouse journal-data.
- name: journal_discard
  type: bool
  level: dev
  default: false
  with_legacy: true
# fio data directory for fio-objectstore
- name: fio_dir
  type: str
  level: advanced
  default: /tmp/fio
  with_legacy: true
- name: rados_mon_op_timeout
  type: secs
  level: advanced
  desc: timeout for operations handled by monitors such as statfs (0 is unlimited)
  default: 0
  min: 0
  flags:
  - runtime
- name: rados_osd_op_timeout
  type: secs
  level: advanced
  desc: timeout for operations handled by osds such as write (0 is unlimited)
  default: 0
  min: 0
  flags:
  - runtime
# true if LTTng-UST tracepoints should be enabled
- name: rados_tracing
  type: bool
  level: advanced
  default: false
  with_legacy: true
- name: cephadm_path
  type: str
  level: advanced
  desc: Path to cephadm utility
  default: /usr/sbin/cephadm
  services:
  - mgr
- name: mgr_module_path
  type: str
  level: advanced
  desc: Filesystem path to manager modules.
  default: @CEPH_INSTALL_DATADIR@/mgr
  services:
  - mgr
- name: mgr_standby_modules
  type: bool
  default: true
  level: advanced
  desc: Start modules in standby (redirect) mode when mgr is standby
  long_desc: By default, the standby modules will answer incoming requests with a
    HTTP redirect to the active manager, allowing users to point their browser at any
    mgr node and find their way to an active mgr.  However, this mode is problematic
    when using a load balancer because (1) the redirect locations are usually private
    IPs and (2) the load balancer can't identify which mgr is the right one to send
    traffic to. If a load balancer is being used, set this to false.
- name: mgr_disabled_modules
  type: str
  level: advanced
  desc: List of manager modules never get loaded
  long_desc: A comma delimited list of module names. This list is read by manager
    when it starts. By default, manager loads all modules found in specified 'mgr_module_path',
    and it starts the enabled ones as instructed. The modules in this list will not
    be loaded at all.
  default: @mgr_disabled_modules@
  services:
  - mgr
  see_also:
  - mgr_module_path
  flags:
  - startup
- name: mgr_initial_modules
  type: str
  level: basic
  desc: List of manager modules to enable when the cluster is first started
  long_desc: This list of module names is read by the monitor when the cluster is
    first started after installation, to populate the list of enabled manager modules.  Subsequent
    updates are done using the 'mgr module [enable|disable]' commands.  List may be
    comma or space separated.
  default: restful iostat
  services:
  - mon
  flags:
  - no_mon_update
  - cluster_create
- name: mgr_data
  type: str
  level: advanced
  desc: Filesystem path to the ceph-mgr data directory, used to contain keyring.
  default: /var/lib/ceph/mgr/$cluster-$id
  services:
  - mgr
  flags:
  - no_mon_update
- name: mgr_tick_period
  type: secs
  level: advanced
  desc: Period in seconds of beacon messages to monitor
  default: 2
  services:
  - mgr
- name: mgr_stats_period
  type: int
  level: basic
  desc: Period in seconds of OSD/MDS stats reports to manager
  long_desc: Use this setting to control the granularity of time series data collection
    from daemons.  Adjust upwards if the manager CPU load is too high, or if you simply
    do not require the most up to date performance counter data.
  default: 5
  services:
  - mgr
- name: mgr_client_bytes
  type: size
  level: dev
  default: 128_M
  services:
  - mgr
- name: mgr_client_messages
  type: uint
  level: dev
  default: 512
  services:
  - mgr
- name: mgr_osd_bytes
  type: size
  level: dev
  default: 512_M
  services:
  - mgr
- name: mgr_osd_messages
  type: uint
  level: dev
  default: 8_K
  services:
  - mgr
- name: mgr_mds_bytes
  type: size
  level: dev
  default: 128_M
  services:
  - mgr
- name: mgr_mds_messages
  type: uint
  level: dev
  default: 128
  services:
  - mgr
- name: mgr_mon_bytes
  type: size
  level: dev
  default: 128_M
  services:
  - mgr
- name: mgr_mon_messages
  type: uint
  level: dev
  default: 128
  services:
  - mgr
- name: mgr_connect_retry_interval
  type: float
  level: dev
  default: 1
  services:
  - common
- name: mgr_service_beacon_grace
  type: float
  level: advanced
  desc: Period in seconds from last beacon to manager dropping state about a monitored
    service (RGW, rbd-mirror etc)
  default: 1_min
  services:
  - mgr
- name: mgr_client_service_daemon_unregister_timeout
  type: float
  level: dev
  desc: Time to wait during shutdown to deregister service with mgr
  default: 1
- name: mgr_debug_aggressive_pg_num_changes
  type: bool
  level: dev
  desc: Bypass most throttling and safety checks in pg[p]_num controller
  default: false
  services:
  - mgr
- name: mon_mgr_digest_period
  type: int
  level: dev
  desc: Period in seconds between monitor-to-manager health/status updates
  default: 5
  services:
  - mon
- name: mon_mgr_beacon_grace
  type: secs
  level: advanced
  desc: Period in seconds from last beacon to monitor marking a manager daemon as
    failed
  default: 30
  services:
  - mon
- name: mon_mgr_inactive_grace
  type: int
  level: advanced
  desc: Period in seconds after cluster creation during which cluster may have no
    active manager
  long_desc: This grace period enables the cluster to come up cleanly without raising
    spurious health check failures about managers that aren't online yet
  default: 1_min
  services:
  - mon
- name: mon_mgr_mkfs_grace
  type: int
  level: advanced
  desc: Period in seconds that the cluster may have no active manager before this
    is reported as an ERR rather than a WARN
  default: 2_min
  services:
  - mon
- name: throttler_perf_counter
  type: bool
  level: advanced
  default: true
  with_legacy: true
- name: event_tracing
  type: bool
  level: advanced
  default: false
  with_legacy: true
- name: bluestore_tracing
  type: bool
  level: advanced
  desc: Enable bluestore event tracing.
  default: false
- name: bluestore_throttle_trace_rate
  type: float
  level: advanced
  desc: Rate at which to sample bluestore transactions (per second)
  default: 0
- name: debug_deliberately_leak_memory
  type: bool
  level: dev
  default: false
  with_legacy: true
- name: debug_asserts_on_shutdown
  type: bool
  level: dev
  desc: Enable certain asserts to check for refcounting bugs on shutdown; see http://tracker.ceph.com/issues/21738
  default: false
- name: debug_asok_assert_abort
  type: bool
  level: dev
  desc: allow commands 'assert' and 'abort' via asok for testing crash dumps etc
  default: false
  with_legacy: true
- name: target_max_misplaced_ratio
  type: float
  level: basic
  desc: Max ratio of misplaced objects to target when throttling data rebalancing
    activity
  default: 0.05
- name: device_failure_prediction_mode
  type: str
  level: basic
  desc: Method used to predict device failures
  long_desc: To disable prediction, use 'none',  'local' uses a prediction model that
    runs inside the mgr daemon.  'cloud' will share metrics with a cloud service and
    query the service for devicelife expectancy.
  default: none
  enum_values:
  - none
  - local
  - cloud
  flags:
  - runtime
- name: gss_ktab_client_file
  type: str
  level: advanced
  desc: GSS/KRB5 Keytab file for client authentication
  long_desc: This sets the full path for the GSS/Kerberos client keytab file location.
  default: /var/lib/ceph/$name/gss_client_$name.ktab
  services:
  - mon
  - osd
- name: gss_target_name
  type: str
  level: advanced
  long_desc: This sets the gss target service name.
  default: ceph
  services:
  - mon
  - osd
- name: debug_disable_randomized_ping
  type: bool
  level: dev
  desc: Disable heartbeat ping randomization for testing purposes
  default: false
- name: debug_heartbeat_testing_span
  type: int
  level: dev
  desc: Override 60 second periods for testing only
  default: 0
- name: librados_thread_count
  type: uint
  level: advanced
  desc: Size of thread pool for Objecter
  default: 2
  tags:
  - client
  min: 1
- name: osd_asio_thread_count
  type: uint
  level: advanced
  desc: Size of thread pool for ASIO completions
  default: 2
  tags:
  - osd
  min: 1
- name: cephsqlite_lock_renewal_interval
  type: millisecs
  level: advanced
  desc: number of milliseconds before lock is renewed
  default: 2000
  tags:
  - client
  see_also:
  - cephsqlite_lock_renewal_timeout
  min: 100
- name: cephsqlite_lock_renewal_timeout
  type: millisecs
  level: advanced
  desc: number of milliseconds before transaction lock times out
  long_desc: The amount of time before a running libcephsqlite VFS connection has
    to renew a lock on the database before the lock is automatically lost. If the
    lock is lost, the VFS will abort the process to prevent database corruption.
  default: 30000
  tags:
  - client
  see_also:
  - cephsqlite_lock_renewal_interval
  min: 100
- name: cephsqlite_blocklist_dead_locker
  type: bool
  level: advanced
  desc: blocklist the last dead owner of the database lock
  long_desc: Require that the Ceph SQLite VFS blocklist the last dead owner of the
    database when cleanup was incomplete. DO NOT CHANGE THIS UNLESS YOU UNDERSTAND
    THE RAMIFICATIONS. CORRUPTION MAY RESULT.
  default: true
  tags:
  - client
- name: bdev_type
  type: str
  level: advanced
  desc: Explicitly set the device type to select the driver if it's needed
  enum_values:
  - aio
  - spdk
  - pmem
  - hm_smr
